{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "For this assigment, we are going to do something like k-fold validation to determine the true expected performance of our model\n",
    "\n",
    "The reason this is a little tricky is due to the fact that we would like to our test and train samples to always have different users.   As we noted in the **prep** workbook, every 4 users is about 10% of the sample, so lets use this to divide our data and determine our average performance of our model.\n",
    "\n",
    "**TASK**:\n",
    "To make this concrete, I want you to divide the full sample up into 9 folds, like this:\n",
    "* fold1: test sample is users >=0 and <4; train sample is users>=4\n",
    "* fold2: test sample is users >=4 and <8; train sample is users<4 or users>=8\n",
    "...etc...\n",
    "\n",
    "Determine the performance of the CNN version of the model (use the model made using the Keras Functional API) for each fold, then average the results.\n",
    "\n",
    "**EXTRA**: Incorporate a multi-head model (with at least 3 heads) each using a different kernel size.  Do the averaging as above.   Can you come up with hyperparamters that beat the performance of the earlier 2-headed model (though that was measured with just a single fold).\n",
    "\n",
    "The assigment below has some starter code in to help you begin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to read data in and normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#\n",
    "# Use this to convert text to floating point\n",
    "def convert_to_float(x):\n",
    "    try:\n",
    "        return np.float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "column_names = ['user-id',\n",
    "                    'activity',\n",
    "                    'timestamp',\n",
    "                    'x-axis',\n",
    "                    'y-axis',\n",
    "                    'z-axis']\n",
    "df = pd.read_csv('/fs/scratch/PAS1495/physics6820/WISDM/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt',\n",
    "                     header=None,\n",
    "                     names=column_names)\n",
    "\n",
    "# Last column has a \";\" character which must be removed ...\n",
    "df['z-axis'].replace(regex=True,\n",
    "      inplace=True,\n",
    "      to_replace=r';',\n",
    "      value=r'')\n",
    "    # ... and then this column must be transformed to float explicitly\n",
    "df['z-axis'] = df['z-axis'].apply(convert_to_float)\n",
    "    # This is very important otherwise the model will not fit and loss\n",
    "    # will show up as NAN\n",
    "#\n",
    "# Get rid if rows wth missing data\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# Define column name of the label vector\n",
    "LABEL = 'ActivityEncoded'\n",
    "# Transform the labels from String to Integer via LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Add a new column to the existing DataFrame with the encoded values\n",
    "df[LABEL] = le.fit_transform(df['activity'].values.ravel())\n",
    "\n",
    "#\n",
    "# Normalize the data: to make things simple, just normalize all of the data (pre train/test) by 20\n",
    "max_all = 20.0\n",
    "df['x-axis'] = df['x-axis'] / 20.0\n",
    "df['y-axis'] = df['y-axis'] / 20.0\n",
    "df['z-axis'] = df['z-axis'] / 20.0\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "max_x = df['x-axis'].max()\n",
    "max_y = df['y-axis'].max()\n",
    "max_z = df['z-axis'].max()\n",
    "\n",
    "print(\"max values \", max_x,max_y,max_z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method to create test/train samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Same labels will be reused throughout the program\n",
    "LABELS = ['Downstairs',\n",
    "          'Jogging',\n",
    "          'Sitting',\n",
    "          'Standing',\n",
    "          'Upstairs',\n",
    "          'Walking']\n",
    "# The number of steps within one time segment\n",
    "TIME_PERIODS = 80    # since there are 50 measurements/sec, this is 1.6 seconds of data\n",
    "# The steps to take from one segment to the next; if this value is equal to\n",
    "# TIME_PERIODS, then there is no overlap between the segments\n",
    "STEP_DISTANCE_TRAIN = 40\n",
    "STEP_DISTANCE_TEST = 80\n",
    "\n",
    "def create_segments_and_labels(df, time_steps, step, label_name):\n",
    "\n",
    "    # x, y, z acceleration as features\n",
    "    N_FEATURES = 3\n",
    "    # Number of steps to advance in each iteration (for me, it should always\n",
    "    # be equal to the time_steps in order to have no overlap between segments)\n",
    "    # step = time_steps\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - time_steps, step):\n",
    "        xs = df['x-axis'].values[i: i + time_steps]\n",
    "        ys = df['y-axis'].values[i: i + time_steps]\n",
    "        zs = df['z-axis'].values[i: i + time_steps]\n",
    "        # Retrieve the most often used label in this segment\n",
    "        label = stats.mode(df[label_name][i: i + time_steps])[0][0]\n",
    "        segments.append([xs, ys, zs])\n",
    "        labels.append(label)\n",
    "\n",
    "    # Bring the segments into a better shape\n",
    "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, time_steps, N_FEATURES)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return reshaped_segments, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method to initialize weights\n",
    "You must to something like this before fitting your model if you do it in a loop.  Otherwize the weights will not change from loop to loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shuffle_weights(model, weights=None):\n",
    "    \"\"\"Randomly permute the weights in `model`, or the given `weights`.\n",
    "    This is a fast approximation of re-initializing the weights of a model.\n",
    "    Assumes weights are distributed independently of the dimensions of the weight tensors\n",
    "      (i.e., the weights have the same distribution along each dimension).\n",
    "    :param Model model: Modify the weights of the given model.\n",
    "    :param list(ndarray) weights: The model's weights will be replaced by a random permutation of these weights.\n",
    "      If `None`, permute the model's current weights.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    # Faster, but less random: only permutes along the first dimension\n",
    "    # weights = [np.random.permutation(w) for w in weights]\n",
    "    model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input,Conv1D, MaxPooling1D,GlobalAveragePooling1D,Dropout,Dense\n",
    "from keras.models import Model\n",
    "# \n",
    "# Our first layer gets the input from our samples - this is 80 time steps by 3 channels\n",
    "#model_m.add(Conv1D(100, 10, activation='relu', input_shape=(80,3)))\n",
    "inputs1 = Input(shape=(80,3))\n",
    "conv1 = Conv1D(100, 10, activation='relu')(inputs1)\n",
    "#\n",
    "# Anoth convolutional layer\n",
    "#model_m.add(Conv1D(100, 10, activation='relu'))\n",
    "conv2 = Conv1D(100, 10, activation='relu')(conv1)\n",
    "#\n",
    "# Max pooling \n",
    "#model_m.add(MaxPooling1D(3))\n",
    "pool1 = MaxPooling1D(3)(conv2)\n",
    "#\n",
    "# Two more convolutional layers\n",
    "#model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "#model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "conv3 = Conv1D(160, 10, activation='relu')(pool1)\n",
    "conv4 = Conv1D(160, 10, activation='relu')(conv3)\n",
    "#\n",
    "# Global average pooling use this instead of \"Flatten\" - it helps reduce overfitting\n",
    "#model_m.add(GlobalAveragePooling1D())\n",
    "glob1 = GlobalAveragePooling1D()(conv4)\n",
    "#\n",
    "drop1 = Dropout(0.5)(glob1)\n",
    "outputs = Dense(num_classes, activation='softmax')(drop1)\n",
    "\n",
    "#\n",
    "# Now define the model\n",
    "model_m = Model(inputs=inputs1, outputs=outputs)\n",
    "print(model_m.summary())    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK: \n",
    "\n",
    "Divide the full sample up into 9 folds, like this:\n",
    "\n",
    "*  fold1: test sample is users >=0 and <4; train sample is users>=4\n",
    "*  fold2: test sample is users >=4 and <8; train sample is users<4 or users>=8 \n",
    "*  ...etc...\n",
    "\n",
    "Determine the performance of the CNN version of the model (use the model made using the Keras Functional API) for each fold, then average the results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example pseudo-code\n",
    "We have 36 users.   So lets group them in steps of 4, and use each group of 4 as our test, and the others as our train.\n",
    "\n",
    "Fill in the rest of the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# You can define the model outside of the loop\n",
    "\n",
    "\n",
    "#\n",
    "# We want our users to \n",
    "user_start = 0\n",
    "for user_groups in range(9):\n",
    "#\n",
    "# Define the users who will form the test group.   The train group is everybody else!\n",
    "    user_start = user_groups*4\n",
    "    user_end = user_start + 4\n",
    "    print()\n",
    "    print(\"User test group\",user_start,user_end)\n",
    "#\n",
    "# Define the test and train dataframes here (using user_start and user_start)\n",
    "\n",
    "#\n",
    "# Create the x_train, y_train ad  x_test, y_test samples from the above dataframes\n",
    "\n",
    "#\n",
    "# Remember to process the y_train and y_test to make one hot versions\n",
    "\n",
    "#\n",
    "# Fit the model with these samples\n",
    "\n",
    "#\n",
    "# Grab the val_accuracy from the appropriate epoch and store it\n",
    "\n",
    "#\n",
    "# When loop is done, average the results to get the overall expected accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA\n",
    "Incorporate a multi-head model (with at least 3 heads) each using a different kernel size.  Do the averaging as above.   Can you come up with hyperparamters that beat the performance of the earlier 2-headed model (though that was measured with just a single fold).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "sys_python36conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
