{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Sequences\n",
    "Thus far in our course we have done both classification and regression analyses.   For our classification, the data has been one of two types:\n",
    "1.  Static feature data: for example, the pulsar dataset.   We have 8 individual features characterizing pulsars as well as background, and our challenge was to develop a model capable of classifying new instances based on those features.  We used fully connected networks (FCNs) to perform this task.  Other tools would work well here, such as SVMs, decision trees, or random forests.\n",
    "2.  Image data: Here we have predominantly used the MNIST dataset.   Here our challenge was again to classify our data as one of multiple possible classes (0-9).   We used both FCNs as well as convolutional neural networks (CNNs). \n",
    "\n",
    "The primary difference in the FCN vs CNN approach is the following:\n",
    "* For FCNs the features are fixed.   For example, when classifying the MNIST sample, we used the pixel intensity at each of the 784 specific pixel positions as our features.\n",
    "* For CNNs the features are \"discovered\".   Again, when classifying the MNIST sample, depending on the exact number of convolutional and pooling layers, the values used in kernels or filters we specify in the convolutional layers are determined (fit) by our training sample, and could be features over small sections of pixels or even over the whole image.\n",
    "\n",
    "In this workbook we will deal with a different sort of dataset: ordered sequences.   We will focus initially on classification of time sequences, but will extend this to classification of text sequences.\n",
    "\n",
    "This workbook is based on examples and code from these sources:\n",
    "1.  [Human Activity Recognition (HAR) Tutorial with Keras and Core ML (Part 1)](https://towardsdatascience.com/human-activity-recognition-har-tutorial-with-keras-and-core-ml-part-1-8c05e365dfa0)  by Nils Ackermann\n",
    "2.  [Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences](https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf) by Nils Ackermann\n",
    "3.  [How to Develop 1D Convolutional Neural Network Models for Human Activity Recognition](https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/) by Jason Brownlee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAR: Human Activity Recognition\n",
    "The analysis we will do in this workbook deals with data collected from smartphones carried by human subjects engaged in normal daily activities: walking, sitting, jogging, standing, climbing upstairs, or descending downstairs.   The smartphones were carried in the front pants pocket by 36 subjects.   The raw data collected are the accelerometer readings in the x,y, and z directions (relative to the smartphones) collected at 50 Hz (or 50 steps/second).   \n",
    "\n",
    "Given the phone orientation when a person is standing:\n",
    "* the **x-direction** is side to side\n",
    "* the **y-direction** is up and down\n",
    "* the **z-direction** is forward and backward\n",
    "as shown in this figure:\n",
    "![har_fig](har_fig.png)\n",
    "\n",
    "A paper describing the results can be found [here](http://www.cis.fordham.edu/wisdm/includes/files/sensorKDD-2010.pdf).\n",
    "\n",
    "A video of the activity can be found [here](https://www.youtube.com/watch?v=XOEN9W05_4A)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data:  \n",
    "\n",
    "The data can be found here:  http://www.cis.fordham.edu/wisdm/dataset.php\n",
    "\n",
    "I have placed this in the scratch area on OSC at this location:\n",
    "/fs/scratch/PAS1495/physics6820/WISDM/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt\n",
    "\n",
    "There is a text file in the same location that has more descriptive information about the data.\n",
    "\n",
    "The columns in the above files are the following:\n",
    "* user-id: this is a number from 1-30\n",
    "* activity: this is a label from the following: Jogging, Walking, Standing, Sitting, Upstairs, Downstairs\n",
    "* timestamp: this is a relative timestamp from phone's uptime in nanoseconds (so the text file says - it is not obvious that this is correct from my inspection of the data)\n",
    "* x-axis: a floating-point values between -20 .. 20\n",
    "                The acceleration in the x direction as measured\n",
    "                by the android phone's accelerometer. \n",
    "                A value of 10 = 1g = 9.81 m/s^2, and\n",
    "                0 = no acceleration.\n",
    "                The acceleration recorded includes gravitational\n",
    "                acceleration toward the center of the Earth, so\n",
    "                that when the phone is at rest on a flat surface\n",
    "                the vertical axis will register +-10. \n",
    "* y-axis: see x-axis\n",
    "* z-axis: see x-axis\n",
    "\n",
    "Let's read the data in so we can explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#\n",
    "# Use this to convert text to floating point\n",
    "def convert_to_float(x):\n",
    "    try:\n",
    "        return np.float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "column_names = ['user-id',\n",
    "                    'activity',\n",
    "                    'timestamp',\n",
    "                    'x-axis',\n",
    "                    'y-axis',\n",
    "                    'z-axis']\n",
    "df = pd.read_csv('/fs/scratch/PAS1495/physics6820/WISDM/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt',\n",
    "                     header=None,\n",
    "                     names=column_names)\n",
    "\n",
    "# Last column has a \";\" character which must be removed ...\n",
    "df['z-axis'].replace(regex=True,\n",
    "      inplace=True,\n",
    "      to_replace=r';',\n",
    "      value=r'')\n",
    "    # ... and then this column must be transformed to float explicitly\n",
    "df['z-axis'] = df['z-axis'].apply(convert_to_float)\n",
    "    # This is very important otherwise the model will not fit and loss\n",
    "    # will show up as NAN\n",
    "#\n",
    "# Get rid if rows wth missing data\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple plots\n",
    "Lets make some simple plots which show how many samples are in the data for: \n",
    "* each activity\n",
    "* each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Show how many training examples exist for each of the six activities\n",
    "df['activity'].value_counts().plot(kind='bar',\n",
    "                                   title='Training Examples by Activity Type')\n",
    "plt.show()\n",
    "# Better understand how the recordings are spread across the different\n",
    "# users who participated in the study\n",
    "#print(df['user-id'].value_counts())\n",
    "df['user-id'].value_counts().plot(kind='bar',\n",
    "                                  title='Training Examples by User')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data\n",
    "We need to split the data into test and train data.   The way we will do this is to see how much time each user spends in each activity.   We will use defaultdicts to keep track of this.\n",
    "\n",
    "Also, since each \"step\" in time is 1/50.0 of a second, we divide by 50 to convert steps to seconds (so activityStepsByUser is actual activity by user in seconds).\n",
    "\n",
    "Be patient!   This next code block takes a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "activitySteps = defaultdict(list)\n",
    "activityStepsByUser = defaultdict(partial(defaultdict, float))\n",
    "activitiesByUser = defaultdict(int)\n",
    "\n",
    "oldActivity = ''\n",
    "oldUser = -1\n",
    "startTime = -1\n",
    "num = 0\n",
    "steps = 0\n",
    "activities = set()\n",
    "#\n",
    "# Loop over each row in our dataset\n",
    "for index, row in df.iterrows():\n",
    "#\n",
    "# Data from the current row\n",
    "    activity = row['activity']\n",
    "    activities.add(activity)\n",
    "    user = row['user-id']\n",
    "    currentTime = row['timestamp']\n",
    "    steps += 1\n",
    "    #print(\"user,activity,ctime\",user,activity,steps)\n",
    "#\n",
    "# Is the activty of this row different from our last row?  How about the user?\n",
    "# If either of these change, collect data\n",
    "    if activity != oldActivity or user != oldUser:\n",
    "#\n",
    "# If oldUser is less than zero then we have not started collecting data yet!\n",
    "        if oldUser >= 0:\n",
    "#\n",
    "# Something changed, so store the old data, and reset the variables to the current activity/user\n",
    "            activitySteps[oldActivity].append(steps/50.0)\n",
    "            activityStepsByUser[oldUser][oldActivity] += steps/50.0\n",
    "            activitiesByUser[oldUser] += 1\n",
    "            oldActivity = activity\n",
    "            oldUser = user\n",
    "            startTime = currentTime\n",
    "            steps = 0\n",
    "        else:\n",
    "            oldUser = user\n",
    "            startTime = currentTime\n",
    "            steps = 0\n",
    "    num += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printout\n",
    "Now we can printout how much time each user spends in each activity.   We see that some of the participants don't spend any time in some of the activites.   we also note that if we order the particpants by user-id, that approximately every 4 users corresponds to about 10% of the data.   \n",
    "\n",
    "Also note that the amount of time each user spends in each ativity is typically about 50-60s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# How much time total for all users and all activities?\n",
    "total_time = 0.0\n",
    "for user in range(37):\n",
    "    for activity in activities:\n",
    "        total_time += activityStepsByUser[user][activity]\n",
    "\n",
    "print('User    Num    Jog    Walk   Stand     Sit  Upstairs   DownSt  TotalTime  TimePerAct Frac')\n",
    "summedFrac = 0.0\n",
    "segmentTimes = []\n",
    "for user in range(37):\n",
    "    print(user,'\\t',activitiesByUser[user],'\\t',end='')\n",
    "    total_user = 0.0\n",
    "    for activity in activities:\n",
    "        total_user += activityStepsByUser[user][activity]\n",
    "        if activityStepsByUser[user][activity]> 0.0:\n",
    "#        if activityStepsByUser[user][activity]> 0.0 and activityStepsByUser[user][activity] < 100:\n",
    "            segmentTimes.append(activityStepsByUser[user][activity])\n",
    "        print(round(activityStepsByUser[user][activity],0),'\\t',end='')\n",
    "    summedFrac += total_user/total_time\n",
    "    timePerActivity = 0.0\n",
    "    if activitiesByUser[user]>0:\n",
    "        timePerActivity = total_user / float(activitiesByUser[user])\n",
    "    print(round(total_user,0),' ',round(timePerActivity,0),' ',round(summedFrac,2))\n",
    "\n",
    "plt.hist(segmentTimes, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split\n",
    "\n",
    "We might want to keep all of a given user's data in either the test or train sample - this way we use data from different people to predict behavior of new people.\n",
    "\n",
    "If we want a 80%/20% split, it looks like we can define:\n",
    "* training: user-id <=28\n",
    "* testing: user-id>28\n",
    "\n",
    "We also need to convert the 'activity' column tfrom text ('Jogging', etc) to a number (1-6) so we can one-hot encode it later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# Define column name of the label vector\n",
    "LABEL = 'ActivityEncoded'\n",
    "# Transform the labels from String to Integer via LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Add a new column to the existing DataFrame with the encoded values\n",
    "df[LABEL] = le.fit_transform(df['activity'].values.ravel())\n",
    "print(df.head(5))\n",
    "\n",
    "# Differentiate between test set and training set\n",
    "df_test = df[df['user-id'] > 28]\n",
    "df_train = df[df['user-id'] <= 28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing feature data\n",
    "As usual, we want to normalize our features.   We will use **training set maximums** to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features for training data set (values between 0 and 1)\n",
    "# Surpress warning for next 3 operation\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "max_x = df_train['x-axis'].max()\n",
    "max_y = df_train['y-axis'].max()\n",
    "max_z = df_train['z-axis'].max()\n",
    "\n",
    "print(\"max values \", max_x,max_y,max_z)\n",
    "\n",
    "df_train['x-axis'] = df_train['x-axis'] / max_x\n",
    "df_train['y-axis'] = df_train['y-axis'] / max_y\n",
    "df_train['z-axis'] = df_train['z-axis'] / max_z\n",
    "# Round numbers\n",
    "df_train = df_train.round({'x-axis': 4, 'y-axis': 4, 'z-axis': 4})\n",
    "\n",
    "df_test['x-axis'] = df_test['x-axis'] / max_x\n",
    "df_test['y-axis'] = df_test['y-axis'] / max_y\n",
    "df_test['z-axis'] = df_test['z-axis'] / max_z\n",
    "# Round numbers\n",
    "df_test = df_test.round({'x-axis': 4, 'y-axis': 4, 'z-axis': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting \n",
    "Let's make some plots to see what the data looks like.   We will select out various activites from our primary dataframe **df**, and see if they make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standing  = df_train[df_train['activity'] == 'Standing'][:200]\n",
    "# gca stands for 'get current axis'\n",
    "ax = plt.gca()\n",
    "\n",
    "df_standing.plot(kind='line',x='timestamp',y='x-axis',ax=ax)\n",
    "df_standing.plot(kind='line',x='timestamp',y='y-axis', color='red', ax=ax)\n",
    "df_standing.plot(kind='line',x='timestamp',y='z-axis', color='green', ax=ax)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df_walking  = df_train[df_train['activity'] == 'Walking'][:200]\n",
    "# gca stands for 'get current axis'\n",
    "ax = plt.gca()\n",
    "\n",
    "df_walking.plot(kind='line',x='timestamp',y='x-axis',ax=ax)\n",
    "df_walking.plot(kind='line',x='timestamp',y='y-axis', color='red', ax=ax)\n",
    "df_walking.plot(kind='line',x='timestamp',y='z-axis', color='green', ax=ax)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_jogging  = df_train[df_train['activity'] == 'Jogging'][:200]\n",
    "# gca stands for 'get current axis'\n",
    "ax = plt.gca()\n",
    "\n",
    "df_jogging.plot(kind='line',x='timestamp',y='x-axis',ax=ax)\n",
    "df_jogging.plot(kind='line',x='timestamp',y='y-axis', color='red', ax=ax)\n",
    "df_jogging.plot(kind='line',x='timestamp',y='z-axis', color='green', ax=ax)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_upstairs  = df_train[df_train['activity'] == 'Upstairs'][:200]\n",
    "# gca stands for 'get current axis'\n",
    "ax = plt.gca()\n",
    "\n",
    "df_upstairs.plot(kind='line',x='timestamp',y='x-axis',ax=ax)\n",
    "df_upstairs.plot(kind='line',x='timestamp',y='y-axis', color='red', ax=ax)\n",
    "df_upstairs.plot(kind='line',x='timestamp',y='z-axis', color='green', ax=ax)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_user  = df_train[df_train['user-id'] == 15][180:230]\n",
    "# gca stands for 'get current axis'\n",
    "ax = plt.gca()\n",
    "\n",
    "df_user.plot(kind='line',x='timestamp',y='x-axis',ax=ax)\n",
    "df_user.plot(kind='line',x='timestamp',y='y-axis', color='red', ax=ax)\n",
    "df_user.plot(kind='line',x='timestamp',y='z-axis', color='green', ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our labeled data samples\n",
    "From our above plots and tables, we see that typical times that each user spends in a given activity is hundreds of seconds.   The smallest non-zero time is about 30 seconds, the largest amount of time is just under 900 seconds.\n",
    "\n",
    "We will define our samples to be 1.6 seconds, and we will assign as the label of each sample the activity that occurs the most in that sample.  To help increase the total number of samples, we can also allow some overlap in our samples.   We will **not** allow any overlap in our test data however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Same labels will be reused throughout the program\n",
    "LABELS = ['Downstairs',\n",
    "          'Jogging',\n",
    "          'Sitting',\n",
    "          'Standing',\n",
    "          'Upstairs',\n",
    "          'Walking']\n",
    "# The number of steps within one time segment\n",
    "TIME_PERIODS = 80    # since there are 50 measurements/sec, this is 1.6 seconds of data\n",
    "# The steps to take from one segment to the next; if this value is equal to\n",
    "# TIME_PERIODS, then there is no overlap between the segments\n",
    "STEP_DISTANCE_TRAIN = 40\n",
    "STEP_DISTANCE_TEST = 80\n",
    "\n",
    "def create_segments_and_labels(df, time_steps, step, label_name):\n",
    "\n",
    "    # x, y, z acceleration as features\n",
    "    N_FEATURES = 3\n",
    "    # Number of steps to advance in each iteration (for me, it should always\n",
    "    # be equal to the time_steps in order to have no overlap between segments)\n",
    "    # step = time_steps\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - time_steps, step):\n",
    "        xs = df['x-axis'].values[i: i + time_steps]\n",
    "        ys = df['y-axis'].values[i: i + time_steps]\n",
    "        zs = df['z-axis'].values[i: i + time_steps]\n",
    "        # Retrieve the most often used label in this segment\n",
    "        label = stats.mode(df[label_name][i: i + time_steps])[0][0]\n",
    "        segments.append([xs, ys, zs])\n",
    "        labels.append(label)\n",
    "\n",
    "    # Bring the segments into a better shape\n",
    "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, time_steps, N_FEATURES)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return reshaped_segments, labels\n",
    "\n",
    "x_train, y_train = create_segments_and_labels(df_train,\n",
    "                                              TIME_PERIODS,\n",
    "                                              STEP_DISTANCE_TRAIN,\n",
    "                                              LABEL)\n",
    "x_test, y_test = create_segments_and_labels(df_test,\n",
    "                                              TIME_PERIODS,\n",
    "                                              STEP_DISTANCE_TEST,\n",
    "                                              LABEL)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print(x_train.shape[0], 'training samples')\n",
    "print('y_train shape: ', y_train.shape)\n",
    "\n",
    "print('x_test shape: ', x_test.shape)\n",
    "print(x_test.shape[0], 'testing samples')\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Time Sequences Using a Fully Connected Network\n",
    "Let's begin with something reasonably simple.   We have labeled samples, each 80 time steps long, with 3 channels of information at each time step.   Let's treat this as $80\\times3=240$ total features.   We can easily write down a multi-layer network with a softmax output to classify this. \n",
    "\n",
    "Our network will look like the figure below:\n",
    "![har_fcn_fig](sequence_fcn.png)\n",
    "“Deep Neural Network Example” by Nils Ackermann is licensed under Creative Commons [CC BY-ND 4.0](https://creativecommons.org/licenses/by-nd/4.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set input & output dimensions\n",
    "num_time_periods, num_sensors = x_train.shape[1], x_train.shape[2]\n",
    "num_classes = le.classes_.size\n",
    "print(list(le.classes_))\n",
    "\n",
    "# Reshape the input data\n",
    "input_shape = (num_time_periods*num_sensors)\n",
    "#x_train = x_train.reshape(x_train.shape[0], input_shape)\n",
    "#x_test = x_test.reshape(x_test.shape[0], input_shape)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('input_shape:', input_shape)\n",
    "\n",
    "# One-hot enocde the output labels\n",
    "from keras.utils import np_utils\n",
    "x_train = x_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_train_hot = np_utils.to_categorical(y_train, num_classes)\n",
    "print('New y_train shape: ', y_train_hot.shape)\n",
    "\n",
    "x_test = x_test.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "y_test_hot = np_utils.to_categorical(y_test, num_classes)\n",
    "print('New y_test shape: ', y_test_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape\n",
    "model_m = Sequential()\n",
    "\n",
    "model_m.add(Dense(100, activation='relu', input_shape=(80,3)))\n",
    "model_m.add(Dense(100, activation='relu'))\n",
    "model_m.add(Dense(100, activation='relu'))\n",
    "model_m.add(Flatten())\n",
    "model_m.add(Dense(6, activation='softmax'))\n",
    "model_m.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "print(model_m.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the FCN\n",
    "Here we use the **callbacks** option to monitor the **validation loss**, waiting patience=4 epochs before deciding to stop training.   We also save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 4\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.har_fcn.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "]\n",
    "\n",
    "# Hyper-parameters\n",
    "BATCH_SIZE = 400\n",
    "EPOCHS = 50\n",
    "\n",
    "# Enable validation to use ModelCheckpoint and EarlyStopping callbacks.\n",
    "history = model_m.fit(x_train,\n",
    "                      y_train_hot,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                      callbacks=callbacks_list,\n",
    "                      validation_data=(x_test, y_test_hot),\n",
    "                      verbose=1)\n",
    "best_val_acc =  history.history['val_acc'][-(patience+1)]\n",
    "print(\"Best validation accuracy is:\",best_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Sequences\n",
    "The performance we obtained for a single train/test split was about 71% (evaluated where the validation losss was at a minimum).   Can we do better?\n",
    "\n",
    "We recall that in our study of images, we found that 2D convolution could help - this was because there were features present in the data that were somewhat translationally and rotationally invariant.   The convolution process also allows us to **discover** features, rather than imposing the features from the outside.   We can also use the convolution process with sequences. In this case however, we will employ **1D convolution**.   \n",
    "\n",
    "First lets define the network using Keras, then we will step through the layers to see if we can understand each of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D,GlobalAveragePooling1D\n",
    "#\n",
    "# Define a sequential model as usual\n",
    "model_m = Sequential()\n",
    "# \n",
    "# Our first layer gets the input from our samples - this is 80 time steps by 3 channels\n",
    "model_m.add(Conv1D(100, 10, activation='relu', input_shape=(80,3)))\n",
    "#\n",
    "# Anoth convolutional layer\n",
    "model_m.add(Conv1D(100, 10, activation='relu'))\n",
    "#\n",
    "# Max pooling \n",
    "model_m.add(MaxPooling1D(3))\n",
    "#\n",
    "# Two more convolutional layers\n",
    "model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "#\n",
    "# Global average pooling use this instead of \"Flatten\" - it helps reduce overfitting\n",
    "model_m.add(GlobalAveragePooling1D())\n",
    "#model_m.add(Flatten())\n",
    "\n",
    "model_m.add(Dropout(0.5))\n",
    "model_m.add(Dense(num_classes, activation='softmax'))\n",
    "print(model_m.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv1D Details!\n",
    "To understand how this works, refer to the figure below.   In most respects this sort of network is extremely similar to the 2D concolutinal networks we use for images, though there are some subtle differences.\n",
    "1.  First, it is important to remember that our data is 80 steps in time, and there are 3 channels at each step.   The convolution moves across the **time dimension** (which is why it is 1d).\n",
    "2.  In both of the figures on the left  underneath the first \"Conv Layer\" heading, time moves from top to bottom.\n",
    "3.   The first convolutional layer has a **kernel** size of 10, and there are 100 such kernels (or filters).   Note that since our input data has 3 channels, our kernels **also** have 3 channels (so the kernels are really size 10x3). In our case, with a kernel size of 10, we convolve this across 10 time steps **and** 3 chanels at each time step, outputting one number, which can be thought of as the weighted average across the 3 channels and 10 time steps.  \n",
    "4.  Since we have a default stride of 1, we move ahead 1 time step and do the same calculation.   Since we have 80 time steps, we end up with **71** total such calculations for each kernel.   Since we have 100 kernels (or filters), we have an output size for the first convolutional layer of $71x100$.\n",
    "5.  The next convolutional layers proceed in the same way, and the max pooling layer behaves just like the corresponding from our image classification networks.\n",
    "6.  Another tricky aspect of this network is a new layer called the GlobalAveragePooling1D.   This turns out to be very helpful to reduce overfitting.  The motivation for it can be found in this paper: [Network in Network](https://arxiv.org/pdf/1312.4400.pdf).\n",
    "\n",
    "\n",
    "![har_fcn_fig](sequence_cnn.png).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the fitter!\n",
    "We run the fitter in exactly the same way as we did the FCN above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.har_cnn.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "]\n",
    "\n",
    "model_m.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 400\n",
    "EPOCHS = 50\n",
    "\n",
    "history = model_m.fit(x_train,\n",
    "                      y_train_hot,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                      callbacks=callbacks_list,\n",
    "                      validation_data=(x_test, y_test_hot),\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 4\n",
    "best_val_acc =  history.history['val_acc'][-(patience+1)]\n",
    "print(\"Best validation accuracy is:\",best_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-headed Network\n",
    "We see from above that the 1D convolutional neural network greatly outperforms the standard FCN.  Awesome!  But if you notice, a key parameter in the network is the kernel size.   We can see from our plots above comparing jogging/walking/sittig/climbing stairs, that the time structure of the various activities is different.  Meaning that a larger kernal size - that covers more time steps - might make more sense for some activities, while a smaller kernel size might be more appropriate for other activities.   Can we combine these in a single network?   Yes!   Enter the **multi-headed network**!\n",
    "\n",
    "The basic idea is simple:\n",
    "1.  We have 2 (or more) networks that get the same input.\n",
    "2.  These two networks are then merged and produce the same softmax output as our above network.\n",
    "\n",
    "To implement this network, it is necessary to use the Keras Functional API, which we have already introduced.  Before we do the multi-headed network, let's do a copy of the above CNN network using the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Conv1D, MaxPooling1D,GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "# \n",
    "# Our first layer gets the input from our samples - this is 80 time steps by 3 channels\n",
    "#model_m.add(Conv1D(100, 10, activation='relu', input_shape=(80,3)))\n",
    "inputs1 = Input(shape=(80,3))\n",
    "conv1 = Conv1D(100, 10, activation='relu')(inputs1)\n",
    "#\n",
    "# Anoth convolutional layer\n",
    "#model_m.add(Conv1D(100, 10, activation='relu'))\n",
    "conv2 = Conv1D(100, 10, activation='relu')(conv1)\n",
    "#\n",
    "# Max pooling \n",
    "#model_m.add(MaxPooling1D(3))\n",
    "pool1 = MaxPooling1D(3)(conv2)\n",
    "#\n",
    "# Two more convolutional layers\n",
    "#model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "#model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "conv3 = Conv1D(160, 10, activation='relu')(pool1)\n",
    "conv4 = Conv1D(160, 10, activation='relu')(conv3)\n",
    "#\n",
    "# Global average pooling use this instead of \"Flatten\" - it helps reduce overfitting\n",
    "#model_m.add(GlobalAveragePooling1D())\n",
    "glob1 = GlobalAveragePooling1D()(conv4)\n",
    "#\n",
    "drop1 = Dropout(0.5)(glob1)\n",
    "outputs = Dense(num_classes, activation='softmax')(drop1)\n",
    "\n",
    "#\n",
    "# Now define the model\n",
    "model_m = Model(inputs=inputs1, outputs=outputs)\n",
    "print(model_m.summary())    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.har_cnn.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "]\n",
    "\n",
    "model_m.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 400\n",
    "EPOCHS = 50\n",
    "\n",
    "history = model_m.fit(x_train,\n",
    "                      y_train_hot,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                      callbacks=callbacks_list,\n",
    "                      validation_data=(x_test, y_test_hot),\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 4\n",
    "best_val_acc =  history.history['val_acc'][-(patience+1)]\n",
    "print(\"Best validation accuracy is:\",best_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the true multi-head network\n",
    "To make this network we remove the 2 convolutional layers at the end of the model - just to cut down on the total number of trainable parameters (to help reduce overfitting).   The structure of the two \"heads\" is exactly the same, except for the kernel size.   It is **not** necessary that the two networks be so similar - this is just done for convenience.   \n",
    "\n",
    "We then **merge** the two heads using a **concatenate** layer, and send the output of that through a softmax to get our final output.   There are alot of choices that can be made here: kernel sizes, number of kernels, number of convolutional layers, number of heads, amount of dropout, etc.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Conv1D, MaxPooling1D,GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "# \n",
    "#\n",
    "inputs1 = Input(shape=(80,3))\n",
    "h1conv1 = Conv1D(filters=100, kernel_size=10, activation='relu')(inputs1)\n",
    "h1conv2 = Conv1D(filters=100, kernel_size=10, activation='relu')(h1conv1)\n",
    "h1pool1 = MaxPooling1D(3)(h1conv2)\n",
    "h1glob1 = GlobalAveragePooling1D()(h1pool1)\n",
    "h1drop1 = Dropout(0.5)(h1glob1)\n",
    "#\n",
    "inputs2 = Input(shape=(80,3))\n",
    "h2conv1 = Conv1D(filters=100, kernel_size=20, activation='relu')(inputs2)\n",
    "h2conv2 = Conv1D(filters=100, kernel_size=20, activation='relu')(h2conv1)\n",
    "h2pool1 = MaxPooling1D(3)(h2conv2)\n",
    "h2glob1 = GlobalAveragePooling1D()(h2pool1)\n",
    "h2drop1 = Dropout(0.5)(h2glob1)\n",
    "\n",
    "#\n",
    "# Concatentate the output of the above two branches\n",
    "merged = concatenate([h1drop1,h2drop1])\n",
    "dense1 = Dense(100, activation='relu')(merged)\n",
    "outputs = Dense(6, activation='softmax')(dense1)\n",
    "\n",
    "#\n",
    "# Now define the model\n",
    "model_m = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "print(model_m.summary())    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.har_cnn.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "]\n",
    "\n",
    "model_m.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 400\n",
    "EPOCHS = 50\n",
    "\n",
    "history = model_m.fit([x_train,x_train],\n",
    "                      y_train_hot,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                      callbacks=callbacks_list,\n",
    "                      validation_data=([x_test,x_test], y_test_hot),\n",
    "                      verbose=1)\n",
    "print(\"Validation accuracies by epoch \",history.history[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 4\n",
    "best_val_acc =  history.history['val_acc'][-(patience+1)]\n",
    "print(\"Best validation accuracy is:\",best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "sys_python36conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
