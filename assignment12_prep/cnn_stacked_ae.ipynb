{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Autoencoders and Classification\n",
    "Although autoencoders are an unsupervised algorithm, it turns out that they can play a very important role in supervised alorithms.\n",
    "\n",
    "The basic idea is this: let's say you have a relatively small dataset of images that you would like to train a classifier on.   Since the dataset is small, you would expect that the performance of the trained classifier on new unseen data will not be so great.   However, you know of a much larger dataset of similar images, but that dataset is unlabeled.   Can this unlabeled dataset help?\n",
    "\n",
    "The answer is yes - and the idea depends on autoencoders.   You train an autoencoder (built as we have using a separate encoder and decoder) on the unlabeled data.  Then you use the encoder - which has learned features from the unlabeled data, and put this in place of your input layers of your standard CNN.  You put a fully connected layer after this, followed by a softmax layer and you then train that combination.   Note that the encoder weights are **not** modified during this procedure!  However the weights of the fully connected layers after the encoder are modified during training.\n",
    "\n",
    "Let's see how this works.   First, we will train a standard CNN, but we will build it with an eye towards replacing the input layer later on with our already trained enocder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "#\n",
    "# See this for more info: https://arxiv.org/pdf/1702.05373.pdf\n",
    "mat = sio.loadmat('/fs/scratch/PAS1043/physics6820/emnist/matlab/emnist-byclass.mat')\n",
    "#print(mat)\n",
    "\n",
    "data = mat['dataset']\n",
    "\n",
    "ex_train = data['train'][0,0]['images'][0,0]\n",
    "ey_train = data['train'][0,0]['labels'][0,0]\n",
    "ex_test = data['test'][0,0]['images'][0,0]\n",
    "ey_test = data['test'][0,0]['labels'][0,0]\n",
    "\n",
    "ex_train = ex_train.reshape( (ex_train.shape[0], 28,28), order='F')\n",
    "ex_test = ex_test.reshape( (ex_test.shape[0], 28,28), order='F')\n",
    "\n",
    "ex_train = ex_train.reshape( (ex_train.shape[0], 784))\n",
    "ex_test = ex_test.reshape( (ex_test.shape[0], 784))\n",
    "ex_train = ex_train.astype('float32') / 255.\n",
    "ex_test = ex_test.astype('float32') / 255.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.DataFrame(ex_train)\n",
    "df_train['label'] = ey_train\n",
    "df_digits_train = df_train[df_train['label']<=9]\n",
    "#print(df_digits_train)\n",
    "x_train = df_digits_train.iloc[:,:784].values\n",
    "x_train = x_train.reshape((x_train.shape[0],28,28,1))\n",
    "y_train = df_digits_train['label'].values\n",
    "print(x_train.shape,y_train.shape)\n",
    "\n",
    "df_test = pd.DataFrame(ex_test)\n",
    "df_test['label'] = ey_test\n",
    "df_digits_test = df_test[df_test['label']<=9]\n",
    "#print(df_digits_test)\n",
    "x_test = df_digits_test.iloc[:,:784].values\n",
    "x_test = x_test.reshape((x_test.shape[0],28,28,1))\n",
    "y_test = df_digits_test['label'].values\n",
    "print(x_test.shape,y_test.shape)\n",
    "\n",
    "#\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_labels_cat = to_categorical(y_train)\n",
    "y_test_labels_cat = to_categorical(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a standard CNN\n",
    "In building the standard network below, we make sure to separate the CNN portion of the network - which we call **cnn_prenetwork** below - from the flattened and fully connected portion of the network.   There is no real difference between this network and the one we designed a couple of weeks ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "#\n",
    "# Define the initial part of the CNN as a \"pre\" network\n",
    "cnn_prenetwork = models.Sequential()\n",
    "cnn_prenetwork.add(layers.Conv2D(30,(5,5),activation='relu',input_shape=(28,28,1)))\n",
    "cnn_prenetwork.add(layers.MaxPooling2D((2,2)))\n",
    "cnn_prenetwork.add(layers.Conv2D(25,(5,5),activation='relu'))\n",
    "cnn_prenetwork.add(layers.MaxPooling2D((2,2)))\n",
    "print(cnn_prenetwork.summary())\n",
    "#\n",
    "# Now make our actual full network - using the above \"pre\" network as our first layer\n",
    "cnn_network = models.Sequential()\n",
    "#\n",
    "# Connect to a dense output layer - just like an FCN\n",
    "cnn_network.add(cnn_prenetwork)\n",
    "cnn_network.add(layers.Flatten())\n",
    "cnn_network.add(layers.Dense(64,activation='relu'))\n",
    "cnn_network.add(layers.Dense(10,activation='softmax'))\n",
    "#\n",
    "# Compile\n",
    "cnn_network.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#\n",
    "# Fit/save/print summary\n",
    "history = cnn_network.fit(x_train,y_train_labels_cat,epochs=5,batch_size=256,\n",
    "                              validation_data=(x_test,y_test_labels_cat))\n",
    "cnn_network.save('fully_trained_model_cnn.h5')\n",
    "print(cnn_network.summary())\n",
    "print(\"Validation accuracy\" ,history.history['val_acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot performance vs epoch\n",
    "From the plot below it is clear that the fit has not yet converged - but we limited the epochs to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_accuracy = history.history['acc']\n",
    "test_accuracy = history.history['val_acc']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training accuracy', 'Test accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now build a stacked autoencoder CNN\n",
    "The only change we will make with this network is to replace **cnn_prenetwork** with the encoder network that we already trained.   We will use the one trained on the larger digit sample, and stored on disk, called **fully_trained_encoder_cnn.h5**.\n",
    "\n",
    "Remember that the encoder we use here was trained as part of an autoencoder.  The goal in that case was to discover features that would help reconstruct the original image.  The autoencoder knew **nothing** about the labels of the digits it was training on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import load_model\n",
    "#\n",
    "#encoder = load_model('fully_trained_encoder_cnn.h5')\n",
    "encoder = load_model('fully_trained_encoder_cnn.h5')\n",
    "#\n",
    "cnn_network = models.Sequential()\n",
    "#\n",
    "# Connect to a dense output layer - just like an FCN\n",
    "cnn_network.add(encoder)\n",
    "cnn_network.add(layers.Flatten())\n",
    "cnn_network.add(layers.Dense(64,activation='relu'))\n",
    "cnn_network.add(layers.Dense(10,activation='softmax'))\n",
    "#\n",
    "# Compile\n",
    "cnn_network.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#\n",
    "# Fit/save/print summary\n",
    "history = cnn_network.fit(x_train,y_train_labels_cat,epochs=5,batch_size=256,\n",
    "                              validation_data=(x_test,y_test_labels_cat))\n",
    "cnn_network.save('fully_trained_model_cnn_stacked_ae.h5')\n",
    "print(cnn_network.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_accuracy = history.history['acc']\n",
    "test_accuracy = history.history['val_acc']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training accuracy', 'Test accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_accuracy = history.history['acc']\n",
    "test_accuracy = history.history['val_acc']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training accuracy', 'Test accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We see that the performance of the stacked autoencoder/classifier is comparable to that of a fully trained CNN classifier.   Again: the encoder we used knew nothing about the labels of our data - it simply learned features useful for encoding/decoding.   But these learned features turned out to be very useful for classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "sys_python36conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
