{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kfoldValidation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Z6Fu0n-Ts40R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classification and k-fold Validation\n",
        "We are going to introduce the idea of k-fold validation using our binary classifier.   For this study, we will use the one-versus all analysis: our classifier is attempting to identify a single predefined digit (single) versus all of the other digits (our background). \n",
        "\n",
        "Up to this point, we have been developing our models by splitting our data into test and train subsets, generally using a 20%/80% test/train split.   The idea being, that we train the model with 80% of the data, and reserve 20% to test the model, with the assumption being that the results we obtain on the test sample are representative of what we should expect when we apply our trained model to new (truly) unseen data.\n",
        "\n",
        "But there are two major problems with this strategy:\n",
        "1.  What if we got \"lucky\" (or \"unlucky\" as the case may be) with the 20% we had reserved for testing?  That is, the particular samples that we have reserved for testing may have features that happen to be distributed in a way that make them much easier (or harder) to classifiy.  Then our model may perform significantly differently from our expectations when we apply it to new data.   \n",
        "2.  We don't use all of our available data in training our model.   It does seem unfortunate that we don't take advantage of the 20% of the data that we reserved for testing.  \n",
        "\n",
        "k-fold validation attempts to deal with each of these issues.\n"
      ]
    },
    {
      "metadata": {
        "id": "1HyRRY_-tg2D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Get the data\n",
        "First let's get the data.  As usual we will have a hook in our code to allow us to switch between the smaller dataset (_short) and the larger dataset."
      ]
    },
    {
      "metadata": {
        "id": "JmGFm9KAs3Ls",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#\n",
        "#short = \"\"\n",
        "short = \"short_\"\n",
        "\n",
        "#\n",
        "# Read in all of the other digits\n",
        "dfAll = pd.DataFrame()\n",
        "for digit in range(10):\n",
        "    print(\"Processing digit \",digit)\n",
        "    fname = 'https://raw.githubusercontent.com/big-data-analytics-physics/data/master/ch3/digit_' + short + str(digit) + '.csv'\n",
        "    df = pd.read_csv(fname,header=None)\n",
        "    df['digit'] = digit\n",
        "    dfAll = pd.concat([dfAll, df])\n",
        "#\n",
        "# Define our \"signal\" digit\n",
        "digitSignal = 5\n",
        "dfA = dfAll[dfAll['digit']==digitSignal]\n",
        "dfB = dfAll[dfAll['digit']!=digitSignal]\n",
        "#\n",
        "# Define the signal column\n",
        "dfA['signal'] = 1\n",
        "dfB['signal'] = 0\n",
        "#\n",
        "# Shuffle our background\n",
        "from sklearn.utils import shuffle\n",
        "dfB = shuffle(dfB)\n",
        "#\n",
        "# Uncomment the next line to limit dfB to be the same length as dfA\n",
        "dfB_use = dfB.head(len(dfA))\n",
        "dfCombined = dfB_use\n",
        "dfCombined = pd.concat([dfCombined, dfA])\n",
        "print(\"Total data size\",len(dfCombined))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4d7Ddm2t5ED",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The \"Old\" Approach: A single test/train split\n",
        "Before we proceed with k-fold validation, lets revisit our standard, single-split test/train approach.   We will split the data, fit it with a model, predict results, and get performance metrics and our ROC curves.   Also thinking ahead, we will put **ALL** of the code specific to fitting **AND** performance measures for the binary classifier into their own modules."
      ]
    },
    {
      "metadata": {
        "id": "rt56eBPgYQjl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "def autovivify(levels=1, final=dict):\n",
        "    return (defaultdict(final) if levels < 2 else\n",
        "            defaultdict(lambda: autovivify(levels-1, final)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jNoX8X7uZECP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Determine the performance\n",
        "def binaryPerformance(y,y_pred,y_score,debug=False):\n",
        "#\n",
        "# Assuming a binary classifier with 1=signal, 0=background\n",
        "  confusionMatrix = autovivify(2,int)\n",
        "  for i in range(len(y_pred)):\n",
        "    trueClass = y[i]\n",
        "    predClass = y_pred[i]\n",
        "    confusionMatrix[trueClass][predClass] += 1\n",
        "\n",
        "  if debug:\n",
        "    for trueClass in range(2):\n",
        "      print(\"True: \",trueClass,end=\"\")\n",
        "      for predClass in range(2):\n",
        "        print(\"\\t\",confusionMatrix[trueClass][predClass],end=\"\")\n",
        "      print()\n",
        "    print()\n",
        "  TP = confusionMatrix[1][1]\n",
        "  FP = confusionMatrix[0][1]\n",
        "  FN = confusionMatrix[1][0]\n",
        "  TN = confusionMatrix[0][0]\n",
        "  \n",
        "  if debug:\n",
        "    print(\"TP predicted true, actually true   \",TP)\n",
        "    print(\"FP predicted true, acutally false  \",FP)\n",
        "    print(\"TN predicted false, actually false \",TN)\n",
        "    print(\"FN predicted false, actually true  \",FN)\n",
        "\n",
        "\n",
        "  precision = TP / (TP + FP)\n",
        "  recall = TP / (TP + FN)\n",
        "  f1_score = 2.0 / ( (1.0/precision) + (1.0/recall) )\n",
        "  \n",
        "  if debug:\n",
        "    print(\"Precision = TP/(TP+FP) = fraction of predicted true actually true \",precision)\n",
        "    print(\"Recall = TP/(TP+FN) = fraction of true class predicted to be true \",recall)\n",
        "    print(\"F1 score = \",f1_score)\n",
        "\n",
        "  #\n",
        "  # Get the ROC curve.  We will use the sklearn function to do this\n",
        "  from sklearn import metrics\n",
        "  #fpr_train, tpr_train, thresholds_train = metrics.roc_curve(y_train, y_train_score, pos_label=1)\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y, y_score, pos_label=1)\n",
        "  #\n",
        "  # Get the auc\n",
        "  auc = metrics.roc_auc_score(y, y_score)\n",
        "  if debug:\n",
        "    print(\"AUC this sample: \",auc_train)\n",
        "  \n",
        "  return precision,recall,auc,fpr, tpr, thresholds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7btmEwVnU6pc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The runFitter method\n",
        "This looks similar to what we have used before.  **NOTE** however that the results are stored in a dictionary, and it is this single dictionary that is returned by calling this method."
      ]
    },
    {
      "metadata": {
        "id": "fKywvoIOU-5A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def runFitter(estimator,X_train,y_train,X_test,y_test):\n",
        "#\n",
        "# Now fit to our training set\n",
        "  estimator.fit(X_train,y_train)\n",
        "#\n",
        "# Now predict the classes and get the score for our traing set\n",
        "  y_train_pred = estimator.predict(X_train)\n",
        "  y_train_score = estimator.decision_function(X_train)   # NOTE: some estimators have a predict_prob method instead od descision_function\n",
        "#\n",
        "# Now predict the classes and get the score for our test set\n",
        "  y_test_pred = estimator.predict(X_test)\n",
        "  y_test_score = estimator.decision_function(X_test)\n",
        "\n",
        "#\n",
        "# Now get the performaance\n",
        "  precision_test,recall_test,auc_test,fpr_test, tpr_test, thresholds_test = binaryPerformance(y_test,y_test_pred,y_test_score,debug=False)\n",
        "  precision_train,recall_train,auc_train,fpr_train, tpr_train, thresholds_train = binaryPerformance(y_train,y_train_pred,y_train_score,debug=False)\n",
        "#\n",
        "# Decide what you want to return: for now, just precision, recall, and auc for both test and train\n",
        "  results = {\n",
        "      'precision_train':precision_train,\n",
        "      'recall_train':recall_train,\n",
        "      'auc_train':auc_train,\n",
        "      'fpr_train':fpr_train, \n",
        "      'tpr_train':tpr_train, \n",
        "      'thresholds_train':thresholds_train,\n",
        "      'precision_test':precision_test,\n",
        "      'recall_test':recall_test,\n",
        "      'auc_test':auc_test,\n",
        "      'fpr_test':fpr_test, \n",
        "      'tpr_test':tpr_test, \n",
        "      'thresholds_test':thresholds_test}\n",
        "\n",
        "  return results\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U2f9Z4uTEBnX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Do the Fit!\n",
        "The following code performs a single test/train split fit.   Note how the results returned from **runFitter** are accessed.\n"
      ]
    },
    {
      "metadata": {
        "id": "Xc75uogpX-Wi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#\n",
        "# Form our test and train data\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_digits,test_digits = train_test_split(dfCombined, test_size=0.3, random_state=42)\n",
        "\n",
        "X_train = train_digits.as_matrix(columns=train_digits.columns[:784])\n",
        "y_train = train_digits['signal'].values\n",
        "\n",
        "X_test = test_digits.as_matrix(columns=test_digits.columns[:784])\n",
        "y_test = test_digits['signal'].values\n",
        "\n",
        "\n",
        "#\n",
        "# Get our estimator and predict\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "estimator = LinearSVC(random_state=42,dual=False,max_iter=100,tol=0.001)    # use dual=False when  n_samples > n_features which is what we have\n",
        "\n",
        "\n",
        "results = runFitter(estimator,X_train,y_train,X_test,y_test)\n",
        "\n",
        "print(\"AUC training data: \",results['auc_train'])\n",
        "print(\"AUC testing data:  \",results['auc_test'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jk04srJKqn7S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Plot results"
      ]
    },
    {
      "metadata": {
        "id": "Lx0DAkW3qwaZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def enable_plotly_in_cell():\n",
        "  import IPython\n",
        "  from plotly.offline import init_notebook_mode\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "  '''))\n",
        "  init_notebook_mode(connected=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J6t7RQq_qrx_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import plotly.plotly as py\n",
        "import numpy as np\n",
        "from plotly.offline import iplot\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "enable_plotly_in_cell()\n",
        "\n",
        "trace0 = go.Scatter(\n",
        "    x=results['fpr_train'],\n",
        "    y=results['tpr_train'],\n",
        "    text=results['thresholds_train'],\n",
        "    mode='line',\n",
        "    name='Trainig set'\n",
        ")\n",
        "\n",
        "trace1 = go.Scatter(\n",
        "    x=results['fpr_test'],\n",
        "    y=results['tpr_test'],\n",
        "    text=results['thresholds_test'],\n",
        "    mode='line',\n",
        "    name='Testing set'\n",
        ")\n",
        "\n",
        "layout = dict(\n",
        "    title='ROC Curve',\n",
        "    xaxis=dict(title='FPR'),\n",
        "    yaxis=dict(title='TPR')\n",
        ")\n",
        "\n",
        "data = [trace0,trace1]      #   this is a list because you might want to plot many data sets\n",
        "iplot(dict(data=data,layout=layout),validate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m5Pfby5oXn0U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# k-fold Validation\n",
        "Now let's introduce a different approach to the simple single test/train split: **k-fold** validation.\n",
        "\n",
        "The basic idea is this: \n",
        "1.   We first randomly shuffle our dataset (very important as it may be ordered).\n",
        "2.   We divide our dataset up into \"k\" approximately even subsamples, or folds. \n",
        "3.   We iterate over each fold:\n",
        "      *   We use that fold as our **testing** sample, and the remaining (k-1) subsamples are combined and used as the training sample\n",
        "      *   We fit our model on the training sample and evaluate it on the testing sample as usual\n",
        "      *   We store the results for each fold.\n",
        "      \n",
        "Note that each sample in our dataset is used once in the testing, and k-1 times in the training."
      ]
    },
    {
      "metadata": {
        "id": "9vkPBk_yvLK3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Shuffle our dataset"
      ]
    },
    {
      "metadata": {
        "id": "arv5tVA3uuV7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "dfCombinedShuffle = shuffle(dfCombined,random_state=42)    # by setting the random state we will get reproducible results\n",
        "\n",
        "X = dfCombinedShuffle.as_matrix(columns=dfCombinedShuffle.columns[:784])\n",
        "y = dfCombinedShuffle['signal'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VoyLHMnuOnL8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create the folds\n",
        "You can do the training and folds simultaneously (this is what the book does), but it is more general to make the folds then do the training.  I prefer this approach, although the downside is that you need to store intermediate results, and then average afterwards.\n",
        "\n",
        "We will use an sklearn function called **StratifiedKFold**.   This will stratify the test and train samples based on the class labels that you give the **split** method of this function.\n",
        "\n",
        "Note that what is returned by **split** are two arrays (test and train) of **indices**.  You use these indicies to pull the actual samples and lables out of your dataset."
      ]
    },
    {
      "metadata": {
        "id": "dxWOdeudO07n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "kfolds = 5\n",
        "skf = StratifiedKFold(n_splits=kfolds)\n",
        "\n",
        "debug = False\n",
        "if debug:\n",
        "  for train_index, test_index in skf.split(X, y):\n",
        "    print()\n",
        "    print(X[test_index])\n",
        "    print(test_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tqjsuqaIcyrx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loop over the folds and fit\n",
        "Remember that the sklearn function **split** returns **indices** into the test and train datasets.   We use thes to form temporary inputs for our fitting code.   We store the intermediate results from each fold (for example to form averages)."
      ]
    },
    {
      "metadata": {
        "id": "zlJNC-fbdCQi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Get our estimator and predict\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "estimator = LinearSVC(random_state=42,dual=False,max_iter=100,tol=0.001)    # use dual=False when  n_samples > n_features which is what we have\n",
        "#\n",
        "# Cresate some vars to keep track of everything\n",
        "fpr_test_list = []\n",
        "tpr_test_list = []\n",
        "thresh_test_list = []\n",
        "avg_auc_test = 0.0\n",
        "avg_auc_train = 0.0\n",
        "numSplits = 0.0\n",
        "#\n",
        "# Also keep track of the \n",
        "#\n",
        "# Now loop\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "  print(\"Training\")\n",
        "  X_train = X[train_index]\n",
        "  y_train = y[train_index]\n",
        "  X_test = X[test_index]\n",
        "  y_test = y[test_index]\n",
        "  \n",
        "#\n",
        "# Now fit to our training set\n",
        "  results = runFitter(estimator,X_train,y_train,X_test,y_test)\n",
        "#\n",
        "# \n",
        "  fpr_test_list.append(results['fpr_test'])\n",
        "  tpr_test_list.append(results['tpr_test'])\n",
        "  thresh_test_list.append(results['thresholds_test'])\n",
        "  avg_auc_test += results['auc_test']\n",
        "  avg_auc_train += results['auc_train']\n",
        "  numSplits += 1.0\n",
        "  print(\"   Split \",numSplits,\"; test AUC \",results['auc_test'],\"; train AUC \",results['auc_train'])\n",
        "#\n",
        "avg_auc_test /= numSplits\n",
        "avg_auc_train /= numSplits\n",
        "print(\"average AUC test:  \",avg_auc_test)\n",
        "print(\"average AUC train: \",avg_auc_train)\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "napfix4me-UO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import plotly.plotly as py\n",
        "import numpy as np\n",
        "from plotly.offline import iplot\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "enable_plotly_in_cell()\n",
        "\n",
        "data = []\n",
        "for kfold in range(kfolds):\n",
        "  data.append(go.Scatter(\n",
        "                x=fpr_test_list[kfold],\n",
        "                y=tpr_test_list[kfold],\n",
        "                text=thresh_test_list[kfold],\n",
        "                mode='line',\n",
        "                name='K-fold '+str(kfold)\n",
        "                ))\n",
        "  \n",
        "  \n",
        "\n",
        "layout = dict(\n",
        "    title='ROC Curve for Folds of Test Data',\n",
        "    xaxis=dict(title='FPR'),\n",
        "    yaxis=dict(title='TPR')\n",
        ")\n",
        "iplot(dict(data=data,layout=layout),validate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vmn_UVib-SVf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# What is the primary purpose and use of k-fold validation?\n",
        "This artcle explains it best:  \n",
        "https://machinelearningmastery.com/train-final-machine-learning-model/\n",
        "\n",
        "I will summarize the main points of why cross-fold validation is used:\n",
        "\n",
        "* It is a method used to estimate the performance of a proposed model on unseen data.\n",
        "* By creating multiple models and testing them on multuple subsets of the data, we can obtain the mean and standard deviation of a performance metric (such  as AUC, or recall), and these can be used to infer the confidence interval on the expected performance of your final model on unseen data.\n",
        "* You can use this same procedure to compare **different** parameter choices for the same underlying model (such as how many layers in an Artificial Neural Network).   The basic idea is this:\n",
        "    * You loop over a set of choices of parameter settings\n",
        "    * For each choice, you run k-fold validation.   Store the mean and standard deviation of your performance metric.\n",
        "    * Choose that parameter setting (or combination of settings) which yield(s) the best performance metric.\n",
        "* You can use this same procedure to compare **different proposed models** (such as a SVM vs a Decision Tree).\n",
        "\n",
        "# What is the \"Final\" model?\n",
        "Generally it is **not** one of the models from the k-fold procedure.   Once you are are satisfied you have chosen the best model, and best settings for that model, you discard all of the intermediate models.   You run your chosen model over **ALL** of the data.   It is this **final** model which you then run on new unseen data.\n"
      ]
    },
    {
      "metadata": {
        "id": "d5sy6dUBJrp9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Example of using cross validation to guide model-making decisions\n",
        "\n",
        "Usually, when given the opportunity, you use **ALL** of the data to fit your model.   But what if it was very costly (in time or money) to run all of your data - it might be helpful to know that only a subset of data is needed for the accuracy you require.   Or maybe you have a small amount of real data, and you are using simulated data to determine how much real data you will need to collect.  In these circumstances you would like to be able to compare your model's performce under these different circumstatnces.\n",
        "\n",
        "We start with 2000 total samples (approximately evenly distributed between signal and background), and increase the data we use in increments, until we have all of the data available (again such that we have equal amounts of signal and background).   How do the performance measures of AUC, recall, and precision, vary as we do this?\n",
        "\n",
        "**NOTE**: This section must be done with the full data - not the \"short\" data sample!"
      ]
    },
    {
      "metadata": {
        "id": "a-vQAUoHbfYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(len(dfCombined))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vuO63DpbEqf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "#\n",
        "# Choose data set sizes we will loop over\n",
        "dataSizes = [2000,4000,6000,8000,10000,12626]\n",
        "#\n",
        "# Define our estimator\n",
        "estimator = LinearSVC(random_state=42,dual=False,max_iter=200,tol=0.001)    # use dual=False when  n_samples > n_features which is what we have\n",
        "\n",
        "#\n",
        "# Define the folds\n",
        "kfolds = 5\n",
        "skf = StratifiedKFold(n_splits=kfolds)\n",
        "\n",
        "#\n",
        "# Lists for storage of intermediate results\n",
        "auc_test_list = []\n",
        "auc_train_list = []\n",
        "rec_test_list = []\n",
        "rec_train_list = []\n",
        "pre_test_list = []\n",
        "pre_train_list = []\n",
        "\n",
        "#\n",
        "# Loop over datasets\n",
        "for dataSize in dataSizes:\n",
        "  print(\"Running data size \",dataSize)\n",
        "#\n",
        "# Grab the first \"dataSise\" rows\n",
        "  dfUse = dfCombinedShuffle.head(dataSize)\n",
        "\n",
        "  X = dfUse.as_matrix(columns=dfUse.columns[:784])\n",
        "  y = dfUse['signal'].values\n",
        "#\n",
        "# Cresate some vars to keep track of everything\n",
        "  avg_auc_test = 0.0\n",
        "  avg_auc_train = 0.0\n",
        "  avg_precision_test = 0.0\n",
        "  avg_precision_train = 0.0  \n",
        "  avg_recall_test = 0.0\n",
        "  avg_recall_train = 0.0\n",
        "  numSplits = 0.0\n",
        "#\n",
        "# Now loop over the folds\n",
        "  for train_index, test_index in skf.split(X, y):\n",
        "    X_train = X[train_index]\n",
        "    y_train = y[train_index]\n",
        "    X_test = X[test_index]\n",
        "    y_test = y[test_index]\n",
        "    print(\"size \",len(X_train))\n",
        "  \n",
        "#\n",
        "# Now fit to our training set\n",
        "    results = runFitter(estimator,X_train,y_train,X_test,y_test)\n",
        "#\n",
        "# \n",
        "    avg_auc_test += results['auc_test']\n",
        "    avg_auc_train += results['auc_train']\n",
        "    \n",
        "    avg_precision_test += results['precision_test']\n",
        "    avg_precision_train += results['precision_train']\n",
        "    \n",
        "    avg_recall_test += results['recall_test']\n",
        "    avg_recall_train += results['recall_train']\n",
        "    \n",
        "    numSplits += 1.0\n",
        "    print(\"   Split \",numSplits,\"; this AUC \",results['auc_test'],results['auc_train'])\n",
        "#\n",
        "  avg_auc_test /= numSplits\n",
        "  avg_auc_train /= numSplits\n",
        "  avg_precision_test /= numSplits\n",
        "  avg_precision_train /= numSplits\n",
        "  avg_recall_test /= numSplits\n",
        "  avg_recall_train /= numSplits\n",
        "\n",
        "  auc_test_list.append(avg_auc_test)\n",
        "  auc_train_list.append(avg_auc_train)\n",
        "  rec_test_list.append(avg_recall_test)\n",
        "  rec_train_list.append(avg_recall_train)\n",
        "  pre_test_list.append(avg_precision_test)\n",
        "  pre_train_list.append(avg_precision_train)\n",
        "\n",
        "  \n",
        "  print(\"   test/train auc\",avg_auc_test,avg_auc_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2LXsB5pod0ux",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for dataSize,aucTrain,aucTest,recTrain,recTest,preTrain,preTest in zip(dataSizes,auc_train_list,auc_test_list,rec_train_list,rec_test_list,pre_train_list,pre_test_list ):\n",
        "  print(round(dataSize,3),round(aucTrain,3),round(aucTest,3),round(recTrain,3),round(recTest,3),round(preTrain,3),round(preTest,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R8R90BD3YISC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Performance plot using AUC"
      ]
    },
    {
      "metadata": {
        "id": "tc6z-VRnYLBj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import plotly.plotly as py\n",
        "import numpy as np\n",
        "from plotly.offline import iplot\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "enable_plotly_in_cell()\n",
        "\n",
        "# Create a trace\n",
        "trace1 = go.Scatter(\n",
        "    x = dataSizes,\n",
        "    y = auc_train_list,\n",
        "    mode = 'line',\n",
        "    name = \"AUC Train\"\n",
        ")\n",
        "# Create a trace\n",
        "trace2 = go.Scatter(\n",
        "    x = dataSizes,\n",
        "    y = auc_test_list,\n",
        "    mode = 'line',\n",
        "    name = \"AUC Test\"\n",
        ")\n",
        "\n",
        "data = [trace1,trace2]\n",
        "layout = dict(\n",
        "    title='AUC vs Data Set Size',\n",
        "    xaxis=dict(title='Data Set Size'),\n",
        "    yaxis=dict(title='AUC')\n",
        ")\n",
        "iplot(dict(data=data,layout=layout),validate=False)\n",
        "\n",
        "# Plot and embed in ipython notebook!\n",
        "iplot(data, filename='basic-scatter')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B51FAArmLfGy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Performance plot using Recall"
      ]
    },
    {
      "metadata": {
        "id": "96j7HApXLjAL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import plotly.plotly as py\n",
        "import numpy as np\n",
        "from plotly.offline import iplot\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "enable_plotly_in_cell()\n",
        "# Create a trace\n",
        "trace1 = go.Scatter(\n",
        "    x = dataSizes,\n",
        "    y = rec_train_list,\n",
        "    mode = 'line',\n",
        "    name = \"Recall Train\"\n",
        ")\n",
        "# Create a trace\n",
        "trace2 = go.Scatter(\n",
        "    x = dataSizes,\n",
        "    y = rec_test_list,\n",
        "    mode = 'line',\n",
        "    name = \"Recall Test\"\n",
        ")\n",
        "\n",
        "layout = dict(\n",
        "    title='Recall vs Data Set Size',\n",
        "    xaxis=dict(title='Data Set Size'),\n",
        "    yaxis=dict(title='Recall')\n",
        ")\n",
        "iplot(dict(data=data,layout=layout),validate=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nOO_lCN7fTzc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Another test: using ALL of the data for the background\n",
        "Throughout this workbook, we restricted the background so that it was the same size as the signal data sample.   Was this a good idea?  Lets use k-fold validation to check this!   We already have the result for restricting the data:"
      ]
    },
    {
      "metadata": {
        "id": "mV3k3ihBf2Vp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Data size:      \",dataSizes[-1])\n",
        "print(\"AUC test:       \",auc_test_list[-1])\n",
        "print(\"Recall test:    \",rec_test_list[-1])\n",
        "print(\"Precision test: \",pre_test_list[-1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "efQsyXdNesXm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How much data do we have?  The dataframes **dfA** and **dfB** contain all of our signal and background:"
      ]
    },
    {
      "metadata": {
        "id": "eWv_BBrWe2Ng",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Length dfA\",len(dfA))\n",
        "print(\"Length dfB\",len(dfB))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RgEb295eeD6U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using ALL of the data: an imbalanced Dataset\n",
        "Using all of the data necessarily means that we will have an imbalanced dataset: whichever digit we pick as our signal, we have 9 times as much background when using all of the other digits as our background.   But - we have alot more background to train our classifier.   Will this help?"
      ]
    },
    {
      "metadata": {
        "id": "gVTUwDVFensc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "dfB = shuffle(dfB)\n",
        "dfCombined = dfB   # this is ALL of the background\n",
        "dfCombined = pd.concat([dfCombined, dfA])\n",
        "dfUse = shuffle(dfCombined)\n",
        "\n",
        "print(\"Length \",len(dfUse))\n",
        "\n",
        "estimator = LinearSVC(random_state=42,dual=False,max_iter=200,tol=0.001)    # use dual=False when  n_samples > n_features which is what we have\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "kfolds = 5\n",
        "skf = StratifiedKFold(n_splits=kfolds)\n",
        "\n",
        "X = dfUse.as_matrix(columns=dfUse.columns[:784])\n",
        "y = dfUse['signal'].values\n",
        "#\n",
        "# Cresate some vars to keep track of everything\n",
        "avg_auc_test = 0.0\n",
        "avg_auc_train = 0.0\n",
        "avg_precision_test = 0.0\n",
        "avg_precision_train = 0.0  \n",
        "avg_recall_test = 0.0\n",
        "avg_recall_train = 0.0\n",
        "numSplits = 0.0\n",
        "\n",
        "\n",
        "#\n",
        "# Now loop\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "  X_train = X[train_index]\n",
        "  y_train = y[train_index]\n",
        "  X_test = X[test_index]\n",
        "  y_test = y[test_index]\n",
        "  print(\"size \",len(X_train))\n",
        "  \n",
        "#\n",
        "# Now fit to our training set\n",
        "  results = runFitter(estimator,X_train,y_train,X_test,y_test)\n",
        "#\n",
        "# \n",
        "  avg_auc_test += results['auc_test']\n",
        "  avg_auc_train += results['auc_train']\n",
        "    \n",
        "  avg_precision_test += results['precision_test']\n",
        "  avg_precision_train += results['precision_train']\n",
        "    \n",
        "  avg_recall_test += results['recall_test']\n",
        "  avg_recall_train += results['recall_train']\n",
        "    \n",
        "  numSplits += 1.0\n",
        "  print(\"   Split \",numSplits,\"; this AUC \",results['auc_test'],results['auc_train'])\n",
        "  print(\"   Split \",numSplits,\"; this AUC \",results['auc_test'],results['auc_train'])\n",
        "  print(\"                      ; this Rec \",results['recall_test'],results['recall_train'])\n",
        "  print(\"                      ; this Pre \",results['precision_test'],results['precision_train'])\n",
        "#\n",
        "avg_auc_test /= numSplits\n",
        "avg_auc_train /= numSplits\n",
        "avg_precision_test /= numSplits\n",
        "avg_precision_train /= numSplits\n",
        "avg_recall_test /= numSplits\n",
        "avg_recall_train /= numSplits\n",
        "\n",
        "print(\"Data size:      \",len(dfUse))\n",
        "print(\"AUC test:       \",avg_auc_test)\n",
        "print(\"Recall test:    \",avg_recall_test)\n",
        "print(\"Precision test: \",avg_precision_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AF1Lnd79g7Jb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Rebalance the data?\n",
        "Let's try something slightly different.   We will use ALL of the background (except for a small part) and ALL of one digit (again except for a small part).   We will then directly resample the single digit sample, reusing the data till it is the same size as the background.\n",
        "\n",
        "We will train and test on this new sample.   The two small portions we removed will be combined into a **validation** sample (called dfCombinedTest below) which we will use to test if this procedure is reasonable."
      ]
    },
    {
      "metadata": {
        "id": "tfaDBtoLhBxf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Length dfA\",len(dfA))\n",
        "print(\"Length dfB\",len(dfB))\n",
        "dfB = shuffle(dfB)\n",
        "dfB1 = dfB.head(62687)\n",
        "dfB2 = dfB.tail(1000)\n",
        "dfA1 = dfA.head(5313)\n",
        "dfA2 = dfA.tail(1000)\n",
        "dfCombinedNew = dfB1   # this is ALL of the background\n",
        "dfCombinedNew = pd.concat([dfCombinedNew, dfA1])\n",
        "dfCombinedNew = pd.concat([dfCombinedNew, dfA1])\n",
        "dfCombinedNew = pd.concat([dfCombinedNew, dfA1])\n",
        "dfCombinedNew = pd.concat([dfCombinedNew, dfA1])\n",
        "dfCombinedNew = pd.concat([dfCombinedNew, dfA1])\n",
        "dfCombinedNew = pd.concat([dfCombinedNew, dfA1])\n",
        "dfCombinedNew = pd.concat([dfCombinedNew, dfA1])\n",
        "dfCombinedNew = pd.concat([dfCombinedNew, dfA1])\n",
        "dfCombinedNew = pd.concat([dfCombinedNew, dfA1])\n",
        "dfUseNew = shuffle(dfCombinedNew)\n",
        "\n",
        "dfCombinedTest = dfB2\n",
        "dfCombinedTest = pd.concat([dfCombinedTest, dfA2])\n",
        "print(\"Length \",len(dfCombinedTest))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EdMR7lm0S0QT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Now use cross validation to estimate performance of the rebalanced sample"
      ]
    },
    {
      "metadata": {
        "id": "3YsgIUWmgfLF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "estimator = LinearSVC(random_state=42,dual=False,max_iter=200,tol=0.001)    # use dual=False when  n_samples > n_features which is what we have\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "kfolds = 5\n",
        "skf = StratifiedKFold(n_splits=kfolds)\n",
        "\n",
        "X = dfUseNew.as_matrix(columns=dfUseNew.columns[:784])\n",
        "y = dfUseNew['signal'].values\n",
        "#\n",
        "# Cresate some vars to keep track of everything\n",
        "avg_auc_test = 0.0\n",
        "avg_auc_train = 0.0\n",
        "avg_precision_test = 0.0\n",
        "avg_precision_train = 0.0  \n",
        "avg_recall_test = 0.0\n",
        "avg_recall_train = 0.0\n",
        "numSplits = 0.0\n",
        "\n",
        "\n",
        "#\n",
        "# Now loop\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "  X_train = X[train_index]\n",
        "  y_train = y[train_index]\n",
        "  X_test = X[test_index]\n",
        "  y_test = y[test_index]\n",
        "  print(\"size \",len(X_train))\n",
        "  \n",
        "#\n",
        "# Now fit to our training set\n",
        "  results = runFitter(estimator,X_train,y_train,X_test,y_test)\n",
        "#\n",
        "# \n",
        "  avg_auc_test += results['auc_test']\n",
        "  avg_auc_train += results['auc_train']\n",
        "    \n",
        "  avg_precision_test += results['precision_test']\n",
        "  avg_precision_train += results['precision_train']\n",
        "    \n",
        "  avg_recall_test += results['recall_test']\n",
        "  avg_recall_train += results['recall_train']\n",
        "    \n",
        "  numSplits += 1.0\n",
        "  print(\"   Split \",numSplits,\"; this AUC \",results['auc_test'],results['auc_train'])\n",
        "  print(\"   Split \",numSplits,\"; this AUC \",results['auc_test'],results['auc_train'])\n",
        "  print(\"                      ; this Rec \",results['recall_test'],results['recall_train'])\n",
        "  print(\"                      ; this Pre \",results['precision_test'],results['precision_train'])\n",
        "#\n",
        "avg_auc_test /= numSplits\n",
        "avg_auc_train /= numSplits\n",
        "avg_precision_test /= numSplits\n",
        "avg_precision_train /= numSplits\n",
        "avg_recall_test /= numSplits\n",
        "avg_recall_train /= numSplits\n",
        "\n",
        "print(\"Data size:      \",len(dfUseNew))\n",
        "print(\"AUC test:       \",avg_auc_test)\n",
        "print(\"Recall test:    \",avg_recall_test)\n",
        "print(\"Precision test: \",avg_precision_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WpjBD3-bSobB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train using the full rebalanced sample, and test using the validation sample"
      ]
    },
    {
      "metadata": {
        "id": "hOhTaZLHpsHC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Get our estimator and predict\n",
        "from sklearn.svm import LinearSVC\n",
        "estimator = LinearSVC(random_state=42,dual=False,max_iter=200,tol=0.001)    # use dual=False when  n_samples > n_features which is what we have\n",
        "\n",
        "\n",
        "\n",
        "X_train = dfCombinedNew.as_matrix(columns=dfCombinedNew.columns[:784])\n",
        "y_train = dfCombinedNew['signal'].values\n",
        "X_test = dfCombinedTest.as_matrix(columns=dfCombinedTest.columns[:784])\n",
        "y_test = dfCombinedTest['signal'].values\n",
        "#\n",
        "  \n",
        "#\n",
        "# Now fit to our training set\n",
        "results = runFitter(estimator,X_train,y_train,X_test,y_test)\n",
        "\n",
        "print(\"Results using full sample and held-out sample: \")\n",
        "print(\"AUC test:       \",results['auc_test'])\n",
        "print(\"Recall test:    \",results['recall_test'])\n",
        "print(\"Precision test: \",results['precision_test'])\n",
        "print(\"AUC train:       \",results['auc_train'])\n",
        "print(\"Recall train:    \",results['recall_train'])\n",
        "print(\"Precision train: \",results['precision_train'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}