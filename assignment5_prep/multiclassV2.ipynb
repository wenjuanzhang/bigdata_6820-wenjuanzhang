{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiclassV2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "wq-vnlibfgoA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Multiclass Classification\n",
        "Multiclass classification is a generalization of binary classification, where instead of 2 classes, we have 3 or more.  It turns out that with the sklearn methods we have been using, it is **extremely simple** to extend our procedures from binary to multiclass classification.\n",
        "\n",
        "One thing that is tricky however - coming up with a performance metric for our proposed model.   Our preferred metric - AUC - does not have a simple analog, since the ROC curve from which it is derived is explicitly defined for a binary classifier.\n",
        "\n",
        "Instead, we will use metrics derived from the **confusion matrix**, including versions of recall and precision."
      ]
    },
    {
      "metadata": {
        "id": "_bYjsGvjhBgL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classifiying All 10 digits simultaneously\n",
        "We will again use the MNIST sample, but this time our goal is develop a model which, given an image of a digit from 0-9, will predict which digit best corresponds to that true digit.   \n",
        "\n",
        "So let's begin by reading in all of our data.   Start with the **short** sample since the full sample takes quite a bit of time to run."
      ]
    },
    {
      "metadata": {
        "id": "vAEpl6Cg7ovV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#\n",
        "# Define our \"signal\" digit\n",
        "#short = \"\"\n",
        "short = \"short_\"\n",
        "\n",
        "#\n",
        "# Read in all of the other digits\n",
        "dfCombined = pd.DataFrame()\n",
        "for digit in range(10):\n",
        "    print(\"Processing digit \",digit)\n",
        "    fname = 'https://raw.githubusercontent.com/big-data-analytics-physics/data/master/ch3/digit_' + short + str(digit) + '.csv'\n",
        "    df = pd.read_csv(fname,header=None)\n",
        "    df['digit'] = digit\n",
        "    dfCombined = pd.concat([dfCombined, df])\n",
        "\n",
        "print(\"Length of sample:     \",len(dfCombined))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O0m2F2yCZiK6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "p0ffLJ4ArMNJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Multi-class performance\n",
        "\n",
        "A good discussion of multiclass performancecan be found here:  https://medium.com/usf-msds/choosing-the-right-metric-for-evaluating-machine-learning-models-part-2-86d5649a5428\n",
        "\n",
        "In the code below, we implement two \"accuracy\" measures.   In our assignment later, we will add: # To be added later:\n",
        "* \"macro\" averaged recall = recall averaged over each class\n",
        "            recall for a single class = Number of items correctly identified as positive out of total true positives for that class: TP/(TP+FN)\n",
        "\n",
        "*    \"macro\" averaged precision = precision averaged over each class\n",
        "            precision for a single class = Number of items correctly identified as positive out of total items identified as positive: TP/(TP+FP)\n"
      ]
    },
    {
      "metadata": {
        "id": "l7nY84AAZmYQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Used to implement the multi-dimensional counter we need in the performance class\n",
        "from collections import defaultdict\n",
        "def autovivify(levels=1, final=dict):\n",
        "    return (defaultdict(final) if levels < 2 else\n",
        "            defaultdict(lambda: autovivify(levels-1, final)))\n",
        "\n",
        "#\n",
        "# Determine the performance\n",
        "def multiPerformance(y,y_pred,y_score,debug=False):\n",
        "#\n",
        "# Make our matrix\n",
        "  confusionMatrix = autovivify(2,int)\n",
        "  classes = set()\n",
        "  totalTrue = defaultdict(int)\n",
        "  totalPred = defaultdict(int)\n",
        "  for i in range(len(y_pred)):\n",
        "    trueClass = y[i]\n",
        "    classes.add(trueClass)\n",
        "    predClass = y_pred[i]\n",
        "    totalTrue[trueClass] += 1\n",
        "    totalPred[predClass] += 1\n",
        "    confusionMatrix[trueClass][predClass] += 1\n",
        "\n",
        "  if debug:\n",
        "    for trueClass in classes:\n",
        "      print(\"True: \",trueClass,end=\"\")\n",
        "      for predClass in classes:\n",
        "        print(\"\\t\",confusionMatrix[trueClass][predClass],end=\"\")\n",
        "      print()\n",
        "    print()\n",
        "#\n",
        "#\n",
        "# Overall accuracy - sum the diagonals and divide by total\n",
        "  accMicro = 0.0\n",
        "  accMacro = 0.0\n",
        "  for cl in classes:\n",
        "    accMicro += confusionMatrix[cl][cl]\n",
        "    accMacro += confusionMatrix[cl][cl]/totalTrue[cl]\n",
        "  accMicro /= len(y)\n",
        "  accMacro = accMacro / len(classes)\n",
        "  results = {\"confusionMatrix\":confusionMatrix,\"accuracyMicro\":accMicro,\"accuracyMacro\":accMacro}\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Aci5isTPo_B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## runFitter Method\n",
        "The runFitter method is pretty similar to what we had before.  The primary change is how the performance method is called, as well as the returned data from that method."
      ]
    },
    {
      "metadata": {
        "id": "K0JnSzY5Zn08",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def runFitter(estimator,X_train,y_train,X_test,y_test,debug=False):\n",
        "#\n",
        "# Now fit to our training set\n",
        "  estimator.fit(X_train,y_train)\n",
        "#\n",
        "# Now predict the classes and get the score for our traing set\n",
        "  y_train_pred = estimator.predict(X_train)\n",
        "  y_train_score = estimator.decision_function(X_train)   # NOTE: some estimators have a predict_prob method instead od descision_function\n",
        "#\n",
        "# Now predict the classes and get the score for our test set\n",
        "  y_test_pred = estimator.predict(X_test)\n",
        "  y_test_score = estimator.decision_function(X_test)\n",
        "\n",
        "#\n",
        "# Now get the performaance\n",
        "  results_test = multiPerformance(y_test,y_test_pred,y_test_score,debug=False)\n",
        "  results_train = multiPerformance(y_train,y_train_pred,y_train_score,debug=False)\n",
        "#\n",
        "# Decide what you want to return: for now, just precision, recall, and auc for both test and train\n",
        "  results = {\n",
        "      'cf_test':results_test['confusionMatrix'],\n",
        "      'cf_train':results_train['confusionMatrix'],\n",
        "      'accuracyMicro_test':results_test['accuracyMicro'],\n",
        "      'accuracyMacro_test':results_test['accuracyMacro'],\n",
        "      'accuracyMicro_train':results_train['accuracyMicro'],\n",
        "      'accuracyMacro_train':results_train['accuracyMacro'],\n",
        "}\n",
        "\n",
        "  return results\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "43bs7n91P4GC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Shuffle the data\n",
        "We must shuffle the data, since the data is in digit order when we read it in."
      ]
    },
    {
      "metadata": {
        "id": "kr-l5M3BZwt2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "dfCombinedShuffle = shuffle(dfCombined,random_state=42)    # by setting the random state we will get reproducible results\n",
        "\n",
        "X = dfCombinedShuffle.as_matrix(columns=dfCombinedShuffle.columns[:784])\n",
        "y = dfCombinedShuffle['digit'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p5V10zI6QHSb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up kfolds"
      ]
    },
    {
      "metadata": {
        "id": "r3QP0DS0Z0jz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "kfolds = 5\n",
        "\n",
        "#skf = StratifiedKFold(n_splits=kfolds)\n",
        "skf = KFold(n_splits=kfolds)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IhvSkkQ0QKg5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loop over folds\n",
        "Here we loop over the folds and calculate the statistics."
      ]
    },
    {
      "metadata": {
        "id": "5i_h-0GHZ9XW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Get our estimator and predict\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "estimator = LinearSVC(random_state=42,dual=False,max_iter=500,tol=0.01)    # use dual=False when  n_samples > n_features which is what we have\n",
        "#estimator = SGDClassifier(random_state=42,max_iter=500,tol=0.01)    # use dual=False when  n_samples > n_features which is what we have\n",
        "#\n",
        "# Cresate some vars to keep track of everything\n",
        "avg_accuracyMicro_test = 0.0\n",
        "avg_accuracyMicro_train = 0.0\n",
        "avg_accuracyMacro_test = 0.0\n",
        "avg_accuracyMacro_train = 0.0\n",
        "numSplits = 0.0\n",
        "#\n",
        "# Also keep track of the \n",
        "#\n",
        "# Now loop\n",
        "lastCF_train = None\n",
        "lastCF_test = None\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "  print(\"Training\")\n",
        "  X_train = X[train_index]\n",
        "  y_train = y[train_index]\n",
        "  X_test = X[test_index]\n",
        "  y_test = y[test_index]\n",
        "  \n",
        "#\n",
        "# Now fit to our training set\n",
        "  results = runFitter(estimator,X_train,y_train,X_test,y_test)\n",
        "#\n",
        "# \n",
        "  avg_accuracyMicro_test += results['accuracyMicro_test']\n",
        "  avg_accuracyMicro_train += results['accuracyMicro_train']\n",
        "  avg_accuracyMacro_test += results['accuracyMacro_test']\n",
        "  avg_accuracyMacro_train += results['accuracyMacro_train']\n",
        "  lastCF_train = results['cf_train']\n",
        "  lastCF_test = results['cf_test']\n",
        "  numSplits += 1.0\n",
        "  print(\"   Split \",numSplits,\"; accuracyMicro test/train\",results['accuracyMicro_test'],results['accuracyMicro_train'],\"; accuracyMacro test/train\",results['accuracyMacro_test'],results['accuracyMacro_train'])\n",
        "#\n",
        "avg_accuracyMicro_test /= numSplits\n",
        "avg_accuracyMicro_train /= numSplits\n",
        "avg_accuracyMacro_test /= numSplits\n",
        "avg_accuracyMacro_train /= numSplits\n",
        "print(\"average accuracyMicro test:  \",avg_accuracyMicro_test)\n",
        "print(\"average accuracyMicro train: \",avg_accuracyMicro_train)\n",
        "print(\"average accuracyMacro test:  \",avg_accuracyMacro_test)\n",
        "print(\"average accuracyMacro train: \",avg_accuracyMacro_train)\n",
        "print(\"Test confusion matrix\")\n",
        "for trueClass in range(10):\n",
        "  print(\"True: \",trueClass,end=\"\")\n",
        "  for predClass in range(10):\n",
        "    print(\"\\t\",lastCF_test[trueClass][predClass],end=\"\")\n",
        "  print()\n",
        "print()\n",
        "print(\"Train confusion matrix\")\n",
        "for trueClass in range(10):\n",
        "  print(\"True: \",trueClass,end=\"\")\n",
        "  for predClass in range(10):\n",
        "    print(\"\\t\",lastCF_train[trueClass][predClass],end=\"\")\n",
        "  print()\n",
        "print()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVfzoNtcQgjF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Examine the results\n",
        "When you run this using the full data sample, it is instructive to examine which digits are misclassified.   Also, note that the matrix is not symmetric, though it is nearly so."
      ]
    }
  ]
}