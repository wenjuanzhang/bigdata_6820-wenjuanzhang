{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlS3r8_lPvRU"
   },
   "source": [
    "# Softmax Regression\n",
    "Softmax regression is a generalization of Logistic Regression, where instead of **two** possible classes, our problem allows for more than two, or what is called **multi-class** classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DRuzmJuzUglq"
   },
   "source": [
    "# Logistic Regression\n",
    "In Logistic Regression, our challenge was **binary**: we need to distinguish between two  classes.   \n",
    "\n",
    "We have as our inputs and outputs:\n",
    "* A set of \"m\" samples, each a vector of \"n\" input features: \n",
    "$$X_i = {x_{1i},x_{2i},x_{3i},...,x_{ni}}$$\n",
    "* A set of corresponding \"m\" outputs, each either 1 (postive/signal class) or 0 (negative/background class)\n",
    "$$y_i = 0 ~or ~1 $$\n",
    "\n",
    "In this case, we defined the output of our classifer as:\n",
    "$$h_\\theta(X)=g(X\\theta)= {1\\over{1+e^{-X\\theta}}}$$\n",
    "\n",
    "and we said that for a given set of features for one sample,  $h_\\theta(X)$ is the probability that y=1 for that specific set of features.   \n",
    "\n",
    "\n",
    "We defined this cost function $J(\\theta)$:\n",
    " $$J(\\theta) = \\sum_{i=i}^m [-y\\log(h_\\theta(X)) - (1-y)\\log(1-h_\\theta(X))]$$\n",
    " \n",
    " And the gradient of the cost function with respect to $\\theta$:\n",
    " $${\\delta J\\over \\delta \\theta_j} = \\sum_{i=i}^m(h_\\theta(X^{(i)})  -y^{(i)})\\cdot X^{(i)}$$\n",
    " \n",
    " \n",
    "The goal of logistic regression is to choose the parameters $\\theta$ so that our predictions $h_\\theta(X)$  are as close to our sample classes *y* as possible, by minimizing the cost function $J(\\theta)$.\n",
    "\n",
    "This is illustrated with the figure below.   In this case, we assume we have 3 input features and 1 output.   We end up with **4** $\\theta$ values.\n",
    "\n",
    "![alt text](https://github.com/big-data-analytics-physics/data/blob/master/images/logistic_classification.jpg?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgsUylMbZp58"
   },
   "source": [
    "# Softmax Regression\n",
    "In Softmax Regression, we need to distinguish between more than two  classes.   \n",
    "This is illustrated with the figure below.   In this case, we assume we have 3 input features and 3 output classes.  Remember that the output in our data sample are 1-hot: only 1 output is true at a time.   We end up with **12** $\\theta$ values: 4 for each of the outputs\n",
    "\n",
    "![alt text](https://github.com/big-data-analytics-physics/data/blob/master/images/softmax_classifcation.jpg?raw=true)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We have as our inputs and outputs:\n",
    "* A set of \"m\" samples, each a vector of \"n\" input features: \n",
    "$$X_i = {x_{1i},x_{2i},x_{3i},...,x_{ni}}$$\n",
    "In the figure above, n=3.\n",
    "* A set of corresponding \"m\" output vectors of length k, where only one of the k values is 1.0, and the other are 0.0 (this is the one-hot encoding):\n",
    "$$y_i = {y_{0i},y_{1i},y_{2i},...,y_{(k-1)i},}$$\n",
    "In the Figure above, k=3\n",
    "\n",
    "In this case, our classifier has \"k\" outputs, each of the form:\n",
    "$$p_k = {{e^{X\\theta_k}}\\over{\\sum_{i=0}^{i=k-1}e^{X\\theta_k}}}$$\n",
    "\n",
    "where $p_k$ is the probability that the class=k for that specific set of features.   \n",
    "\n",
    "\n",
    "We will define our cost function $J(\\theta)$:\n",
    "$$J(\\theta) = -{1\\over{m}}\\sum_{i=1}^m \\sum_{j=0}^{k-1}1[y^{(i)}=j] ~log{e^{\\theta_j X}\\over{\\sum_{\\ell=0}^k}e^{\\theta_\\ell X}}$$\n",
    "Note that the term $1[y^{(i)}=j]$ equals 1 when the true output $y^{i}=1$ for the specific output class j.\n",
    "\n",
    " And the gradient of the cost function with respect to $\\theta$ for a single output $j$ is:\n",
    " $${\\delta J\\over{\\partial \\theta_j}} =X \\left( 1-{e^{\\theta_j X}\\over{\\sum_{\\ell=0}^k e^{\\theta_\\ell X} }} \\right) $$\n",
    " Note that this is a vector of length (n+1) (for the n features plus the $\\theta_0$ term), and note also that there are \"k\" of these vectors (one for each output).\n",
    "\n",
    "We end up with **(n+1)*k** $\\theta$ values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNc2-bZxoUVa"
   },
   "source": [
    "## Implementation of the softmax function\n",
    "We want to implement this:\n",
    "\n",
    "$$p_k = {{e^{X\\theta_k}}\\over{\\sum_{i=0}^{i=k-1}e^{X\\theta_k}}}$$\n",
    "\n",
    "where pt_k$ is the probability that the class=k for that specific set of features.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cenyA7SxZkNj"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# softmax regression\n",
    "def softmax(Theta,Xp):\n",
    "  # data has m input rows, n features, k outputs\n",
    "  \n",
    "  # assume Theta is (n+1) by k matrix\n",
    "  # assume Xp is an m by (n+1) matrix\n",
    "  z = np.dot(Xp,Theta)   # this is now an m by k matrix\n",
    "  z -= np.max(z)         # get the max and subtract, helps with big numbers \n",
    "                         # see: https://stats.stackexchange.com/questions/304758/softmax-overflow\n",
    "  res = np.exp(z) / np.sum(np.exp(z),axis=1)[:,np.newaxis]\n",
    "  return res  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yP480PMGoebK"
   },
   "source": [
    "## Next we implement the cost for softmax\n",
    "\n",
    "Our cost function again $J(\\theta)$:\n",
    "$$J(\\theta) = -{1\\over{m}}\\sum_{i=1}^m \\sum_{j=0}^{k-1}1 \\left\\{ y^{(i)}=j \\right\\} ~log{e^{\\theta_j X}\\over{\\sum_{\\ell=0}^k}e^{\\theta_\\ell X}}$$\n",
    "Note that the term $1\\left\\{y^{(i)}=j\\right\\}$ equals 1 when the true output $y^{i}=1$ for the specific output class j.\n",
    "\n",
    "The function we will actually implement below has an additional term for **regularization**.   We initially set this to be 0.\n",
    "$$J(\\theta) = -{1\\over{m}}\\sum_{i=1}^m \\sum_{j=0}^{k-1}1 \\left\\{ y^{(i)}=j \\right\\} ~log{e^{\\theta_j X}\\over{\\sum_{\\ell=0}^{k-1}e^{\\theta_\\ell X}}} +{\\lambda\\over{2}} \\sum_{\\ell=0}^{k-1}\\theta_\\ell ^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDnP0UgOpM-w"
   },
   "outputs": [],
   "source": [
    "def calc_cost_softmax(Theta,Xp,yp_oneHot,Lambda):\n",
    "  m = Xp.shape[0] #First we get the number of training examples\n",
    "  probs = softmax(Theta,Xp)\n",
    "  cost = (-1 / m) * np.sum(yp_oneHot * np.log(probs)) + (Lambda/2.0)*np.sum(np.square(Theta))\n",
    "  return cost,grad \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EuNRKfh2oyN3"
   },
   "source": [
    "## Next we implement the gradient for softmax\n",
    "Our gradient of the cost function with respect to $\\theta$ for a single output $j$:\n",
    " $${\\delta J\\over{\\partial \\theta_j}} =X \\left( 1-{e^{\\theta_j X}\\over{\\sum_{\\ell=0}^k e^{\\theta_\\ell X} }} \\right) $$\n",
    " Note that this is a vector of length (n+1) (for the n features plus the $\\theta_0j$ term), and note also that there are \"k\" of these vectorss (one for each output).\n",
    " \n",
    " Including the term for regularization, we get:\n",
    " $${\\delta J\\over{\\partial \\theta_j}} =X \\left( 1-{e^{\\theta_j X}\\over{\\sum_{\\ell=0}^k e^{\\theta_\\ell X} }} \\right) + \\lambda\\theta_j$$\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygK5CADvpHYa"
   },
   "outputs": [],
   "source": [
    "def calc_gradient_softmax(Theta,Xp,yp_oneHot,Lambda):\n",
    "  m = Xp.shape[0] #First we get the number of training examples\n",
    "  probs = softmax(Theta,Xp)\n",
    "  grad = (-1 / m) * np.dot(Xp.T,(yp_oneHot - probs)) + Lambda*Theta #And compute the gradient for that loss\n",
    "  return cost,grad  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6516iVJVo3QB"
   },
   "source": [
    "## Combine gradient and cost\n",
    "Since in both routines we calculate the probabilities for all of our samples, it makes no sense to do this twice, so lets combine both functions into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEoYa1kkofSh"
   },
   "outputs": [],
   "source": [
    "def calc_cost_and_gradient_softmax(Theta,Xp,yp_oneHot,Lambda):\n",
    "  m = Xp.shape[0] #First we get the number of training examples\n",
    "  probs = softmax(Theta,Xp)\n",
    "  cost = (-1 / m) * np.sum(yp_oneHot * np.log(probs)) + (Lambda/2.0)*np.sum(np.square(Theta))\n",
    "  grad = (-1 / m) * np.dot(Xp.T,(yp_oneHot - probs)) + Lambda*Theta #And compute the gradient for that loss\n",
    "  return cost,grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OMcdoQMD6Bww"
   },
   "source": [
    "## Iterating until we converge\n",
    "The basic algorithm then to implement gradient descent looks like this:\n",
    "1. Initialize each of the $\\theta$ parameters to some reasonable value (0 is common, or a random number).   Remember\n",
    "  *  We have an axis that is of length (n features + 1)\n",
    "  * A separate axis of length (k) outputs\n",
    "2. Choose a learning rate $\\alpha$, maxmimum allowed iterations, and a precision for the cost decrease to reach.   We will leave $Lambda$ as 0.0.\n",
    "3. Have an outer loop that checks that we have not exceeded our maximum number of allowed iterations **AND** that the cost is still decreasing.\n",
    "4. Calculate the gradient and update our parameters like so:\n",
    "$$\\theta_j := \\theta_j - \\alpha {\\partial J\\over \\partial \\theta_j}(\\theta)$$\n",
    "5. Calculate the cost for this iteration and compare it to the cost of the previous iteration.\n",
    "6. If the change in cost is small enough (below our chosen precision), declare victory and jump out of the loop.\n",
    "\n",
    "It is helpful to keep track of the cost for each iteration, so you can plot it and inspect its behavior.   And of course you need to keep track of the last value of the $\\theta$ parameters so you can return them.\n",
    "\n",
    "An implementation of this iteration algorithm is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SvDbsHmC6Icg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def fit_data(Xp,yp_oneHot,learningRate,max_iterations,scale=True,delta=0.001,Lambda=0.0,iterations_min=2):\n",
    "#\n",
    "# Get the initial values\n",
    "  m,features = Xp.shape   # this has the true \"n\" features +1 for the \"ones\" column\n",
    "#\n",
    "# How many outputs do we have\n",
    "  m,outputs = yp_oneHot.shape\n",
    "#\n",
    "# Set the starting theta values\n",
    "  Theta = np.zeros((features,outputs))\n",
    "  print(\"Starting theta\",Theta.shape)\n",
    "  costList = []\n",
    "#\n",
    "# Calculate our initial cost\n",
    "  cost,grad = calc_cost_and_gradient_softmax(Theta,Xp,yp_oneHot,Lambda)\n",
    "  cost_change = delta+0.1\n",
    "  cost = 1000000\n",
    "  iterations = 0\n",
    "#\n",
    "# In the while loop, \"delta\" is the precision\n",
    "  while (iterations<iterations_max) and (cost_change>delta):\n",
    "    last_cost = cost\n",
    "#\n",
    "# Get the cost and gradient\n",
    "    cost,grad = calc_cost_and_gradient_softmax(Theta,Xp,yp_oneHot,Lambda)\n",
    "    #print(\"cost,grad \",cost,grad)\n",
    "#\n",
    "# Update the theta parameters\n",
    "    Theta = Theta - learningRate*grad\n",
    "#\n",
    "# Calculate the cost change\n",
    "    cost_change = last_cost - cost\n",
    "#\n",
    "# Store the cost\n",
    "    costList.append(cost)\n",
    "    iterations += 1\n",
    "    \n",
    "  return Theta,iterations,costList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rc9S8Q0I6ODH"
   },
   "source": [
    "## Get the Data\n",
    "We will use the MNIST data sample to test our softmax regression algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZLFjGuuzwYZH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Form our test and train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#short = \"\"\n",
    "short = \"short_\"\n",
    "dfCombined = pd.DataFrame()\n",
    "#\n",
    "# Read in digits\n",
    "for digit in range(10):\n",
    "  print(\"digit\",digit)\n",
    "  fname = 'https://raw.githubusercontent.com/big-data-analytics-physics/data/master/ch3/digit_' + short + str(digit) + '.csv'\n",
    "  df = pd.read_csv(fname,header=None)\n",
    "  df['digit'] = digit\n",
    "  dfCombined = pd.concat([dfCombined, df])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yl76vzLqa74"
   },
   "source": [
    "## Make Separate Test and Train Samples\n",
    "We will do a simple 70/30 split to form our Train/Test sample.\n",
    "\n",
    "We also need to:\n",
    "* Scale the input data.   Since we know the input pixel data goes from 0-255, we can just divide by 255.\n",
    "* Add the ones column to the input features.\n",
    "* Convert our output labels to 1-hot.   We will use a **keras** utility for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiEFNaYKwlHo"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "\n",
    "train_digits,test_digits = train_test_split(dfCombined, test_size=0.3, random_state=42)\n",
    "yTrain = train_digits['digit'].values\n",
    "XTrain = train_digits.as_matrix(columns=train_digits.columns[:784])\n",
    "\n",
    "yTest = test_digits['digit'].values\n",
    "XTest = test_digits.as_matrix(columns=test_digits.columns[:784])\n",
    "\n",
    "#\n",
    "# one hot encode the labels\n",
    "num_classes = len(np.unique(yTrain))\n",
    "print(\"Number distinct classes \",num_classes)\n",
    "yTrain_oneHot = to_categorical(yTrain, num_classes=num_classes)\n",
    "yTest_oneHot = to_categorical(yTest, num_classes=num_classes)\n",
    "for i in range(10):\n",
    "  print(\"digit \",yTrain[i],\"encoding\",yTrain_oneHot[i])\n",
    "  \n",
    "#\n",
    "# We need to normalize our data - just divide by 256!\n",
    "XTrain = XTrain/255.0\n",
    "XTest = XTest / 255.0\n",
    "#\n",
    "# Add the ones column to the test and train sets\n",
    "ones = np.ones((len(XTrain),1))\n",
    "XTrain = np.append(ones,XTrain,axis=1)\n",
    "ones = np.ones((len(XTest),1))\n",
    "XTest = np.append(ones,XTest,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9SIfjAjxJd6"
   },
   "outputs": [],
   "source": [
    "iterations_max = 100\n",
    "iterations_min = 50\n",
    "learningRate = 0.1\n",
    "delta = 0.0001\n",
    "Theta,iterations,costList = fit_data(XTrain,yTrain_oneHot,learningRate,iterations_max)\n",
    "print(\"Iterations \",iterations)\n",
    "print(\"Cost:\",costList[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P2GcTegGxktm"
   },
   "outputs": [],
   "source": [
    "def getProbsAndPreds(Theta,someX):\n",
    "    probs = softmax(Theta,someX)\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return probs,preds\n",
    "\n",
    "def getAccuracy(Theta,someX,someY):\n",
    "    prob,prede = getProbsAndPreds(Theta,someX)\n",
    "    accuracy = sum(prede == someY)/(float(len(someY)))\n",
    "    return accuracy\n",
    "  \n",
    "print('Training Accuracy: ', getAccuracy(Theta,XTrain,yTrain))\n",
    "print('Test Accuracy: ', getAccuracy(Theta,XTest,yTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "softmax_regression_intro.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
