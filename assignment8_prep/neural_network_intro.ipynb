{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlS3r8_lPvRU"
   },
   "source": [
    "# Introduction to Neural Networks\n",
    "We have thus far dealt with two types of simple classifiers:\n",
    "1.  Logistic: when we have two classes\n",
    "2.  Softmax: when we have more than two classes\n",
    "\n",
    "In this workbook, we will introduce yet another classifier - a neural network.   In fact, once can view both Logistic Regression and Softmax Regression as simple versions of neural networks:\n",
    "* They both have an input layer, where the features are presented to the algorithm.\n",
    "* They have an output layer, with 1 or more output.\n",
    "* The inputs are connected directly to the outputs via the $\\theta$ or **weight** matrix.\n",
    "\n",
    "Neural networks add another level of complexity to our classifier, by adding one or more so-called **hidden** layers between the input layer and the output layer.   \n",
    "\n",
    "A cartoon of such a network is shown below:\n",
    "![alt text](https://github.com/big-data-analytics-physics/data/blob/master/images/neural_network_model.jpg?raw=true)\n",
    "\n",
    "In fact the lower part of the neural network is exactly the same as the softmax regression that we discussed earlier:\n",
    "\n",
    "![alt text](https://github.com/big-data-analytics-physics/data/blob/master/images/softmax_classifcation.jpg?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfZeVvT94LOB"
   },
   "source": [
    "# Activation functions\n",
    "In all of the networks we have used so far, as well as the neural network we are using today, we employ **activation** functions at each of the nodes.   These activations can (and often will) be different depending on the placement of the node:\n",
    "* Input nodes: here the activation is simply **identity**: the output of the node ($A_1$ in the above graphic) is simply the input.\n",
    "* Hidden nodes: here we have a variety of possibilities, but they usually are chosen from among 3: sigmoid, tanh, and relu (rectified linear unit).  In our neural network we will use $tanh$.   See the section titled \"Commonly used activation functions\" [in this reference ](http://cs231n.github.io/neural-networks-1/) for more details.\n",
    "* Output nodes: here we typically have *sigmoid* if there is a single output, or *softmax* if there are multiple outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgsUylMbZp58"
   },
   "source": [
    "#Neural Network Outputs and Cost Function\n",
    "\n",
    "We have as our inputs and outputs:\n",
    "* A set of \"m\" samples, each a vector of \"n\" input features: \n",
    "$$X_i = {x_{1i},x_{2i},x_{3i},...,x_{ni}}$$\n",
    "* Instead of a single set of weights $\\theta$, we will have two sets of weights $W_1$ and $W_2$, which connect the input layer to the hidden layer, and the hidden layer to the output layer.\n",
    "* As with softmax, our classifier has \"k\" outputs, each of the form:\n",
    "$$p_k = {{e^{X\\theta_k}}\\over{\\sum_{i=0}^{i=k-1}e^{X\\theta_k}}}$$\n",
    "\n",
    "where $p_k$ is the probability that the class=k for that specific set of features.   \n",
    "\n",
    "\n",
    "We will define our cost function $J(W_1,W_2)$:\n",
    "$$J(W_1,W_2) = -{1\\over{m}}\\sum_{i=0}^{k-1}y_{i,c} ~log~{p_{i,c}}$$\n",
    "where\n",
    "*   $y_{i,c}$ is a binary indicator (0 or 1) if class label c is the correct classification for observation i\n",
    "*  $p_{i,c}$ - predicted probability observation i is of class c\n",
    "\n",
    "Although this might look different, this is actually just a more compact way of writing the same cost function we used for softmax resgression.   In addtion, with a little bit of algebra, we could show that this is exactly the same cost function we used for logistic regression.\n",
    "\n",
    "As with *softmax*, our goal is to find the set of weights $W_1$ and $W_2$, which minimize the cost function J.   Before we get to that, lets implement the functions necessary to perform the neural network calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I950QD887BD_"
   },
   "source": [
    "## Some activation functions and their derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g02DQoxMogpT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(z):\n",
    "  sm = 1.0 / (1.0 + np.exp(z))\n",
    "  return sm\n",
    "\n",
    "def sigmoid_deriv(z):\n",
    "  sm = sigmoid(z)*(1-sigmoid(z))\n",
    "  return sm\n",
    "\n",
    "def tanh(z):\n",
    "  return np.tanh(z)\n",
    "\n",
    "\n",
    "def tanh_deriv(z):\n",
    "  return 1 - np.square(np.tanh(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNc2-bZxoUVa"
   },
   "source": [
    "## Implementation of the softmax function\n",
    "We implement a slightly different version of this than we did previously.   Before we had:\n",
    "\n",
    "$$p_k = {{e^{X\\theta_k}}\\over{\\sum_{i=0}^{i=k-1}e^{X\\theta_k}}}$$\n",
    "\n",
    "where $p_k$ is the probability that the class=k for that specific set of features.   \n",
    "\n",
    "Instead we will use:\n",
    "$$p_k = {{e^{z}}\\over{\\sum_{i=0}^{i=k-1}e^{z}}}$$\n",
    "\n",
    "Notice in our implelmentation below, we find the max term in the exponential sum, and add it back into the numerator and denominator of the softmax expression.   You can check for yourself that this does not change the expression.   However, it does help to avoid overflow when calculating very large exponentials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cenyA7SxZkNj"
   },
   "outputs": [],
   "source": [
    "def softmaxNew(v):\n",
    "  logC = -np.max(v)\n",
    "  return np.exp(v + logC)/np.sum(np.exp(v + logC), axis = 1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yP480PMGoebK"
   },
   "source": [
    "## Next we implement the cost function\n",
    "As with softmax, the cost function we will actually implement below has an additional term for **regularization**.   We initially set this to be 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDnP0UgOpM-w"
   },
   "outputs": [],
   "source": [
    "def calc_cost(yp_oneHot,output,w1,w2,Lambda=0.0):\n",
    "  m = yp_oneHot.shape[0] #First we get the number of training examples\n",
    "  cost = (-1 / m) * np.sum(yp_oneHot * np.log(output)) + (Lambda/2.0)*(np.sum(np.square(w1[:, 1:])) + np.sum(np.square(w2[:, 1:])))\n",
    "  return cost \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EuNRKfh2oyN3"
   },
   "source": [
    "## The Forward Pass\n",
    "Sending a single instance of our feature vectors through the neural network is called a **forward pass**.   If we look at the cartoon of our network, this is actually pretty straightforward to implement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygK5CADvpHYa"
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, w1, w2):\n",
    "#\n",
    "#\n",
    "# x: input matrix, dimension features by samples\n",
    "# w1: weight matrix connecting input layer to hidden layer (takes place of earlier Theta matrix) \n",
    "#       ==> dimension hidden nodes by (features+1)\n",
    "# w2: weight matrix connecting hidden layer to output layer (takes place of earlier Theta matrix) \n",
    "#       ==> dimension output nodes by (hidden nodes+1)\n",
    "# a1: the \"output\" (also called the activation) of the input layer => just a copy of the input layer\n",
    "#     we need to add the ones column which activates the boas\n",
    "  ones = np.ones((len(x),1))\n",
    "  a1 = np.append(ones,x,axis=1)\n",
    "#\n",
    "# z2: the input to the hidden layer = weight w1 matrix applied to (input features plus bias)\n",
    "  z2 = a1.dot(w1.T)\n",
    "        #print(\"z2.shape\",z2.shape)\n",
    "        #applies the tanh function to obtain the input mapped to a distrubution of values between -1 and 1\n",
    "#\n",
    "# The output of the hidden layer is the input passed through an \"activation\" function.  This can\n",
    "# be sigmoid, tanh, relu, etc\n",
    "  a2 = tanh(z2)\n",
    "#\n",
    "# Need to add \"ones\" column to this just like a1\n",
    "  ones = np.ones((len(a2),1))\n",
    "  a2 = np.append(ones,a2,axis=1)\n",
    "\n",
    "#\n",
    "# z3: the input to the output layer = weight w2 matrix applied to (a2 plus bias)  \n",
    "  z3 = a2.dot(w2.T)\n",
    "# \n",
    "# The \"output\" of the output layer is pass through the softmax activation\n",
    "  a3 = softmaxNew(z3)\n",
    "  return a1, z2, a2, z3, a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6516iVJVo3QB"
   },
   "source": [
    "## Backpropagation\n",
    "Remember that our goal is to find the weights $W_1$ and $W_2$ that minimize the cost function J.  To do this using simple gradient descent would be difficult, so instead we will use a concept called **backpropagation**.  \n",
    "\n",
    "There are some helpful writeups that might make things clear for you:\n",
    "* [A simple overview of the chain rule and backpropagation ](https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html)\n",
    "* [A detailed  application of the chain rule and backpropagation](http://www.briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4)\n",
    "* [The derivative of the softmax output and the cross entropy cost function](https://deepnotes.io/softmax-crossentropy)\n",
    "\n",
    "\n",
    "We want to find the best $W_1$ and $W_2$, so we will adjust these weights in proportion to how much they contribute to the overall cost: \n",
    "$$W_1:= W_1 - \\alpha {\\partial J\\over \\partial W_1}$$\n",
    "$$W_2:= W_2 - \\alpha {\\partial J\\over \\partial W_2}$$\n",
    "\n",
    "This looks just like gradient descent, but the challenge is: how do we determine ${\\partial J\\over \\partial \\theta_j}$?   This is where backpropagation comes in.\n",
    "\n",
    "If we look at the forward pass, we see how we get from an input X to an output a3:\n",
    "$$x\\rightarrow a_1 \\rightarrow z_2=a_1 w_1 \\rightarrow a_2=f_{tanh}(z_2) \\rightarrow z_3=a_2 w_2 \\rightarrow a_3=f_{softmax}(z_3) $$\n",
    "\n",
    "The basic idea is look at the forward  pass, and view our cost function as a series of **nested** equations.\n",
    "* Thinking of J as a function of $W_2$:\n",
    "$$J = f_{cost}( f_{a3}(f_{z3}(f_{W_2})))$$\n",
    "\n",
    "* Thinking of J as a function of $W_1$:\n",
    "$$J = f_{cost}( f_{a3}(f_{z3}(f_{a2}(f_{z2}(f_{W_1}))))$$\n",
    "\n",
    "To get the partial derivatives of J with respect to $W_1$ and $W_2$, we use the chain rule:\n",
    " $${\\partial J\\over \\partial W_2} =    {\\partial J\\over \\partial a_3} ~ {\\partial a_3\\over \\partial z_3} ~{\\partial z_3\\over \\partial W_2}       $$\n",
    "\n",
    " $${\\partial J\\over \\partial W_1} =    {\\partial J\\over \\partial a_3} ~ {\\partial a_3\\over \\partial z_3} ~{\\partial z_3\\over \\partial a_2}  ~ {\\partial a_2\\over \\partial z_2}  ~ {\\partial z_2\\over \\partial W_1}      $$\n",
    " \n",
    "Here are each of the above derivatives:\n",
    "1.  From the third link above, we can get the combined derivative of the softmax output and the cross entropy cost function$ {\\partial J\\over \\partial a_3}{\\partial a_3\\over \\partial z_3} = (a_3 - y_i)$\n",
    "3. ${\\partial z_3\\over \\partial W_2} = a_2$\n",
    "4. ${\\partial z_3\\over \\partial a_2} = W_2 $\n",
    "5. ${\\partial a_2\\over \\partial z_2} = deriv(tanh(z_2))$\n",
    "6. ${\\partial z_2\\over \\partial W_1}  = a_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEoYa1kkofSh"
   },
   "outputs": [],
   "source": [
    "def backpropMine(a1, a2, a3, z2, y_enc, w1, w2,Lambda):\n",
    "\n",
    "#\n",
    "  pJ_pa3__pa3_pz3 = (a3 - y_enc)\n",
    "  pz3_pw2 = a2\n",
    "#\n",
    "# Pull it all together\n",
    "  grad_w2 = pJ_pa3__pa3_pz3.T.dot(pz3_pw2)\n",
    "\n",
    "  pz3_pa2 = w2\n",
    "  ones = np.ones((len(z2),1))\n",
    "  z2 = np.append(ones,z2,axis=1)\n",
    "  pa2_pz2 = tanh_deriv(z2)\n",
    "  pz2_pw1 = a1\n",
    "  sigma2 = pJ_pa3__pa3_pz3.dot(pz3_pa2) * pa2_pz2\n",
    "#\n",
    "# Pull it all together\n",
    "  grad_w1 = sigma2[:, 1:].T.dot(pz2_pw1)\n",
    "#\n",
    "# add the regularization term\n",
    "  grad_w1[:, 1:]+= (w1[:, 1:]*Lambda) # derivative of .5*l2*w1^2\n",
    "  grad_w2[:, 1:]+= (w2[:, 1:]*Lambda) # derivative of .5*l2*w2^2\n",
    "  return grad_w1, grad_w2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OMcdoQMD6Bww"
   },
   "source": [
    "## Iterating until we converge\n",
    "The basic algorithm then to implement gradient descent looks like this:\n",
    "1. Initialize each of the $W_1$ and $W_2$  parameters to some reasonable value - in this case, it turns out that initializing these weights to 0 would be a *bad idea*.  Instead, we initialize them to a random, but small, number. Remember that these two matrices connect subsequent layers, so:\n",
    "  *  $W_1$ is a matrix of (nhidden) by (nfeatures +1)\n",
    "  * $W_2$ is a matrix of (noutputs) by (nhidden+1)\n",
    "2. Choose a learning rate $\\alpha$, and number of maxmimum allowed iterations.  Iterations in which we have processed a full training set are called **epochs**.  \n",
    "3. It is common for the algorithms to pass in chunks of the training set at a time (called mini-batches), and then update the weights after each mini-batch.   This idea is implemented below.   It is also common for the training to be stopped once the cost begins to increase on the testing set.   This is **not** implemented below.\n",
    "4. Run a forward pass to get our values at each stage.\n",
    "5. Run backpropagation to get our gradients for our weights and adjust the weights.\n",
    "\n",
    "It is helpful to keep track of the cost for each iteration (or with each mini-batch), so you can plot it and inspect its behavior.   And of course you need to keep track of the last value of the $W_1$ and $W_2$ parameters so you can return them.\n",
    "\n",
    "An implementation of this iteration algorithm is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0LAhPJkCtgNk"
   },
   "outputs": [],
   "source": [
    "def fit(X, y_oneHot, n_hidden=100,epochs=100,numBatches=500,Lambda=0.0,learning_rate=0.001):\n",
    "  \n",
    "  decay_rate=0.00001\n",
    "#\n",
    "# Get copies of the data\n",
    "  X_data = X.copy()\n",
    "  y_enc = y_oneHot.copy()\n",
    "# Get the initial values\n",
    "  m,n_features = X_data.shape   # this has the true \"n\" features \n",
    "#\n",
    "# How many outputs do we have\n",
    "  m,n_output = y_enc.shape\n",
    "#\n",
    "# Initialize the weights to small random numbers\n",
    "  w1 = np.random.uniform(-1.0, 1.0, size = n_hidden *(n_features + 1)).reshape(n_hidden, (n_features + 1))/(10.0*n_features + 1)\n",
    "  w2 = np.random.uniform(-1.0, 1.0, size= n_output*(n_hidden+1)).reshape(n_output,n_hidden+ 1) /(10.0*n_hidden + 1)\n",
    "  prev_grad_w1 = np.zeros(w1.shape)\n",
    "  prev_grad_w2 = np.zeros(w2.shape)\n",
    "  costs = []\n",
    "\n",
    "# Run through the dataset some fixed number of epochs\n",
    "  for i in range(epochs):\n",
    "    learning_rate /= (1 + decay_rate*i)\n",
    "#\n",
    "# Split the data up into chunks\n",
    "    mini = np.array_split(range(X_data.shape[0]), numBatches)\n",
    "    print(\"epoch\",i)\n",
    "    for idx in mini:\n",
    "#\n",
    "# Forward pass\n",
    "      a1, z2, a2, z3, a3= forward_pass(X_data[idx], w1, w2)\n",
    "      cost = calc_cost(y_enc[idx], a3, w1, w2, Lambda=Lambda)\n",
    "      costs.append(cost)\n",
    "\n",
    "# compute gradient via backpropagation\n",
    "      grad1, grad2 = backpropMine(a1=a1, a2=a2, a3=a3, z2=z2, y_enc=y_enc[idx], w1=w1, w2=w2, Lambda=Lambda)\n",
    "      w1_update, w2_update = learning_rate*grad1, learning_rate*grad2\n",
    "      w1 += -w1_update\n",
    "      w2 += -w2_update\n",
    "      prev_grad_w1, prev_grad_w2 = w1_update, w2_update\n",
    "  return w1,w2,costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7lMh-w6AC6MQ"
   },
   "source": [
    "# Apply the Neural Network Algorithm to the MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rc9S8Q0I6ODH"
   },
   "source": [
    "## Get the Data\n",
    "We will use the MNIST data sample to test our softmax regression algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZLFjGuuzwYZH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Form our test and train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#short = \"\"\n",
    "short = \"short_\"\n",
    "dfCombined = pd.DataFrame()\n",
    "#\n",
    "# Read in digits\n",
    "for digit in range(10):\n",
    "  fname = 'https://raw.githubusercontent.com/big-data-analytics-physics/data/master/ch3/digit_' + short + str(digit) + '.csv'\n",
    "  df = pd.read_csv(fname,header=None)\n",
    "  df['digit'] = digit\n",
    "  dfCombined = pd.concat([dfCombined, df])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yl76vzLqa74"
   },
   "source": [
    "## Make Separate Test and Train Samples\n",
    "We will do a simple 70/30 split to form our Train/Test sample.\n",
    "\n",
    "We also need to:\n",
    "* Scale the input data.   Since we know the input pixel data goes from 0-255, we can just divide by 255.\n",
    "* Convert our output labels to 1-hot.   We will use a **keras** utility for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiEFNaYKwlHo"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "\n",
    "train_digits,test_digits = train_test_split(dfCombined, test_size=0.3, random_state=42)\n",
    "yTrain = train_digits['digit'].values\n",
    "XTrain = train_digits.as_matrix(columns=train_digits.columns[:784])\n",
    "\n",
    "yTest = test_digits['digit'].values\n",
    "XTest = test_digits.as_matrix(columns=test_digits.columns[:784])\n",
    "\n",
    "#\n",
    "# one hot encode the labels\n",
    "num_classes = len(np.unique(yTrain))\n",
    "print(\"Number distinct classes \",num_classes)\n",
    "yTrain_oneHot = to_categorical(yTrain, num_classes=num_classes)\n",
    "yTest_oneHot = to_categorical(yTest, num_classes=num_classes)\n",
    "for i in range(10):\n",
    "  print(\"digit \",yTrain[i],\"encoding\",yTrain_oneHot[i])\n",
    "  \n",
    "#\n",
    "# We need to normalize our data - just divide by 256!\n",
    "XTrain = XTrain/255.0\n",
    "XTest = XTest / 255.0\n",
    "#\n",
    "print(\"XTrain\",XTrain.shape)\n",
    "print(\"XTest\",XTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ViDEXZHyC24c"
   },
   "source": [
    "## Now run the algorithm\n",
    "Set our parameters to reasonable values and run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9SIfjAjxJd6"
   },
   "outputs": [],
   "source": [
    "\n",
    "n_hidden=100\n",
    "epochs=100\n",
    "learning_rate=0.0001\n",
    "numBatches=20\n",
    "\n",
    "w1,w2,costs = fit(XTrain, yTrain_oneHot, n_hidden=n_hidden,epochs=epochs,numBatches=numBatches,learning_rate=learning_rate)\n",
    "print(\"costs \",costs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vh4IgpcPHtbM"
   },
   "source": [
    "## Examine Results\n",
    "Look at the performance of our classifier.    We can look at both the accuracy (for test and train), as well as the cost plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFLKOWSw-djS"
   },
   "outputs": [],
   "source": [
    "def getResults(X,w1,w2):\n",
    "#\n",
    "# Run a forward pass for our input data\n",
    "  a1, z2, a2, z3, a3 = forward_pass(X, w1, w2)\n",
    "#\n",
    "# Our outut probability vector for each class is a3\n",
    "#\n",
    "# Get the max probabilites\n",
    "  probs = np.max(a3, axis = 1)\n",
    "#\n",
    "# Get the predicted classes\n",
    "  classes = np.argmax(a3, axis = 1)\n",
    "#\n",
    "  return classes,probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lFahKlK-poj"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Train results\n",
    "classes,probs = getResults(XTrain,w1,w2)\n",
    "correct = 0\n",
    "for i in range(yTrain.shape[0]):\n",
    "  if yTrain[i] == classes[i]:\n",
    "    correct += 1\n",
    "acc = 100.0*correct / yTrain.shape[0]\n",
    "print(\"Train accuracy \",acc)\n",
    "#\n",
    "# Test results\n",
    "classes,probs = getResults(XTest,w1,w2)\n",
    "correct = 0\n",
    "for i in range(yTest.shape[0]):\n",
    "  if yTest[i] == classes[i]:\n",
    "    correct += 1\n",
    "acc = 100.0*correct / yTest.shape[0]\n",
    "print(\"Test accuracy \",acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note the difference in the enable_plotly_in_cell call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLZqzelLV4un"
   },
   "outputs": [],
   "source": [
    "def enable_plotly_in_cell():\n",
    "  import IPython\n",
    "  from plotly.offline import init_notebook_mode\n",
    "#\n",
    "# OLD (google colab)\n",
    "#  display(IPython.core.display.HTML('''\n",
    "#        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "#  '''))\n",
    "#  init_notebook_mode(connected=False)\n",
    "#\n",
    "# New (OSC) [thanks to Stephen Gant for this!]\n",
    "  init_notebook_mode(connected=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJq1Sgy1VySy"
   },
   "outputs": [],
   "source": [
    "from plotly.offline import iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "enable_plotly_in_cell()\n",
    "data = go.Scatter(\n",
    "    x=np.array(range(0,len(costs))),\n",
    "    y=costs,\n",
    "    mode='markers',\n",
    "    name=\"fitted data\"\n",
    ")\n",
    "\n",
    "\n",
    "iplot(dict(data=[data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "neural_network_intro.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
