{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "77jg7cxEhq7G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qt337AdUhvy4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment 3: End-to-End Machine Learning Project\n",
        "The material in this assignment is based on Chapter 2 of Hands-On Machine Learning with Scikit-Learn and TensorFlow, by Aurelieln Geron."
      ]
    },
    {
      "metadata": {
        "id": "GbvCKt6Vh0_D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# What problem are we trying to solve and how will we solve it?\n",
        "In our case, we will be trying to build a model of California housing prices using Census data.   The primary goal: be able to predict the median housing price in any California district, using the data available in this dataset.  This problem is an example of **regression**, where the prediction of our model (or its output) is a continuous variable. This is in contrast to **classification**, where the prediction of our model (or its output) is a class or group.\n",
        "\n",
        "The typical steps in such an analysis vary depending on the problem, but they usually include the following:\n",
        "1.  Get the data.\n",
        "2.  Minimally clean and prepare the data.\n",
        "3.  Explore the data, typically using visualizations.\n",
        "4.  Select a model appropriate for your particular problem and train it.\n",
        "5.  Test the model using unseen data.\n",
        "6.  Fine tune the model.\n",
        "7.  Present the results.\n",
        "\n",
        "We will go through all of these steps.  We won't dwell on the details of the model - we will use it like a **black box**.   Later on in the course, we will spend more time on the details."
      ]
    },
    {
      "metadata": {
        "id": "8wA6JbzpjKXG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1-lmAycnjLOT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1) Get the data"
      ]
    },
    {
      "metadata": {
        "id": "4Oeg0rsyjO4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c5c3552b-6026-4d75-8fd1-3d907dae9d6c"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Now let's print some data to the screem\n",
        "housing = pd.read_csv(\"https://raw.githubusercontent.com/big-data-analytics-physics/data/master/ch2/housing.csv\")\n",
        "print(\"Housing columns:\",housing.columns)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Housing columns: Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
            "       'total_bedrooms', 'population', 'households', 'median_income',\n",
            "       'median_house_value', 'ocean_proximity'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dURp0ITTtIPk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MIMyIkQ9s--0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our goal in this assignment: predict the **median_house_value** given all of the other data.   \"median_house_value\" will be our label.  All of the other columns are our **features**."
      ]
    },
    {
      "metadata": {
        "id": "6paGjENxja4V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2) Explore the data\n",
        "As we did in chapter 1, we are going to want to explore the data.\n",
        "\n",
        "1.  Look at a few rows of the dataset: use housing.head().\n",
        "2.  Get some info about the names and types of the columns in the dataframe, the number of rows, and how much memory the dataframe takes up: using housing.info()\n",
        "3.  Get some basic statistical info about the dataframe (mean, std, etc): use housing.describe()\n",
        "4.  Get correlations among all of the columns: use housing.corr()"
      ]
    },
    {
      "metadata": {
        "id": "dnrJhyejj9ml",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#3) Feature Engineering:\n",
        "\n",
        "Feature engineering refers to combining existing features to form new ones. These combination might be simple (like the result of adding/subracting/multiplying/etc) or they could be more complex - like the results of a sophisticated analysis. The basic idea is to add information for each candidate data point, which will hopefully improve whatever model we end up using to perform our predictions.\n",
        "\n",
        "In our case there are some obvious new features we can create.\n",
        "\n",
        "1.  rooms_per_househoud\n",
        "2.  bedrooms/househoud\n",
        "3.  bedrooms/room\n",
        "4.  people per househoud\n",
        "\n",
        "Think about the startified sampling that we did earlier, and note that by far the most correlated variable in our dataset in **median_income**.   So when we split our data we would like to know for sure that our test sample is close in distribution to the median income of our train sample.   Will this be true if we just randomly split the data?   Welaready know that he answer is \"not quite\".\n",
        "\n",
        "To test this, let's make a **categorical** vaiable which describes median income.   We will have 5 categories, running from 1.0 (low) to 5.0 (high)."
      ]
    },
    {
      "metadata": {
        "id": "J9VqTjyCqcBv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "housing['income_cat'] = np.ceil(housing['median_income'] /1.5)\n",
        "housing['income_cat'].where(housing['income_cat']<5.0,5.0,inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gH9wX4-ilIyx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4)  Train/Test Splitting\n",
        "Out goal is to design an algorithm to prediction housing prices.   To test our model, we will want to split our data into two parts:\n",
        "1.  Training sample: This is the sample we will train our model on.\n",
        "2.  Testing sample: This is the **unseen** data that we will test our trained model on.  Good performance on this sample will ensure that our model generalizes well.\n",
        "\n",
        "Use a split of 80% train and 20% test.   You can do a **random split**, but a better split is stratified accoring to the income category variable we defined above.   No matter what split you end up using, make sure you see how well the test and train sets agree in that variable.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "dTqfFIgamFp-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5) Dealing with missing data\n",
        "You could try:\n",
        "1.  Removing all rows with any missing data\n",
        "2.  Replacing the missing data with the mean of the column:  **NOTE**: if you do this, you must get the means from the **training** set.    Think about why this is the case."
      ]
    },
    {
      "metadata": {
        "id": "Fgfzg21anxCs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6) Feature Scaling\n",
        "We will use feature scaling as we did with the fligth dataset.  In this case, use **standardization**.   Remember: you need to use the **training** set to **fit** the transformer, and you need to use the **transformer** on **both** the training and test sets.\n",
        "\n",
        "Remember that we do not use these techniques for **categorical** columns (something different will be done).\n"
      ]
    },
    {
      "metadata": {
        "id": "UhfjvI0ZqCFw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 7) Combining everything before fitting\n",
        "\n",
        "Refer to the ealrlier workbook titled \"Putting Humpty-Dumpty back together!\"\"\n",
        "\n",
        "After all of our above work we should have:\n",
        "1.   two numpy arrays containing our \"scaled\" numerical features, one for our training sample and one for our testing sample\n",
        "2.   two one-hot-encoded numpy arrays for our categorical variable, one for our training sample and one for our testing sample\n",
        "\n",
        "We need to combine these so we have **one** training numpy array, and **one** testing numpy array.   Along with each of these, we will have **label** arrays, made from the median_house_value column for the test and train samples.\n"
      ]
    },
    {
      "metadata": {
        "id": "D6drgULrrf8g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 8) Fit the data and test the fit\n",
        "As before the fit model will be linear regression (we are using more than just a single feature but is it still just linear regression).   Test the fit "
      ]
    },
    {
      "metadata": {
        "id": "SQwR8G-ot6MC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "model = LinearRegression()\n",
        "#\n",
        "# Do this on test AND training set!\n",
        "lin_mse = mean_squared_error(test_housing_labels,test_housing_labels_pred)\n",
        "lin_rmse = np.sqrt(lin_mse)    ## Remember to take the square root!\n",
        "print(\"Mean squared error and the root mean square\",lin_mse,lin_rmse)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4iENJRWaWGct",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 9) Some extra stuff\n",
        "\n",
        "If you are looking for more to do!\n",
        "1.  **Making maps**\n",
        "This data is interesting since it has latitude and longitude. Previously we made world maps, but this depended on our data having tags which were country names. This is different. This will be more like a scatter-plot, but arranged on an existing map (primarily California). How do we do this?\n",
        "Google: plotly map scatter\n",
        "Take the code from the first example and modify it:\n",
        "\n",
        "2.  *IF* you used random smaple for your test/train split, try using instead stratified sampling based on the income category variable.\n",
        "\n",
        "3.   We probably should have done this first.... but how do we *know* that our fit imporved our knowledge?   Is there a simple predictor that we could have used instead?   How about if we predict the price simply based on the mean (or the median) of all housing prices?   Use the mean squared error to do this\n",
        "\n",
        "4.   Try another predictor from sklearn:  RandomForestRegressor and/or DecisionTreeRegressor.   Make sure you test the fit results (using mean_squared_error) on BOTH the training AND test sets!"
      ]
    }
  ]
}