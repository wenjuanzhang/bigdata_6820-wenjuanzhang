{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Python Jobs on OSC\n",
    "The jupyter environment that we have been using for the past 2 months is a powerful platform for testing code and exploring datasets and tools, but it is not the right platform for a production environment.   In addition, once we start analyzing very large datasets that will necessarily require long processing times, it may be more sensible to use straight python scripts.   When running such scripts on a farm-processing system such as OSC with potentially hundreds of simultaneous users, there is an additional requirement to use a **scheduling system** to handle the running of each person's **job**.   This notebook will walk us through each of these situations.\n",
    "\n",
    "In this notebook, I will give you instructions showing you:\n",
    "1.  How to build a simple python script based on work we have already done in previous jupyter notebooks.\n",
    "2.  How to run that job interactively from the **command line**.   Note this only should be done for **very,very** short jobs.\n",
    "3.  How to build a **bash** script to run that python script using the **pbs** batch submission system on OSC.\n",
    "4.  How to utilize the gpu capabilities of the OSC system.\n",
    "\n",
    "# Getting started\n",
    "You should already have a jupyter session started on the OSC Pitzer system.   We will need to use this **as well as** a separate terminal window for **shell** access.   The terminal window will give us the ability to use the **command line**.   We can get shell access in one of two ways:\n",
    "1.  Go to your OnDemand dashboard, select the **Clusters** tab along the top, then select **>_Pitzer Shell Access**.   This will open a tab in your browser which is connected directly to your **/home** file system on OSC.   **This is the option I will assume for the directions below.**\n",
    "2.  Go to your OnDemand dashboard, select the **Interactive Apps** tab along the top, then select **Pitzer Desktop**.  You will be prompted to set up your environment - the only change I would recommend is the number of hours, use the defaults for everything else.   You will then need to wait till the resources for your desktop are allocated.   Once they are ready you click **Launch**, and then a graphical window will open up showing the \"desktop\" OSC environment.   If you click on the **terminal** icon (along the bottom of the desktop window), a terminal window will open and you can follow the remaining instructions below.   The primary difference with this method versus the **Pitzer Shell** approach is that you can view graohical objects (like jpeg/png/pdf) in the Pitzer Desktop.  The other difference is that the Pitzer Shell has no time limit, while the Pitzer Desktop does.\n",
    "\n",
    "# Making a python script\n",
    "In the terminal window, navigate to your assignments/assignments_11_prep directory.\n",
    "\n",
    "We will need to make a new file, and to do this we need an editor.   If you are using the graphical enviroment (Pitzer Desktop) you can use a graphical editor such as **gedit**, but if you are using the Pitzer Shell like I am, you need a command line editor.  There are a number of such editors available, but the easiest to use is called **nano**.  Note that nano can also be used in the terminal window on the Pitzer Desktop as well.\n",
    "\n",
    "At the command prompt (which I am assuming is the \"$\\$$\" symbol), type:\n",
    "\n",
    "     $ nano cnn_intro.py\n",
    "\n",
    "and then hit return.  \n",
    "\n",
    "You will see a screen that looks like this:\n",
    "![nano](files/nano_screenshot.png \"nano screen\")\n",
    "\n",
    "An important feature about nano: the mouse does not do anything!   To move around in the window, you can use:\n",
    "* arrow or page keys\n",
    "* \"^V\" (or control-V) to move down\n",
    "* \"^Y\" (or control-Y) to move up\n",
    "\n",
    "To put code in the window, you can of course type it in, but I want you to copy the following three code blocks and past them in the nano window (in the order shown):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#\n",
    "# Used to implement the multi-dimensional counter we need in the performance class\n",
    "from collections import defaultdict\n",
    "def autovivify(levels=1, final=dict):\n",
    "    return (defaultdict(final) if levels < 2 else\n",
    "            defaultdict(lambda: autovivify(levels-1, final)))\n",
    "def getPerformance(network,images,labels_cat,labels):\n",
    "#\n",
    "# Get the overall performance for the test sample\n",
    "    loss, acc = network.evaluate(images,labels_cat)\n",
    "#\n",
    "# Get the individual predictions for each sample in the test set\n",
    "    predictions = network.predict(images)\n",
    "#\n",
    "# Get the max probabilites for each rows\n",
    "    probs = np.max(predictions, axis = 1)\n",
    "#\n",
    "# Get the predicted classes for each row\n",
    "    classes = np.argmax(predictions, axis = 1)\n",
    "#\n",
    "# Now loop over the first twenty samples and compare truth to prediction\n",
    "#print(\"Label\\t Pred\\t Prob\")\n",
    "#for label,cl,pr in zip(smear_labels[:20],classes[:20],probs[:20]):\n",
    "#    print(label,'\\t',cl,'\\t',round(pr,3))\n",
    "#\n",
    "# Get confustion matrix\n",
    "    cf = autovivify(2,int)\n",
    "    for label,cl in zip(labels,classes):\n",
    "        cf[label][cl] += 1\n",
    "#\n",
    "    return loss,acc,cf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "#\n",
    "# Change the folling to False to run on the full data\n",
    "# NOTE: Keep true when running interactively!!\n",
    "short = True\n",
    "if short:\n",
    "    train_images = train_images[:7000,:]\n",
    "    train_labels = train_labels[:7000]\n",
    "    test_images = test_images[:3000,:]\n",
    "    test_labels = test_labels[:3000]\n",
    "#\n",
    "print(\"Train info\",train_images.shape, train_labels.shape)\n",
    "print(\"Test info\",test_images.shape, test_labels.shape)\n",
    "train_images = train_images.reshape((train_images.shape[0],28*28))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape((test_images.shape[0],28*28))\n",
    "test_images = test_images.astype('float32')/255\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels_cat = to_categorical(train_labels)\n",
    "test_labels_cat = to_categorical(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "#\n",
    "# Make sure the shape of the input is correct (the last \",1\" is the number of \"channels\"=1 for grayscale)\n",
    "train_images = train_images.reshape((train_images.shape[0],28,28,1))\n",
    "test_images = test_images.reshape((test_images.shape[0],28,28,1))\n",
    "#\n",
    "cnn_network = models.Sequential()\n",
    "#\n",
    "# First convolutional layer\n",
    "cnn_network.add(layers.Conv2D(30,(5,5),activation='relu',input_shape=(28,28,1)))\n",
    "# Pool\n",
    "cnn_network.add(layers.MaxPooling2D((2,2)))\n",
    "#\n",
    "# Second convolutional layer\n",
    "cnn_network.add(layers.Conv2D(25,(5,5),activation='relu'))\n",
    "# Pool\n",
    "cnn_network.add(layers.MaxPooling2D((2,2)))\n",
    "#\n",
    "# Connect to a dense output layer - just like an FCN\n",
    "cnn_network.add(layers.Flatten())\n",
    "cnn_network.add(layers.Dense(64,activation='relu'))\n",
    "cnn_network.add(layers.Dense(10,activation='softmax'))\n",
    "#\n",
    "# Compile\n",
    "cnn_network.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#\n",
    "# Fit/save/print summary\n",
    "history = cnn_network.fit(train_images,train_labels_cat,epochs=2,batch_size=256,validation_data=(test_images,test_labels_cat))\n",
    "cnn_network.save('fully_trained_model_cnn.h5')\n",
    "print(cnn_network.summary())\n",
    "#\n",
    "# Get the overall performance for the test sample\n",
    "test_loss, test_acc = cnn_network.evaluate(test_images,test_labels_cat)\n",
    "print(\"Test sample loss: \",test_loss, \"; Test sample accuracy: \",test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our changes\n",
    "Once you have entered the above three blocks, make sure you can move up and down through the code using the arrow keys as well as the page up and down functions (I found that control-y and control-Y moved up, but that only control-V moved down).\n",
    "\n",
    "To save your changes, type control-x.  You will be prompted if you want to \"Save modified buffer\", and answer \"Yes\" by typing y.\n",
    "\n",
    "You will then be asked for the file name you want to save your changes as.  If you had originally just typed \"nano\" with no file, you would enter a new file name here.  If as above you had typed \"nano cnn_intro.py\", you should see:\n",
    "\n",
    "    File Name to Write: cnn_intro.py \n",
    "    \n",
    "Just hit return and it will be saved as that file name.\n",
    "\n",
    "## Running our python script\n",
    "To run our script, simply type:\n",
    "\n",
    "    $ python cnn_intro.py \n",
    "    \n",
    "If you see the following message:\n",
    "\n",
    "    ImportError: No module named keras.datasets\n",
    "    \n",
    "Then there is something wrong with your python version (probably), so type:\n",
    "\n",
    "    $ python --version\n",
    "    \n",
    "If this returns \"Python 2.7.5\" then you have the wrong version of python.  To fix this, type:\n",
    "\n",
    "    module load python/3.6-conda5.2\n",
    "    \n",
    "Try \"python --version\" again, and you should see: \"Python 3.6.6 :: Anaconda custom (64-bit)\".   Now try running your python script again:\n",
    "\n",
    "    python cnn_intro.py \n",
    "    \n",
    "Your code should now run, and after the 2 epochs (running on the small sample), you should see an accuracy of around 88%.   We kept the sample size and number of epochs low because we have an interactive session that we are sharing with many users.  Below we will show how we can run much longer jobs.\n",
    "\n",
    "**Aside:** what if in the future I can't remember the exact version of python?  Or if I want to know if other modules/software are available (like matlab)?  I can use the \"module spider xxxx\" command, where xxxx is the base name of the software I am interested in.  For example, if I type:  \n",
    "\n",
    "    module spider python  \n",
    "\n",
    "I get the following output:  \n",
    "    Versions:  \n",
    "        python/2.7-conda5.2  \n",
    "        python/3.6-conda5.2  \n",
    "\n",
    "\n",
    "## The PBS System\n",
    "The batch system at OSC can do some incredibly complicated things, but our initial use will be very simple: run a single, short \"job\", and return the results.  We will submit the above python script to the batch system and let it process the data - we simply sit back and wait for it to finish!\n",
    "\n",
    "There is alot of detail that I will skip over, since you can get alot out of the simple things I will show you.  However, after class I encourage you to take a look at the full documentation available [here](https://www.osc.edu/supercomputing/batch-processing-at-osc).\n",
    "\n",
    "Let's first submit a job, and then while we wait, we can look at how we control the job we submitted.\n",
    "\n",
    "In the terminal window, let's use nano again to make a new file, this time a **bash** script. \n",
    "\n",
    "    nano pbs_run.sh\n",
    "\n",
    "Copy the following code block (which is **not** python but **bash** shell commands) into the above file.   After enter the text, type control-x to save the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PBS -N cnn_mnist\n",
    "#PBS -l walltime=0:30:00\n",
    "#PBS -l nodes=1:ppn=1\n",
    "#PBS -l mem=4761MB\n",
    "#PBS -j oe\n",
    "\n",
    "# uncomment if using qsub\n",
    "cd $PBS_O_WORKDIR\n",
    "echo $PBS_O_WORKDIR\n",
    "\n",
    "module load python/3.6-conda5.2\n",
    "python -u cnn_intro.py >&! cnn_intro_output.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting a job to the batch system.\n",
    "If you type \"ls -alrt\" and hit return in your terminal window, you should see something like the following (of course the time stamps as well as the formatting will be different):  \n",
    "xxxxxxx   3335 Mar  6 22:24 cnn_intro.py  \n",
    "xxxxxxx 394688 Mar  6 22:31 fully_trained_model_cnn.h5  \n",
    "xxxxxxx    242 Mar  6 22:43 pbs_run.sh  \n",
    "\n",
    "\"ls -alrt\" lists information about files in a folder (or directory), in \"reverse time order\", so that the most recently modified files are at the bottom.   You will find this to be a very useful command!\n",
    "\n",
    "The above files are the following:\n",
    "*  **cnn_intro.py**: Your original python script.\n",
    "*  **fully_trained_model_cnn.h5**:  The trained model you got from running thst python script interactively.\n",
    "*  **pbs_run.sh**: The script you just made to run the python script on the batch system\n",
    "\n",
    "To \"submit\" your job, simply type:\n",
    "\n",
    "    qsub pbs_run.sh\n",
    "    \n",
    "You may be asked to specify an account.  If so, modify the above to be something like:\n",
    "\n",
    "    qsub -A PASxxxx pbs_run.sh\n",
    "\n",
    "where \"PASxxxx\" is a valid account number.  If your command is successful, you should see a message like this printed to the screen:\n",
    "\n",
    "    408559.pitzer-batch.ten.osc.edu\n",
    "\n",
    "That first number (408559) is the session ID for the job.   To get an idea if your job is running (or queued or completed), type:\n",
    "\n",
    "    qstat -u osuXXXX\n",
    "    \n",
    "where \"osuXXXX\" is your user ID.   You can find this on your OnDemand desktop in the upper right hand corner.\n",
    "\n",
    "Your job may take a few minutes to start, and then it will finish quickly.   While we wait for that, let's look at the bash script in detail.\n",
    "\n",
    "## The bash script\n",
    "A **bash** script is simply a collection of commands that you could execute in the terminal.  However, \"scripting\" the commands can get fairly complex, to the point where a bash script could look much like a \"real\" programming script (like python).   For our batch scripts, they will be fairly simple.   A key difference for our **batch** scripts, is that they contain commands (the \\#PBS lines below) that **also** tell the batch submission system **PBS** how to execute our job.\n",
    "\n",
    "Let's walk through the above batch script line by line, and see what each line does.\n",
    "* #PBS -N cnn_mnist  \n",
    "  This command tells PBS what your job's name is.  \n",
    "* #PBS -l walltime=0:30:00  \n",
    "  This command tells PBS how much time your job will need.   You want this to be comfortably larger than the time your job will actually take, since as soon as you exceed this time, your job is stopped. \n",
    "* #PBS -l nodes=1:ppn=1 \n",
    "    This command tells PBS how many nodes and ppn (processors per node, or cores) your job needs.  Since each core contributes about 4GB to the available memory, specifying the number of cores also determines how much memory your job has.\n",
    "* #PBS -j oe  \n",
    "    This command tells PBS to join standard output as well as errors into the same file.  \n",
    "* \\# uncomment if using qsub  \n",
    "    In batch scripts, lines beginning with \\# are ignored by the system.  Any other line is executed as though it was a command you typed in the terminal.  So this line is just a comment.   The above \\#PBS lines are **also** comments - but they are special in that the batch system is designed to look for lines beginning with \\#PBS and interpret them as commands.\n",
    "* cd \\$PBS_O_WORKDIR  \n",
    "    This line \"changes directory\" to the **environment variable** pointed to by **\\$PBS_O_WORKDIR**.   This is the directory the job was submitted from.  \n",
    "* echo \\$PBS_O_WORKDIR  \n",
    "    The bash **echo** command is like the python **print** command - it prints things to the screen.   This simply prints the **value** of the \\$PBS_O_WORKDIR environment variable.  \n",
    "* module load python/3.6-conda5.2  \n",
    "    This makes sure your submitted job uses the correct version of python.  \n",
    "* python -u cnn_intro.py >&! cnn_intro_output.log  \n",
    "    Finally!   Our actual python script!   There are some changes to how we ran it before (which we did with a simple \"python cnn_intro.py\".\n",
    "    * The \"-u\" says \"print stuff to the screen and don't buffer it first (which is the default).\n",
    "    * The \">&! cnn_intro_output.log\" says: put all output that would go to the screen including all errors, and put it into a file named \"cnn_intro_output.log\".   You can look at this file while your job is running.\n",
    "\n",
    "\n",
    "## When the job finishes\n",
    "Once the job starts, as well as when it finishes, new files will appear in the folder that you submitted the job from.\n",
    "If you type \"ls -alrt\" and hit return in your terminal window, you should see the following:  \n",
    "xxxxxxx   3335 Mar  6 22:24 cnn_intro.py  \n",
    "xxxxxxx 394688 Mar  6 22:31 fully_trained_model_cnn.h5  \n",
    "xxxxxxx    242 Mar  6 22:43 pbs_run.sh  \n",
    "xxxxxxx   8031 Mar  6 22:51 cnn_intro_output.log  \n",
    "xxxxxxx    407 Mar  6 22:51 cnn_mnist.o408559  \n",
    "\n",
    "There are two new files:\n",
    "* **cnn_intro_output.log**: This file is all of the output that would normally go to the screen, if you were running the job interactively (or in a jupyter notebook).  If something went wrong with your job, you may be able to diagnose it here. \n",
    "* **cnn_mnist.o408559**: This is the \".o\" file.  Notice it starts with the name you gave the job, and ends with the job ID assigned by the system.  This file contains useful information from the batch system, indicating how much cputime the job took, as well as how much memory it used.   This will be **very** helpful, especially when you are thinking of running larger or smaller jobs.   You can use this information to determine if your next job needs more resources (or else it might crash or end too soon) or less resources (in which case you can usually get your results faster).\n",
    "\n",
    "## Running a longer job\n",
    "Now lets run a longer job using the pbs system.  Make the following changes:\n",
    "* Modify the python script so that it runs over the full MNIST sample.   \n",
    "* Also, run for 10 epochs instead of 2.\n",
    "\n",
    "With these changes, do you think we need to modify our bash script to add more time?  I think not, but if I make a mistake in this estimate (especially on a job which might take hours), it might be a sad day!   Let's leave the bash script as it was.  Submit the bash script again using qsub (after having made the above changes to your **python** script).   While it is running see if you can estimate how much longer it might take to finish.\n",
    "\n",
    "## Using GPUs\n",
    "As we know, our machine learning programs require extensive matrix mathematics in order to determine the weights and biases for the models we design.   It turns out that **graphics processing units** or **GPUs** are very efficient at matrix mathematics, and if we have access to GPU processing power, we can potentially speed up our jobs by factors of 10 or more.  A nice (and short) article (along with some good references) on the use of GPUs in machine learning can be found [here](https://www.datascience.com/blog/cpu-gpu-machine-learning).\n",
    "\n",
    "The OSC system has GPU resources on both the **Owens** and **Pitzer** clusters:\n",
    "* Owens: 160 NVIDIA Tesla P100\n",
    "* Pitzer: 64 NVIDIA Tesla V100 (two each on 32 nodes)\n",
    "\n",
    "To take advantage of GPUs, we need to make some modifications to our python environment, specifically with **tensorflow**.   In your terminal window, type the following commands:\n",
    "\n",
    "    $ pip uninstall tensorflow  \n",
    "\n",
    "    $ pip install --user tensorflow-gpu\n",
    "\n",
    "The **tensorflow-gpu** package can run on both **CPU-only** systems as well as systems which have **GPUs**.  If no GPU is detected, the software will default to the CPU version.\n",
    "\n",
    "Next, we need to modify our pbs script to tell pbs that we want to use GPUs.  Note that since these are scarce (relative to CPUs), you should be careful to only use them when necessary.  As we will see, GPUs can greatly accelerate your job's **running** time, but in general they will delay your job's **starting** time.  Typically what you care about is your job's **finishing** time (which is the sum of the above starting and running times).\n",
    "\n",
    "In the terminal window, use nano to make a new pbs script for running our gpu version:\n",
    "\n",
    "    nano pbs_run_gpu.sh\n",
    "\n",
    "Cop the following script into the above file, then exit and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PBS -N cnn_mnist\n",
    "#PBS -l walltime=0:30:00\n",
    "#PBS -l nodes=1:ppn=1:gpus=1\n",
    "#PBS -l mem=4761MB\n",
    "#PBS -j oe\n",
    "\n",
    "# uncomment if using qsub\n",
    "cd $PBS_O_WORKDIR\n",
    "echo $PBS_O_WORKDIR\n",
    "\n",
    "module load python/3.6-conda5.2\n",
    "module load cuda/10.0.130\n",
    "python -u cnn_intro.py >&! cnn_intro_output_gpu.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GPU script\n",
    "The main changes in the pbs submission script are the following:\n",
    "1. We modified one of the pbs commands to add a single gpu:  \n",
    "    #PBS -l nodes=1:ppn=1:gpus=1\n",
    "2. We made sure to load the **cuda** module so that **tensorflow** can take advantage of the available GPU resources.\n",
    "\n",
    "What is **CUDA**?  CUDA stands for Compute Unified Device Architecture, and is an extension of the C programming language and was created by nVidia. nVidia is a company tha designs graphics processing units for the gaming and professional markets.  Using CUDA allows programmers to take advantage of the massive parallel computing power of an nVidia graphics card in order to do general purpose computation.  We don't explicitly use CUDA in our python scripts, and neither does keras.  It is tensorflow that utilizes CUDA.\n",
    "\n",
    "We submit the above script in exactly the same way:\n",
    "\n",
    "    qsub pbs_run_gpu.sh\n",
    "    \n",
    "As noted, this may take longer to start, but it will be **much** faster to run.   When it is done, look at the \".o\" file returned by the pbs system.   Does the running time make sense?\n",
    "\n",
    "Given how long the CPU version took compared to the GPU version, does it make sense for a job like this to use GPUs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "sys_python36conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
