{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Normal\" Autoencoders\n",
    "We have already investigated \"standard\" autoencoders.   These autoencoders are designed to reproduce on output a high fidelity version of the input - but not by simply copying it.   They do this by compressing the input data using a neural network, with a narrow \"bottleneck\" layer in the middle.   This bottleneck layer - through training\" - discovers a \"compressed latent encoding\".   As we have seen, this latent encoding can be used in a number of applications: compression, denoising, and (when the encoding and latent layers are fixed) used in a separate classification network.\n",
    "\n",
    "A standard autoencoder is illustrated in the figure below.   The vector $z$ is the latent enocding of the input vector $X$.   This latent vector $z$ is a vector of $numbers$ - one value for each of the latent dimensions.\n",
    "\n",
    "![standard autoencoder](standard_ae.jpeg)\n",
    "\n",
    "\n",
    "Thinking of the encoder as a \"recognition model\" and the decoder as a \"generative model\" is interesting.   Could we use a standard autoencoder to generate new data?   This would be very helpful, especially if we needed new data for our supervised models.   What if wanted to sample the underlying latent vector space?   Variational autoencoders allow us to do this.\n",
    "\n",
    "Some useful reading for this workbook:\n",
    "1.  [Variational autoencoders.](https://www.jeremyjordan.me/variational-autoencoders/)\n",
    "2.  [Using variational autoencoders to learn variations in data](https://news.sophos.com/en-us/2018/06/15/using-variational-autoencoders-to-learn-variations-in-data/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "For this task we will again use the standard MNIST sample that comes with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "short = False\n",
    "if short:\n",
    "    train_images = train_images[:7000,:]\n",
    "    train_labels = train_labels[:7000]\n",
    "    test_images = test_images[:3000,:]\n",
    "    test_labels = test_labels[:3000]\n",
    "#\n",
    "print(\"Train info\",train_images.shape, train_labels.shape)\n",
    "print(\"Test info\",test_images.shape, test_labels.shape)\n",
    "train_images = train_images.reshape((train_images.shape[0],784))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape((test_images.shape[0],784))\n",
    "test_images = test_images.astype('float32')/255\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels_cat = to_categorical(train_labels)\n",
    "test_labels_cat = to_categorical(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very simple autoencoder\n",
    "Below we will make a very simple autoencoder using a fully connected network.   This network consists of the following:\n",
    "1.  An input layer: This is just the 784 pixels from the image.\n",
    "2.  The encoder: this layer has 784 inputs, an encoding_dim=256, and a bottleneck dimension of 2 (or more) outputs.\n",
    "3.  The decoder: this layer takes the 2 outputs of the encoder as input, then has 784 outputs, just like the input to the encoder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 256  \n",
    "bottleneck_dim = 10 \n",
    "#\n",
    "# Define out model\n",
    "network = models.Sequential()\n",
    "#\n",
    "# This is the \"encoder\"\n",
    "network.add(layers.Dense(encoding_dim,activation='relu',input_shape=(28*28,)))\n",
    "network.add(layers.Dense(bottleneck_dim,activation='relu'))\n",
    "#\n",
    "# This is the decoder\n",
    "network.add(layers.Dense(28*28,activation='sigmoid'))\n",
    "#\n",
    "# Now compile the network!\n",
    "network.compile(optimizer='adadelta',loss='binary_crossentropy',metrics=['mse'])\n",
    "print(\"network: \",network.summary())\n",
    "\n",
    "history = network.fit(train_images, train_images,\n",
    "                epochs=10,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(test_images, test_images))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the network\n",
    "We may want to use this network later, so lets save it!   Use a reasonable name so we can tell what it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(history.history)\n",
    "network.save('ae_fcn_bottleneck_'+str(bottleneck_dim)+'.h5')  # \"tag\" the file with the number of dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "To examine performance, we can first look to see if our autoencoder is completely trained.   Lets look at loss and mean-squared error (\"mse\") for train vs test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = history.history['mean_squared_error']\n",
    "test_loss = history.history['val_mean_squared_error']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training MSE', 'Test MSE'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance, Part II\n",
    "Next we can try the \"eye test\": do the images at output look close to the images at input?\n",
    "\n",
    "To test this, we first run all of our **test** (unseen by the network during training) images through the **predict** function of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = network.predict(test_images)\n",
    "print(\"decoded_imgs.shape\",decoded_imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance, Part II (continued):\n",
    "Now we can take a random set of digits (we use the first 10 below) from our test sample, and compare the original images with the output of our autoencoder for those same images.   Note that the output images look close - but there is some blurriness to them.  If the bottleneck dimension is very low (like 2) the images may even look incorrect (but close)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(test_images[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance vs Bottleneck dimension\n",
    "If we vary the bottle neck dimension from 2 to 40, we see, not unexpectedly, large changes in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Functional API\n",
    "Implementing the variational autoencoder can be done with keras, but it is a little tricky.  It turns out that it is much easier to implement using the Keras **functional API**.   For details on this, see [here](https://keras.io/models/model/).\n",
    "\n",
    "To see how the functional API works, lets implement the exact same simple autoencoder as we did above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "original_dim = 28*28\n",
    "encoding_dim = 256  \n",
    "bottleneck_dim = 10  \n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "\n",
    "#\n",
    "# Define out model\n",
    "#network = models.Sequential()\n",
    "#\n",
    "# This is the \"encoder\" - \"Old style\"\n",
    "#network.add(layers.Dense(encoding_dim,activation='relu',input_shape=(28*28,)))\n",
    "#network.add(layers.Dense(bottleneck_dim,activation='relu'))\n",
    "#\n",
    "# The functional API requires that we explicitly define an input layer\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "#\n",
    "# These are the two layers we defined above\n",
    "h1 = Dense(encoding_dim, activation='relu')(x)    # layer input is x, output is h1\n",
    "h2 = Dense(bottleneck_dim, activation='relu')(h1)   # layer input is h1, output is h2\n",
    "#\n",
    "# This is the decoder\n",
    "#network.add(layers.Dense(28*28,activation='sigmoid'))\n",
    "x_decoded = Dense(original_dim, activation='sigmoid')(h2)   # layer input is h2, output is x_decoded\n",
    "#\n",
    "# We could use the standard \"binary_crossentropy\" function but this shows how to define your own!\n",
    "def vae_loss(x, x_decoded):\n",
    "    xent_loss = metrics.binary_crossentropy(x, x_decoded)\n",
    "    return xent_loss\n",
    "#\n",
    "# Here we actually define the model\n",
    "network = Model(inputs=x, outputs=x_decoded)\n",
    "\n",
    "#\n",
    "# Compiling is basically the same, except that we use our loss function\n",
    "#network.compile(optimizer='adadelta',loss='binary_crossentropy',metrics=['mse'])\n",
    "network.compile(optimizer='adadelta', loss=vae_loss,metrics=['mse'])\n",
    "#\n",
    "# Fitting is the same\n",
    "history = network.fit(train_images, train_images,\n",
    "                epochs=10,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(test_images, test_images))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();\n",
    "\n",
    "\n",
    "decoded_imgs = network.predict(test_images)\n",
    "print(\"decoded_imgs.shape\",decoded_imgs.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(test_images[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n",
    "Now let us try to use a different approach to determine the latent representation $z$.   Instead of each dimension in $z$ being a number (where the recognition model learns the mapping from $X$ to $z$), we instead use a representation where the each dimension in the vector $z$ is represented by a function.   This new model is shown in the figure below:\n",
    "\n",
    "\n",
    "![variational autoencoder](vae_1.jpeg)\n",
    "\n",
    "The idea is the following:\n",
    "1.  We use the function q(z|x) to infer the latent state $z$ which was used to generate the observation $x$.   The encoder network is used to learn this function q.\n",
    "2.  The function p(x|z) is then used to reconstruct $x$ given the latent vector $z$.   The decoder network is used to learn this function p.\n",
    "\n",
    "The function we will choose is a gaussion, and each dimension is then represented by the mean and sigma of the gaussian.   The key idea is that $z$ is not just a numerical representation generated by the encoding network: $z$ is **sampled** from gaussians!  To make things concrete:\n",
    "1.   We assume the underlying prior distributions for our latent vectors are gaussians.\n",
    "2.   The latent vector $z$ is actually generated from a vector of (mean,sigma) **pairs**.   This could be as few as **two** pairs (so the dimension of the latent vector $z$ is 2).\n",
    "3.   The vector $z$ is **sampled** from (mean,sigma) pairs.   In practice, this is done by using the following sum: \n",
    "$$z = \\mu + \\epsilon \\cdot \\sigma$$\n",
    "where $\\epsilon$ is sample from a gaussian with mean zero and standard deviation 1.0.\n",
    "\n",
    "4.   In order to make this work,, we need to modify our loss function to consist of two terms:\n",
    "      * the reconstruction loss: this is exactly the same as for a standard autoencoder.  This ensures that the output $X'$ resembles the input $X$.\n",
    "      * the Kullback-Leibler (KL) divergence: This ensures that the learned function $q(z|x)$ function actually models the assumed prior distribution - in this case a gaussian - for each dimension of our latent space.   See [here](https://towardsdatascience.com/demystifying-kl-divergence-7ebe4317ee68) for more details.\n",
    "      The loss function looks like this:\n",
    "      $$loss = L(X,X') + \\sum_j KL(q_j(z|x) || p(z)) $$\n",
    "      \n",
    "      \n",
    "Our new graphical model for the variational autoencoder looks like this:\n",
    "\n",
    "![variational autoencoder](vae_2.jpeg)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder implementation\n",
    "The implementation below come directly from the Keras developers, and can be found in its unmodifed form [here](https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py).\n",
    "\n",
    "I rearranged some of the code and made some minor modifications to help with clarity.  I also added comments throughout the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Example of VAE on MNIST dataset using MLP\n",
    "\n",
    "The VAE has a modular design. The encoder, decoder and VAE\n",
    "are 3 models that share weights. After training the VAE model,\n",
    "the encoder can be used to generate latent vectors.\n",
    "The decoder can be used to generate MNIST digits by sampling the\n",
    "latent vector from a Gaussian distribution with mean = 0 and std = 1.\n",
    "\n",
    "# Reference\n",
    "\n",
    "[1] Kingma, Diederik P., and Max Welling.\n",
    "\"Auto-Encoding Variational Bayes.\"\n",
    "https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "# z = z_mean + sqrt(var) * epsilon\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "# network parameters\n",
    "original_dim = 784\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "epochs = 50\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "#\n",
    "# Here is the input layer\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "#\n",
    "# This is one dense layer of shape 512\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "#\n",
    "# Take the outout of the first dense layer and send this output into\n",
    "# *two* *separate* layers\n",
    "# The out of these layers are the mean and sigma vectors for our latent vector x\n",
    "#   ===> Note we actually use the log(sigma) rather than sigma - this prevents sigma from \n",
    "#        going negative and make the model more stable\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# \n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# This \"trick\" is needed to make backpropagation work when you have a \n",
    "# stochastic effect (the sampling of the sigma part of the z vector)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "#\n",
    "# instantiate encoder model\n",
    "# The \"encoder\" is everything from the input layer to the combined mean,sigma, and \n",
    "# sampled z vector\n",
    "encoder = Model(inputs=inputs, outputs=[z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "#\n",
    "# build decoder model\n",
    "# This does not actually specify the input - \"latent_inputs\" is just a placeholder\n",
    "# We just know that it is a vector of dimension \"latent_dim\"\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "#\n",
    "# This is an intermediate layer for the decoder\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "#\n",
    "# The output layer has sigmoid activation (just like our normal autoencoder)\n",
    "# and the same dimension as our input\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "# instantiate VAE model\n",
    "# ==> This next line defines the output as the output of the decoder (of course)\n",
    "# ==> The input to the decoder is the *third* output of the encoder ==> the z vector\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "# \n",
    "# We need to specify a \"custom\" loss, since we need both the reconstruction loss\n",
    "# *AND* the KL loss\n",
    "#\n",
    "# Setup two possible choices for the recon loss\n",
    "msa = False\n",
    "if mse:\n",
    "    reconstruction_loss = mse(inputs, outputs)\n",
    "else:\n",
    "    reconstruction_loss = binary_crossentropy(inputs,\n",
    "                                                  outputs)\n",
    "#\n",
    "# Scale by the input size - this is to keep it on the same scale\n",
    "# as the KL loss\n",
    "reconstruction_loss *= original_dim\n",
    "#\n",
    "# Now define the KL loss\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "\n",
    "#\n",
    "# USe a function to supply the loss we will give the fit method\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * metrics.binary_crossentropy(inputs,outputs)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "#\n",
    "# Now compile our model!\n",
    "vae.compile(optimizer='adam',loss=vae_loss)\n",
    "print(\"summary\")\n",
    "print(vae.summary())\n",
    "plot_model(vae,\n",
    "               to_file='vae_mlp.png',\n",
    "               show_shapes=True)\n",
    "# \n",
    "# Now fit our model\n",
    "history = vae.fit(train_images,train_images,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_data=(test_images, test_images))\n",
    "#vae.fit(x_train,\n",
    "#                epochs=epochs,\n",
    "#                batch_size=batch_size,\n",
    "#                validation_data=(x_test, None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();\n",
    "\n",
    "\n",
    "\n",
    "decoded_imgs = vae.predict(test_images)\n",
    "print(\"decoded_imgs.shape\",decoded_imgs.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(test_images[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Visualization of the Variational AE Space\n",
    "Given the separate encoder and decoder models, we can make two interesting plots:\n",
    "1. Encoder: Here we can give the encoder real images (we will use the labeled test images for this).  Remember from above that our encoder outputs the sampled $z$ vector, but **also** the z_mean (and log_z_sigma) vector.   If our latent dimension is at the default value above of 2, we can visualize the 2D space that our encoder discovered.   Note that similar looking digits fall close to one another in this image!\n",
    "2. Decoder: Here we can do something cool: since our encoder learned a functional representation of our latent space, we can provide the inputs of that function to the decoder.   Here we smoothly sample the 2D space of the z-vector from +/- 4 in both dimensions, and feed that vector to the decoder.  In the figure below, we do this in 30 steps, and then visualize the resulting 30x30 grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_mnist\"):\n",
    "    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n",
    "\n",
    "    # Arguments\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test, y_test = data\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "    # display a 30x30 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-4, 4, n)\n",
    "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "# Pass te encoder and decoder to the plotting routine\n",
    "models = (encoder, decoder)\n",
    "data = (test_images, test_labels)\n",
    "\n",
    "plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=batch_size,\n",
    "                 model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some things to try\n",
    "Play with with bottleneck dimension for both the standard autoencoder as well as the variational autoencoder.\n",
    "\n",
    "# Will this work?\n",
    "It should be possible to use a latent dimension of 3, and visualize the above plots (at least the first one) in 3D.   Try it!  (Note: I have not actually done this.... so it might not work!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "sys_python36conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
