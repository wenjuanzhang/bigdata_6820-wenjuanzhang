{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Convolutional Neural Networks\n",
    "We have been using **fully connected networks** (FCNs) to classify the MNIST dataset, and in the last assignment we designed a network which could do this with an accuracy of around 98%.   Convolutional Neural Networks, or Convnets, or CNNS (fake networks!), are another even more powerful tool for classifying images such as MNIST.   You might ask, what do Convnets do that FCNs can't?\n",
    "\n",
    "To understand this, let's take another look at our MNIST FCN.  If you have not already, examine and run the jupyter notebook in assignment10_prep called **train_fcn_model_mnist.ipynb**.   After you run this, you will have a stored version of the compiled and fit FCN called **fully_trained_model_fcn.h5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull in the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "short = False\n",
    "if short:\n",
    "    train_images = train_images[:7000,:]\n",
    "    train_labels = train_labels[:7000]\n",
    "    test_images = test_images[:3000,:]\n",
    "    test_labels = test_labels[:3000]\n",
    "#\n",
    "print(\"Train info\",train_images.shape, train_labels.shape)\n",
    "print(\"Test info\",test_images.shape, test_labels.shape)\n",
    "train_images = train_images.reshape((train_images.shape[0],28*28))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape((test_images.shape[0],28*28))\n",
    "test_images = test_images.astype('float32')/255\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels_cat = to_categorical(train_labels)\n",
    "test_labels_cat = to_categorical(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on the test set using our previous network\n",
    "We should get around 98% (will vary depending on the randomly initialized weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "network_name = 'fully_trained_model_fcn.h5'\n",
    "trained_network = load_model(network_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A method to get performance numbers\n",
    "The following method will be helpful later to get loss, accuracy, and the confusion matrix for our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#\n",
    "# Used to implement the multi-dimensional counter we need in the performance class\n",
    "from collections import defaultdict\n",
    "def autovivify(levels=1, final=dict):\n",
    "    return (defaultdict(final) if levels < 2 else\n",
    "            defaultdict(lambda: autovivify(levels-1, final)))\n",
    "def getPerformance(network,images,labels_cat,labels):\n",
    "#\n",
    "# Get the overall performance for the test sample\n",
    "    loss, acc = network.evaluate(images,labels_cat)\n",
    "#\n",
    "# Get the individual predictions for each sample in the test set\n",
    "    predictions = network.predict(images)\n",
    "#\n",
    "# Get the max probabilites for each rows\n",
    "    probs = np.max(predictions, axis = 1)\n",
    "#\n",
    "# Get the predicted classes for each row\n",
    "    classes = np.argmax(predictions, axis = 1)\n",
    "#\n",
    "# Now loop over the first twenty samples and compare truth to prediction\n",
    "#print(\"Label\\t Pred\\t Prob\")\n",
    "#for label,cl,pr in zip(smear_labels[:20],classes[:20],probs[:20]):\n",
    "#    print(label,'\\t',cl,'\\t',round(pr,3))\n",
    "#\n",
    "# Get confustion matrix\n",
    "    cf = autovivify(2,int)\n",
    "    for label,cl in zip(labels,classes):\n",
    "        cf[label][cl] += 1\n",
    "#\n",
    "    return loss,acc,cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,acc,cf = getPerformance(trained_network,test_images,test_labels_cat,test_labels)\n",
    "print(\"   Results\")\n",
    "print(\"   Loss,acc\",round(loss,4),round(acc,4))\n",
    "for trueClass in range(10):\n",
    "    print(\"   True: \",trueClass,end=\"\")\n",
    "    for predClass in range(10):\n",
    "        print(\" \\t\",cf[trueClass][predClass],end=\"\")\n",
    "    print()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we improve on this?\n",
    "When we ask about how we might improve on this performance, we should think about a few things:\n",
    "1.  How big is our network?   Does it scale to images larger than our 28x28x1 digit images?\n",
    "2.  How sensitive is our network to small (or even large) changes in our inputs?\n",
    "3.  Even if the previous two points are a problem, we can still ask if it is possible to improve our performance over the FCN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How big is our network\n",
    "Keras gives us a tool to get summary information about our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trained_network.input)\n",
    "print(trained_network.output)\n",
    "print(trained_network.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Number of parameters\n",
    "Notice that the two layers are called **dense**: these are fully connected layers, meaning there is a connection from every output of one layer to every input of the next layer.   Here is how we get the parameter counts:\n",
    "1.  dense_1 (the \"_1\" is just a label from when the network was created, it has no significance): We have 784 inputs each connected to 400 hidden nodes: 784*400=313600 parameters, plus another 400 \"bias\" parameters (1 for each node) which gives us a total of 314,000 parameters for the hidden layer.\n",
    "2.  dense_2: we have 400 inputs (one each from the hidden layer) connected to 10 outputs: 400*10 + 10(bias)=4010 parameters for the output layer.\n",
    "\n",
    "So we have 318,010 total parameters for a network which is used to classify small 28x28x1 greyscale images.   If we went to megapixel color images, we would have 1000x1000x3 = 3,000,000 input pixels, and if we have a 400 node hidden layer (which is probably too small), we end up with more than 1.2 billion parameters.... this does not scale!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity to variations in the input\n",
    "The types of variations we want to consider include:\n",
    "1.  Shifts of the input image (up/down and/or right/left).\n",
    "2.  Scaling of the input image (making it bigger or smaller (still within the 28x28 pixel window).\n",
    "3.  Rotations of the input image.\n",
    "We could also include shearing of the input image, but for now lets just consider the first 3.   \n",
    "\n",
    "Keras includes a method for performing all of these operations on an image.   Let's define a method to do this, using a single image as an input, and also define a method to display the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.preprocessing.image as kpi\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data_gen_args = dict(featurewise_center=True,\n",
    "                     featurewise_std_normalization=True,\n",
    "                     rotation_range=90,\n",
    "                     width_shift_range=0.1,\n",
    "                     height_shift_range=0.1,\n",
    "                     zoom_range=0.2)\n",
    "image_datagen = kpi.ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "def transform_image(img,tx=0,ty=0,zoom=1.0,rotation=0.0,shear=0.0):\n",
    "    transform_parameters = {}\n",
    "    orig_image = np.array(img, copy=True).reshape(28,28,1)\n",
    "    transform_parameters['theta'] = rotation\n",
    "    transform_parameters['zx'] = zoom\n",
    "    transform_parameters['zy'] = zoom\n",
    "    transform_parameters['tx'] = tx\n",
    "    transform_parameters['ty'] = ty\n",
    "    transform_parameters['shear'] = shear\n",
    "    orig_image = image_datagen.apply_transform(x=orig_image, transform_parameters=transform_parameters)\n",
    "    return orig_image\n",
    "\n",
    "def plot_image(img):\n",
    "    one_image = img.reshape(28,28)\n",
    "    plt.imshow(one_image, cmap='hot')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Pick a random image from our test dataset, and do the following:\n",
    "1.  Rotate 45 degrees.\n",
    "2.  Rotate 45 degrees plus zoom out (make the digit smaller)\n",
    "3.  Rotate 45 degrees plus zoom out plus translate the image to the upper corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability of the FCN to Variations in the input\n",
    "We are now ready to systematically answer the question: how well does the FCN handle images that are slight (or not-so-slight) variations of the data it was trained on.   \n",
    "\n",
    "Here is what we will do: \n",
    "1.  Loop over every image in the test set\n",
    "2.  Choose a random +/- shift (in x and y) from a subset (0-4 pixels in increments of 1).\n",
    "3.  Randomly shift the image over that shift.\n",
    "4.  Store the transformed image in a list\n",
    "When we are done, we will run that list of images through our original FCN and note the performance, comparing it to the original performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "for shift in [0,1,2,3,4]:\n",
    "    print()\n",
    "    print(\"Shift \",shift)\n",
    "    imgList = []\n",
    "    count = 0\n",
    "    for img in test_images[:]:\n",
    "        if random.uniform(0.0,1.0) > 0.5:\n",
    "            tx = shift\n",
    "        else:\n",
    "            tx = -shift\n",
    "        if random.uniform(0.0,1.0) > 0.5:\n",
    "            ty = shift\n",
    "        else:\n",
    "            ty = -shift\n",
    "#        tx = random.uniform(-shift,shift)\n",
    "#        ty = random.uniform(-shift,shift)\n",
    "        trans_image = transform_image(img,tx=tx,ty=ty,zoom=1.0,rotation=0.0,shear=0.0)\n",
    "        imgList.append(trans_image)\n",
    "#\n",
    "# Convert to np array\n",
    "    npa_images = np.asarray(imgList, dtype=np.float32)\n",
    "    npa_images = npa_images.reshape((npa_images.shape[0],28*28))\n",
    "#\n",
    "    smear_loss,smear_acc,smear_cf = getPerformance(trained_network,npa_images,test_labels_cat,test_labels)\n",
    "    print(\"   Results\")\n",
    "    print(\"   Loss,acc\",smear_loss,smear_acc)\n",
    "    for trueClass in range(10):\n",
    "        print(\"   True: \",trueClass,end=\"\")\n",
    "        for predClass in range(10):\n",
    "            print(\" \\t\",smear_cf[trueClass][predClass],end=\"\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is bad.** As soon as we get 2 pixels out, the performance drops by almost 30%!   So the FCN is not stable at all against small variations in input.\n",
    "\n",
    "## Task 2: Test FCN Stability Further\n",
    "Using a similar strategy as above, try the following:\n",
    "1.  Rotations in the range: [0.0,20.0,40.0,60.0,80.0] (keep shifts=0)\n",
    "2.  Zooms in the range[1.0,1.25,1.5,1.75,2.0] (keep shifts and rotations=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcomings of FCNs\n",
    "We see that standard, fully-connected neural networks, although powerful, have some clear shortcomings when applied to image classification:\n",
    "1.  They do not scale well.  Reasonable-sized images would require an enormous number of parameters.   This in turn would require a corresponding increase in the number of training samples in order to determine the parameters accurately.\n",
    "2.  They are dependent on the specific pixel relationships within the image.   Performance degrades substantially as soon as there is a minor devation from these relationships.\n",
    "\n",
    "Both of these issues are related: the FCN does not take advantage of the fact that - generally - in image classification, the images tend to be built from underlying common features.   In the case of MNIST images, these are the curves and lines and corners which make up the individual digits.   Convnets attempt to take advantage of these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Convnet\n",
    "Before explaining how convnets work, lets try to build a simple network to classify MNIST images.\n",
    "\n",
    "For comparison, we first show the code we use to build and train an FCN, followed by similar code to build and train a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "#\n",
    "# Make sure the shape of the input is correct\n",
    "train_images = train_images.reshape((train_images.shape[0],28*28))\n",
    "test_images = test_images.reshape((test_images.shape[0],28*28))\n",
    "\n",
    "fcn_network = models.Sequential()\n",
    "#\n",
    "# Hidden\n",
    "fcn_network.add(layers.Dense(400,activation='tanh',input_shape=(28*28,)))\n",
    "#\n",
    "# Output\n",
    "fcn_network.add(layers.Dense(10,activation='softmax'))\n",
    "#\n",
    "# Compile\n",
    "fcn_network.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# \n",
    "# Fit/save/print summary\n",
    "history = fcn_network.fit(train_images,train_labels_cat,epochs=15,batch_size=128,validation_data=(test_images,test_labels_cat))\n",
    "fcn_network.save('fully_trained_model_fcn.h5')\n",
    "print(fcn_network.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "#\n",
    "# Make sure the shape of the input is correct (the last \",1\" is the number of \"channels\"=1 for grayscale)\n",
    "train_images = train_images.reshape((train_images.shape[0],28,28,1))\n",
    "test_images = test_images.reshape((test_images.shape[0],28,28,1))\n",
    "#\n",
    "cnn_network = models.Sequential()\n",
    "#\n",
    "# First convolutional layer\n",
    "cnn_network.add(layers.Conv2D(30,(5,5),activation='relu',input_shape=(28,28,1)))\n",
    "# Pool\n",
    "cnn_network.add(layers.MaxPooling2D((2,2)))\n",
    "#\n",
    "# Second convolutional layer\n",
    "cnn_network.add(layers.Conv2D(25,(5,5),activation='relu'))\n",
    "# Pool\n",
    "cnn_network.add(layers.MaxPooling2D((2,2)))\n",
    "#\n",
    "# Connect to a dense output layer - just like an FCN\n",
    "cnn_network.add(layers.Flatten())\n",
    "cnn_network.add(layers.Dense(64,activation='relu'))\n",
    "cnn_network.add(layers.Dense(10,activation='softmax'))\n",
    "#\n",
    "# Compile\n",
    "cnn_network.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#\n",
    "# Fit/save/print summary\n",
    "history = cnn_network.fit(train_images,train_labels_cat,epochs=5,batch_size=256,validation_data=(test_images,test_labels_cat))\n",
    "cnn_network.save('fully_trained_model_cnn.h5')\n",
    "print(cnn_network.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Get the overall performance for the test sample\n",
    "test_loss, test_acc = cnn_network.evaluate(test_images,test_labels_cat)\n",
    "print(\"Test sample loss: \",test_loss, \"; Test sample accuracy: \",test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of CNN and FCN\n",
    "There are a couple of things to notice when comparing the output from the two code blocks above:\n",
    "1.  The performance of the CNN is better than the FCN after 5 epochs.  A careful examination of the training set accuracies reveals that the CNN is still undertrained (and so can perform better if we increase the number of epochs).\n",
    "2.  The number of parameters needed to specify the CNN is 40.3k, about 7 times smaller than the FCN!\n",
    "3.  The training time per step is much longer (about 10x) for the CNN than it is for the FCN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Test CNN Stability \n",
    "What we don't know yet, is how stable the CNN is to variations in the input images.\n",
    "Using a similar strategy as we used above for the FCN, calaculate the performance under the following variations:\n",
    "1.  Shifts in the range [0,1,2,3,4]\n",
    "2.  Rotations in the range: [0.0,20.0,40.0,60.0,80.0] (keep shifts=0)\n",
    "3.  Zooms in the range[1.0,1.25,1.5,1.75,2.0] (keep shifts and rotations=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
