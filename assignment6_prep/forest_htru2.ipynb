{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forest_htru2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "H-XII8KKiNzp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Random Forests\n",
        "In this workbook, we will examine **random forests**.   As you might expect, random forests are a collection of **decision trees**.   More precisely, random forests are an ensemble of decision trees, designed to overcome two drawbacks of decsion trees:\n",
        "1.  Decision trees are prone to overfitting the data.  Now this is a limitation that can be overcome, as we saw in the previous example, but may be more difficult depending on sample size and the nature of the problem you are working on.\n",
        "2.  Decision trees are sensitive to the actual distribution of the training data, such that small perturbations in one single distribution can yield a very different tree.\n",
        "\n",
        "The basic idea is very simple: instead of training a single tree, we will train a large number of trees and average the results.   But of course if we use the same data, we will get the same trees.   To overcome this, **random** forests introduce the following ideas:\n",
        "1.  Randomize the training data.   The technique used is called **bootstrap aggregating** or **bagging** for short.   If we have **n** samples in our training data, we select **n** samples **with replacement** from that same dataset.   On average this will choose about 63% of the training data for each tree (meaning that there are copies in the training set).   The key idea is that each tree in the forest sees a different (but overlapping) subset of the data.\n",
        "2.  Randomize the features used by each tree in the forest.  By default, the random forests in scikit use only a subset of the available features (by default the rounded sqrt(number of input features)) when deciding which feature to use to split the data at each node in a given tree.\n",
        "\n",
        "How are the results of all of these tree combined?   As we saw with our decision tree workbook, a decision tree can predict a class (0,1,2, etc), or a set of probabilities.   When combining an ensemble of trees we have two options to predict the result for a single sample:\n",
        "1.  Hard voting: use the class predicted by most of the trees.\n",
        "2.  Soft voting: average the probabilites, and use the class with the highest probability.   This allows higher performance by giving greater weight to more confident predicitons.   Sklearn uses this method for random forests."
      ]
    },
    {
      "metadata": {
        "id": "TrW0n0ghoYgl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature Importance\n",
        "A very useful property of random forests is that they can be used to assess **feature importance**.   The idea is that since different trees use different subsets of available features at each split point within each tree, we have a mechanism for assessing how important a specific feature is for our model.   A feature's importance is related to how much that feature reduces impurity on average across all of the trees in the forest.\n",
        "\n",
        "To help us test this idea, we will add a **random** feature to our dataset.   This feature should neither help nor hurt our classification, and when we  check our feature importances later, we would expect that the random feature will be ranked very low in feature importance.\n"
      ]
    },
    {
      "metadata": {
        "id": "NvfjJRFB0yRQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Getting the data:\n",
        "We will again use the puslar data as we did for the decision tree workbook.   The data come from the UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/HTRU2\n",
        "\n",
        "As usual the data is on github: 'https://raw.githubusercontent.com/big-data-analytics-physics/data/master/HTRU2/HTRU_2a.csv'\n",
        "\n",
        "As noted above, will will add a separate column labeled \"random\" which is essentailly a *fake* feature. "
      ]
    },
    {
      "metadata": {
        "id": "luIJ0Dguwewg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#\n",
        "# Read in all of the other digits\n",
        "fname = 'https://raw.githubusercontent.com/big-data-analytics-physics/data/master/HTRU2/HTRU_2a.csv'\n",
        "dfAll = pd.read_csv(fname)\n",
        "#\n",
        "# Add a feature called \"random\" with random numbers from 0-1\n",
        "dfAll['random'] = np.random.random(size=len(dfAll))\n",
        "print(dfAll.head(5))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "okYk61Ix0hHF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reorder data columns\n",
        "Our code below expects the **last** column to have the **class** label.  Since we added the **random** column this is not longer true, so in the code below we reorder the columns."
      ]
    },
    {
      "metadata": {
        "id": "icnD0lyr7j3E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dfAll = dfAll[['Profile_mean','Profile_stdev','Profile_skewness','Profile_kurtosis','DM_mean',\n",
        "        'DM_stdev','DM_skewness','DM_kurtosis','random','class']]\n",
        "num_features = 9\n",
        "print(dfAll.head(5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h0m8yoxK0ef1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Defining Signal\n",
        "The **class** variable distinguished signal from background.   As usual,  **1** is signal (pulsars) and **0** is background.   We make sure the sample is balanced so we have equal numbers of pulsars and non-pulsars."
      ]
    },
    {
      "metadata": {
        "id": "cZVQuKLBTV3T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# The data already has a 0/1 class variable that defines signal (1) and background (0)\n",
        "#\n",
        "# The data is already combined but it will be usefull to split it so we can look at \n",
        "# signal and background separately.\n",
        "dfA = dfAll[dfAll['class']==1]\n",
        "dfB = dfAll[dfAll['class']==0]\n",
        "\n",
        "print(\"Length of signal sample:     \",len(dfA))\n",
        "print(\"Length of background sample: \",len(dfB))\n",
        "\n",
        "#\n",
        "# Shuffle the data here\n",
        "from sklearn.utils import shuffle\n",
        "dfBShuffle = shuffle(dfB)\n",
        "#\n",
        "# Uncomment the next line to limit dfB to be the same length as dfA\n",
        "#dfB_use = dfBShuffle\n",
        "dfB_use = dfBShuffle.head(len(dfA))\n",
        "\n",
        "\n",
        "dfCombined = dfB_use\n",
        "dfCombined = pd.concat([dfCombined, dfA])\n",
        "dfCombined = shuffle(dfCombined)\n",
        "\n",
        "print(\"Size of signal sample \",len(dfA))\n",
        "print(\"Size of background sample \",len(dfB_use))\n",
        "print(\"Size of combined sample \",len(dfCombined))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lvhYHv5X2K_p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Some useful methods\n",
        "We have our usual helpful methods of autovivify and enable_plotly_in_cell, as well as getDecisionTreeGraphic, which we can use to visualize an individual tree within a forest."
      ]
    },
    {
      "metadata": {
        "id": "2qH7R-6S2OVm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# This allows multidimensional counters (and other more complicated strucutres!)\n",
        "from collections import defaultdict\n",
        "def autovivify(levels=1, final=dict):\n",
        "    return (defaultdict(final) if levels < 2 else\n",
        "            defaultdict(lambda: autovivify(levels-1, final)))\n",
        "  \n",
        "def enable_plotly_in_cell():\n",
        "  import IPython\n",
        "  from plotly.offline import init_notebook_mode\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "  '''))\n",
        "  init_notebook_mode(connected=False)\n",
        "  \n",
        "def getDecisionTreeGraphic(estimator,feature_names,class_names):\n",
        "  from sklearn import tree\n",
        "  from io import StringIO\n",
        "  import pydot_ng as pydot \n",
        "  import graphviz\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  dot_data = StringIO()\n",
        "\n",
        "  #                         class_names=classColumn,\n",
        "  out = tree.export_graphviz(estimator,out_file=None,\n",
        "                           feature_names=feature_names,\n",
        "                           class_names=class_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True,\n",
        "                            node_ids=1,)\n",
        "  import pydotplus\n",
        "  pydot_graph = pydotplus.graph_from_dot_data(out)\n",
        "  pydot_graph.set_size('\"7,7!\"')\n",
        "  #print(pydot_graph.getvalue())\n",
        "  #graph = graphviz.Source( out)\n",
        "  graph = graphviz.Source(pydot_graph.to_string())\n",
        "  return graph\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fmcl8xlvFDKI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Performance Method"
      ]
    },
    {
      "metadata": {
        "id": "G1udjEbIFGHt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Determine the performance\n",
        "def binaryPerformance(y,y_pred,y_score,debug=False):\n",
        "#\n",
        "# Assuming a binary classifier with 1=signal, 0=background\n",
        "  confusionMatrix = autovivify(2,int)\n",
        "  for i in range(len(y_pred)):\n",
        "    trueClass = y[i]\n",
        "    predClass = y_pred[i]\n",
        "    confusionMatrix[trueClass][predClass] += 1\n",
        "\n",
        "  if debug:\n",
        "    for trueClass in range(2):\n",
        "      print(\"True: \",trueClass,end=\"\")\n",
        "      for predClass in range(2):\n",
        "        print(\"\\t\",confusionMatrix[trueClass][predClass],end=\"\")\n",
        "      print()\n",
        "    print()\n",
        "  TP = confusionMatrix[1][1]\n",
        "  FP = confusionMatrix[0][1]\n",
        "  FN = confusionMatrix[1][0]\n",
        "  TN = confusionMatrix[0][0]\n",
        "  \n",
        "  if debug:\n",
        "    print(\"TP predicted true, actually true   \",TP)\n",
        "    print(\"FP predicted true, acutally false  \",FP)\n",
        "    print(\"TN predicted false, actually false \",TN)\n",
        "    print(\"FN predicted false, actually true  \",FN)\n",
        "\n",
        "\n",
        "  precision = TP / (TP + FP)\n",
        "  recall = TP / (TP + FN)\n",
        "  f1_score = 2.0 / ( (1.0/precision) + (1.0/recall) )\n",
        "  \n",
        "  if debug:\n",
        "    print(\"Precision = TP/(TP+FP) = fraction of predicted true actually true \",precision)\n",
        "    print(\"Recall = TP/(TP+FN) = fraction of true class predicted to be true \",recall)\n",
        "    print(\"F1 score = \",f1_score)\n",
        "\n",
        "  #\n",
        "  # Get the ROC curve.  We will use the sklearn function to do this\n",
        "  from sklearn import metrics\n",
        "  #fpr_train, tpr_train, thresholds_train = metrics.roc_curve(y_train, y_train_score, pos_label=1)\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y, y_score, pos_label=1)\n",
        "  #\n",
        "  # Get the auc\n",
        "  auc = metrics.roc_auc_score(y, y_score)\n",
        "  if debug:\n",
        "    print(\"AUC this sample: \",auc)\n",
        "  \n",
        "  return precision,recall,auc,fpr, tpr, thresholds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIhQBrAXz3V8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  The runFitter Method\n",
        "We will use the same form of the runFitter method we used in our decision tree workbook."
      ]
    },
    {
      "metadata": {
        "id": "PrB1nvOxz5gq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def runFitter(estimator,X_train,y_train,X_test,y_test,debug=False):\n",
        "#\n",
        "# Now fit to our training set\n",
        "  estimator.fit(X_train,y_train)\n",
        "#\n",
        "# Now predict the classes and get the score for our traing set\n",
        "  y_train_pred = estimator.predict(X_train)\n",
        "  y_train_score = estimator.predict_proba(X_train)[:,1]   # NOTE: some estimators have a predict_prob method instead od descision_function\n",
        "#\n",
        "# Now predict the classes and get the score for our test set\n",
        "  y_test_pred = estimator.predict(X_test)\n",
        "  y_test_score = estimator.predict_proba(X_test)[:,1]\n",
        "\n",
        "#\n",
        "# Now get the performaance\n",
        "  precision_test,recall_test,auc_test,fpr_test, tpr_test, thresholds_test\\\n",
        "    = binaryPerformance(y_test,y_test_pred,y_test_score,debug)\n",
        "  precision_train,recall_train,auc_train,fpr_train, tpr_train, thresholds_train\\\n",
        "    = binaryPerformance(y_train,y_train_pred,y_train_score,debug)\n",
        "#\n",
        "# Decide what you want to return: for now, just precision, recall, and auc for both test and train\n",
        "  results = {\n",
        "      'precision_train':precision_train,\n",
        "      'recall_train':recall_train,\n",
        "      'auc_train':auc_train,\n",
        "      'fpr_train':fpr_train, \n",
        "      'tpr_train':tpr_train, \n",
        "      'thresholds_train':thresholds_train,\n",
        "      'precision_test':precision_test,\n",
        "      'recall_test':recall_test,\n",
        "      'auc_test':auc_test,\n",
        "      'fpr_test':fpr_test, \n",
        "      'tpr_test':tpr_test, \n",
        "      'thresholds_test':thresholds_test}\n",
        "\n",
        "  return results\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lfCPNlDBGEzp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prepare the data\n",
        "As usual we shuffle it first and then dump the dataframe into an **X** features numpy array and a **y** labels number array."
      ]
    },
    {
      "metadata": {
        "id": "8BMr0hJsjM1c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "dfCombinedShuffle = shuffle(dfCombined,random_state=42)    # by setting the random state we will get reproducible results\n",
        "\n",
        "X = dfCombinedShuffle.as_matrix(columns=dfCombinedShuffle.columns[:num_features])\n",
        "y = dfCombinedShuffle['class'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BmJAtZwNn39r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training a single random forest, with k-fold validation.\n",
        "Let's train a single random forest estimator, including cross validation.   But first we have to define the settings for the random forest estimator.   Here is the call definition from sklearn:\n",
        "\n",
        "    class sklearn.ensemble.RandomForestClassifier(n_estimators=’warn’, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
        "    \n",
        "You will notice many parameters which are there to control the individual trees in the forest, such as **max_depth**,  **max_leaf_nodes** etc.   These serve exaclty the same purpose as described in the decision tree workbook.  We will only modify one of these - the **max_depth**  of the trees.   \n",
        "\n",
        "The other primary parameter is **n_estimators** which is simply the number of trees in the forest.   We will typically set this to 100, but forests with 500, 1000, or even 5000 trees are not uncommon.   The bigger the forest the longer it will take to train.   This can be parallelized, which is what **n_jobs** is for.   For now, we will leave this and all of the other parameters at their default values, unless specified otherwise."
      ]
    },
    {
      "metadata": {
        "id": "A6quxoM0oZyi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "kfolds = 5\n",
        "skf = StratifiedKFold(n_splits=kfolds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ByI13WOobjS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "estimator = RandomForestClassifier(n_estimators=100, max_depth=6,random_state=42)\n",
        "\n",
        "avg_precision_train = 0.0\n",
        "avg_recall_train = 0.0\n",
        "avg_auc_train = 0.0\n",
        "avg_precision_test = 0.0\n",
        "avg_recall_test = 0.0\n",
        "avg_auc_test = 0.0\n",
        "numSplits = 0.0\n",
        "#\n",
        "# Now loop\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "  print(\"Training\")\n",
        "  numSplits += 1\n",
        "  X_train = X[train_index]\n",
        "  y_train = y[train_index]\n",
        "  X_test = X[test_index]\n",
        "  y_test = y[test_index]\n",
        "  \n",
        "#\n",
        "# Now fit to our training set\n",
        "  results = runFitter(estimator,X_train,y_train,X_test,y_test,debug=False)\n",
        "\n",
        "  avg_precision_train += results['precision_train']\n",
        "  avg_recall_train += results['recall_train']\n",
        "  avg_auc_train += results['auc_train']\n",
        "#\n",
        "  avg_precision_test += results['precision_test']\n",
        "  avg_recall_test += results['recall_test']\n",
        "  avg_auc_test += results['auc_test']\n",
        "#\n",
        "avg_precision_train /= numSplits\n",
        "avg_recall_train /= numSplits\n",
        "avg_auc_train /= numSplits\n",
        "avg_precision_test /= numSplits\n",
        "avg_recall_test /= numSplits\n",
        "avg_auc_test /= numSplits\n",
        "# \n",
        "# Now print\n",
        "print(\"Precision train/test \",round(avg_precision_train,3),round(avg_precision_test,3))\n",
        "print(\"Recall train/test    \",round(avg_recall_train,3),round(avg_recall_test,3))\n",
        "print(\"AUC train/test       \",round(avg_auc_train,3),round(avg_auc_test,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4mxLY47RPqZP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Overfitting vs Underfitting\n",
        "As with decision trees, we would like to make our parameter choices such that we are neither significantly over or under-fitting our training data.   \n",
        "\n",
        "To keep things simple, we will again vary only one parameter - the max_depth of the trees in the forest.   To choose the optimal point, we will plot both training and testing error vs max_depth.   \n",
        "\n",
        "We will again use 1-precision, 1-recall, and 1-AUC, as a measure of the **error** in our models."
      ]
    },
    {
      "metadata": {
        "id": "eg-04NVuRD0j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Get our estimator and predict\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "#\n",
        "# Create a dataframe to store our results\n",
        "dfError = pd.DataFrame(columns=['max_depth','trainError_pre','testError_pre',\n",
        "                                    'trainError_rec','testError_rec',\n",
        "                                    'trainError_auc','testError_auc'])\n",
        "\n",
        "for max_depth in range(1,20):\n",
        "  print(\"training with max depth =\",max_depth)\n",
        "  estimator = RandomForestClassifier(n_estimators=100,random_state=42,max_depth=max_depth)\n",
        "  avg_precision_train = 0.0\n",
        "  avg_recall_train = 0.0\n",
        "  avg_auc_train = 0.0\n",
        "  avg_precision_test = 0.0\n",
        "  avg_recall_test = 0.0\n",
        "  avg_auc_test = 0.0\n",
        "  numSplits = 0.0\n",
        "#\n",
        "# Now loop\n",
        "  for train_index, test_index in skf.split(X, y):\n",
        "    numSplits += 1\n",
        "    X_train = X[train_index]\n",
        "    y_train = y[train_index]\n",
        "    X_test = X[test_index]\n",
        "    y_test = y[test_index]\n",
        "\n",
        "#\n",
        "# Now fit to our training set\n",
        "    results = runFitter(estimator,X_train,y_train,X_test,y_test,debug=False)\n",
        "    #print(\"results\",results)\n",
        "\n",
        "    avg_precision_train += results['precision_train']\n",
        "    avg_recall_train += results['recall_train']\n",
        "    avg_auc_train += results['auc_train']\n",
        "#\n",
        "    avg_precision_test += results['precision_test']\n",
        "    avg_recall_test += results['recall_test']\n",
        "    avg_auc_test += results['auc_test']\n",
        "#\n",
        "  avg_precision_train /= numSplits\n",
        "  avg_recall_train /= numSplits\n",
        "  avg_auc_train /= numSplits\n",
        "  avg_precision_test /= numSplits\n",
        "  avg_recall_test /= numSplits\n",
        "  avg_auc_test /= numSplits\n",
        "#\n",
        "# Fill dataframe\n",
        "  dfError = dfError.append({\n",
        "     'max_depth':max_depth,\n",
        "     'trainError_pre':1.0-avg_precision_train,'testError_pre':1.0-avg_precision_test,\n",
        "     'trainError_rec':1.0-avg_recall_train,'testError_rec':1.0-avg_recall_test,\n",
        "     'trainError_auc':1.0-avg_auc_train,'testError_auc':1.0-avg_auc_test\n",
        "      }, ignore_index=True)\n",
        "# \n",
        "# Now print\n",
        "print(dfError.head)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bBV3cwhiTYyC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Plotting results\n",
        "\n",
        "Lets look at the error measures:\n",
        "1.  train/test Error using precision: remember that **precision** = TP/(TP+FP) = \"Total actual positive found as positive\" / \"Total our model identified as positive\".   It is the fraction of identified postives that are truly positive.   **(1-precision)** is then the **error** - the fraction of our identified postives that are incorrect.\n",
        "2.  train/test Error using recall: remember that **recall** = TP/(TP+FN) = \"Total actual positive found as positive\" / \"Total actual positive\".   It is the fraction of actual positives that our model managed to identify.   **(1-recall)** is then the **error** - the fraction of actual postives that we failed to identify.\n",
        "3.  train/test Error using AUC: remember that AUC measures the probability that a randomly chosen positive example is properly ranked above a randomly chosen negative example.   **(1-AUC)** is then the probability that we will fail to do this."
      ]
    },
    {
      "metadata": {
        "id": "tmvKv77hToRD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import plotly.plotly as py\n",
        "import numpy as np\n",
        "from plotly.offline import iplot\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "enable_plotly_in_cell()\n",
        "# Create a trace\n",
        "trace1 = go.Scatter(\n",
        "    x = dfError['max_depth'],\n",
        "    y = dfError['trainError_pre'],\n",
        "    mode = 'line',\n",
        "    name = \"Training error\"\n",
        ")\n",
        "# Create a trace\n",
        "trace2 = go.Scatter(\n",
        "    x = dfError['max_depth'],\n",
        "    y = dfError['testError_pre'],\n",
        "    mode = 'line',\n",
        "    name = \"Testing Error\"\n",
        ")\n",
        "\n",
        "layout = dict(\n",
        "    title='Error (Precision) vs Model Complexity',\n",
        "    xaxis=dict(title='max_depth'),\n",
        "    yaxis=dict(title='Error (fraction)')\n",
        ")\n",
        "\n",
        "data = [trace1, trace2]\n",
        "iplot(dict(data=data,layout=layout),validate=False)\n",
        "\n",
        "# Create a trace\n",
        "trace1 = go.Scatter(\n",
        "    x = dfError['max_depth'],\n",
        "    y = dfError['trainError_rec'],\n",
        "    mode = 'line',\n",
        "    name = \"Training error\"\n",
        ")\n",
        "# Create a trace\n",
        "trace2 = go.Scatter(\n",
        "    x = dfError['max_depth'],\n",
        "    y = dfError['testError_rec'],\n",
        "    mode = 'line',\n",
        "    name = \"Testing Error\"\n",
        ")\n",
        "\n",
        "layout = dict(\n",
        "    title='Error (Recall) vs Model Complexity',\n",
        "    xaxis=dict(title='max_depth'),\n",
        "    yaxis=dict(title='Error (fraction)')\n",
        ")\n",
        "\n",
        "data = [trace1, trace2]\n",
        "iplot(dict(data=data,layout=layout),validate=False)\n",
        "# Create a trace\n",
        "trace1 = go.Scatter(\n",
        "    x = dfError['max_depth'],\n",
        "    y = dfError['trainError_auc'],\n",
        "    mode = 'line',\n",
        "    name = \"Training error\"\n",
        ")\n",
        "# Create a trace\n",
        "trace2 = go.Scatter(\n",
        "    x = dfError['max_depth'],\n",
        "    y = dfError['testError_auc'],\n",
        "    mode = 'line',\n",
        "    name = \"Testing Error\"\n",
        ")\n",
        "\n",
        "layout = dict(\n",
        "    title='Error (AUC) vs Model Complexity',\n",
        "    xaxis=dict(title='max_depth'),\n",
        "    yaxis=dict(title='Error (fraction)')\n",
        ")\n",
        "\n",
        "data = [trace1, trace2]\n",
        "iplot(dict(data=data,layout=layout),validate=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gZF3LyIWHV_m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature Importance (1)\n",
        "A very useful property of random forests is that they can be used to assess **feature importance**.   To get this from a trained random forest, we need to first train a single random forest, using our optimal settings for our hyperparameters.\n",
        "\n",
        "Note that in the code below:\n",
        "1.  We call runFitter once.\n",
        "2.  We feed runFitter the same X,y arrays for both training and testing (since we don't really care about the testing set anymore).\n",
        "3.  Remember that the expected performance is **not** the result printed out below, but the avereged result from k-fold validation from above (at our optimal hyperparameter settings).\n"
      ]
    },
    {
      "metadata": {
        "id": "vAUqquMXt2tK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "dfCombinedShuffle = shuffle(dfCombined,random_state=42)    # by setting the random state we will get reproducible results\n",
        "\n",
        "X = dfCombinedShuffle.as_matrix(columns=dfCombinedShuffle.columns[:num_features])\n",
        "y = dfCombinedShuffle['class'].values\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "estimator = RandomForestClassifier(n_estimators=100, max_depth=6,random_state=42,oob_score=True)\n",
        "\n",
        "results = runFitter(estimator,X,y,X,y,debug=False)\n",
        "\n",
        "print(\"Precision:\",results['precision_train'])\n",
        "print(\"Recall:   \",results['recall_train'])\n",
        "print(\"AUC:      \",results['auc_train'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SkyPt0Ab7jjb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature Importance (2)\n",
        "Now that we have trained our random forest estimator, we can get the list of feature importances by accessing the **attribute** of our estimator like this:\n",
        "    * estimator.feature_importances_\n",
        "\n",
        "This is a **list**, ordered exactly as our features that we fed to the estimator - so it has the same order as **dfCombinedShuffle.columns** (up to but not including the **class** column).\n",
        "\n",
        "The code below shows how to access the feaure importance, as well as how to order it from maximum to minimum.\n",
        "\n",
        "There is a very interesting discussion of issues around the calculation of feature importances here:  https://explained.ai/rf-importance/index.html"
      ]
    },
    {
      "metadata": {
        "id": "zDK4jv8cwBk_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "importanceByName = {}\n",
        "print(\"unsorted importance\")\n",
        "for name,importance in zip(dfCombinedShuffle.columns[:num_features],estimator.feature_importances_):\n",
        "  importanceByName[name] = importance\n",
        "  print(\"Name,importance\",name,round(importance,3))\n",
        "#\n",
        "# Now sort and print\n",
        "print()\n",
        "print(\"Sorted importance\")\n",
        "for name in sorted(importanceByName, key=importanceByName.get, reverse=True):\n",
        "  print(\"Name,importance\",name,round(importanceByName[name],3))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hrc3GGnT9P17",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Additional things to try:\n",
        "Here are some things to explore to make sure you understand how things work:\n",
        "1.   You can add a second random variable to the features.   Do both random features end up at the bottom of importances?\n",
        "2.  You can remove the random variable and make sure your results are unaffected.\n",
        "3.   You can try different settings of trees: ntrees=10, or ntrees=200.   How much do your results change?"
      ]
    }
  ]
}