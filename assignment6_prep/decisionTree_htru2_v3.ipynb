{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decisionTree_htru2_v3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "H-XII8KKiNzp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Decision Trees, Over- and Under-fitting, and the Bias-Variance Tradeoff\n",
        "In this workbook, we will examine **decision trees**, a relatively straighforward technique for classification.   We will use the example dataset and model to introduce the ideas of over and underfitting a model, and the relationship of these ideas to bias and variance.\n",
        "\n",
        "The data we will use is a public sample of pulsar data.   It includes feature data on approximately 18k human-labeled samples.   A full description of the data and the underlying analysis can be found in this thesis:\n",
        "http://www.scienceguyrob.com/wp-content/uploads/2016/12/WhyArePulsarsHardToFind_Lyon_2016.pdf\n",
        "\n",
        "The data come from the UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/HTRU2"
      ]
    },
    {
      "metadata": {
        "id": "lvhYHv5X2K_p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Some useful methods\n",
        "We have our usual helpful methods of **autovivify** and **enable_plotly_in_cell**.\n",
        "\n",
        "We introduce a new method: **getDecisionTreeGraphic**.   This method will allow us to visualize a Decision Tree classifier.\n"
      ]
    },
    {
      "metadata": {
        "id": "2qH7R-6S2OVm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# This allows multidimensional counters (and other more complicated strucutres!)\n",
        "from collections import defaultdict\n",
        "def autovivify(levels=1, final=dict):\n",
        "    return (defaultdict(final) if levels < 2 else\n",
        "            defaultdict(lambda: autovivify(levels-1, final)))\n",
        "  \n",
        "def enable_plotly_in_cell():\n",
        "  import IPython\n",
        "  from plotly.offline import init_notebook_mode\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "  '''))\n",
        "  init_notebook_mode(connected=False)\n",
        "  \n",
        "def getDecisionTreeGraphic(estimator,feature_names,class_names):\n",
        "  from sklearn import tree\n",
        "  from io import StringIO\n",
        "  import pydot_ng as pydot \n",
        "  import graphviz\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  dot_data = StringIO()\n",
        "\n",
        "  #                         class_names=classColumn,\n",
        "  out = tree.export_graphviz(estimator,out_file=None,\n",
        "                           feature_names=feature_names,\n",
        "                           class_names=class_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True,\n",
        "                            node_ids=1,)\n",
        "  import pydotplus\n",
        "  pydot_graph = pydotplus.graph_from_dot_data(out)\n",
        "  pydot_graph.set_size('\"7,7!\"')\n",
        "  #print(pydot_graph.getvalue())\n",
        "  #graph = graphviz.Source( out)\n",
        "  graph = graphviz.Source(pydot_graph.to_string())\n",
        "  return graph\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NvfjJRFB0yRQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Getting the data:\n",
        "As usual the data is on github.   "
      ]
    },
    {
      "metadata": {
        "id": "luIJ0Dguwewg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#\n",
        "# Read in all of the other digits\n",
        "fname = 'https://raw.githubusercontent.com/big-data-analytics-physics/data/master/HTRU2/HTRU_2a.csv'\n",
        "dfAll = pd.read_csv(fname)\n",
        "print(dfAll.head(5))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h0m8yoxK0ef1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Defining Signal\n",
        "The **class** variable distinguishes signal from background.   As usual,  **1** is signal (pulsars) and **0** is background."
      ]
    },
    {
      "metadata": {
        "id": "cZVQuKLBTV3T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# The data already has a 0/1 class variable that defines signal (1) and background (0)\n",
        "#\n",
        "# The data is already combined but it will be usefull to split it so we can look at \n",
        "# signal and background separately.\n",
        "dfA = dfAll[dfAll['class']==1]\n",
        "dfB = dfAll[dfAll['class']==0]\n",
        "\n",
        "print(\"Length of signal sample:     \",len(dfA))\n",
        "print(\"Length of background sample: \",len(dfB))\n",
        "\n",
        "#\n",
        "# Shuffle the data here\n",
        "from sklearn.utils import shuffle\n",
        "dfBShuffle = shuffle(dfB)\n",
        "#\n",
        "# Uncomment the next line to limit dfB to be the same length as dfA\n",
        "#dfB_use = dfBShuffle\n",
        "dfB_use = dfBShuffle.head(len(dfA))\n",
        "\n",
        "\n",
        "dfCombined = dfB_use\n",
        "dfCombined = pd.concat([dfCombined, dfA])\n",
        "dfCombined = shuffle(dfCombined)\n",
        "\n",
        "print(\"Size of signal sample \",len(dfA))\n",
        "print(\"Size of background sample \",len(dfB_use))\n",
        "print(\"Size of combined sample \",len(dfCombined))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4IPMlRrLictw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Examine the Data\n",
        "\n",
        "Let's do a quick visualization of the correlations - we can look at which variables ar emore strongly correlated with the **class** varaible."
      ]
    },
    {
      "metadata": {
        "id": "TydTfq902ypY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corr = dfCombined.corr()\n",
        "corr.style.background_gradient().set_precision(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dp4TEqJ5kWJS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Some plots\n",
        "The correlation table above indicates that **Profile_mean** and **Profile_skewness** are among the variables correlated strongly with **class**.   Let's plot signal and backgrund for those two variables."
      ]
    },
    {
      "metadata": {
        "id": "qo4Fs1PwkYoU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import plotly.plotly as py\n",
        "import numpy as np\n",
        "from plotly.offline import iplot\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "enable_plotly_in_cell()\n",
        "#\n",
        "# First plot\n",
        "variable = 'Profile_mean'\n",
        "trace1 = go.Histogram(\n",
        "    x=dfB_use[variable],\n",
        "    opacity=0.75,\n",
        "    name=\"Class=0\",\n",
        "    histnorm='probability'\n",
        ")\n",
        "trace2 = go.Histogram(\n",
        "    x=dfA[variable],\n",
        "    opacity=0.75,\n",
        "    name=\"Class=1\",\n",
        "    histnorm='probability'\n",
        ")\n",
        "\n",
        "data = [trace1, trace2]\n",
        "layout = dict(\n",
        "    title='Comparing Pulsar Classes',\n",
        "    xaxis=dict(title=variable),\n",
        "    yaxis=dict(title='Fraction')\n",
        ")\n",
        "iplot(dict(data=data,layout=layout),validate=False)\n",
        "\n",
        "#\n",
        "# second plot\n",
        "variable = 'Profile_skewness'\n",
        "trace1 = go.Histogram(\n",
        "    x=dfB_use[variable],\n",
        "    opacity=0.75,\n",
        "    name=\"Class=0\",\n",
        "    histnorm='probability'\n",
        ")\n",
        "trace2 = go.Histogram(\n",
        "    x=dfA[variable],\n",
        "    opacity=0.75,\n",
        "    name=\"Class=1\",\n",
        "    histnorm='probability'\n",
        ")\n",
        "\n",
        "data = [trace1, trace2]\n",
        "layout = dict(\n",
        "    title='Comparing Pulsar Classes',\n",
        "    xaxis=dict(title=variable),\n",
        "    yaxis=dict(title='Fraction')\n",
        ")\n",
        "iplot(dict(data=data,layout=layout),validate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jgUwz6zhtHzc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Additional: \n",
        "Look at a few of the other variables."
      ]
    },
    {
      "metadata": {
        "id": "Fmcl8xlvFDKI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Performance Method\n",
        "Define this for later use."
      ]
    },
    {
      "metadata": {
        "id": "G1udjEbIFGHt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Determine the performance\n",
        "def binaryPerformance(y,y_pred,y_score,debug=False):\n",
        "#\n",
        "# Assuming a binary classifier with 1=signal, 0=background\n",
        "  confusionMatrix = autovivify(2,int)\n",
        "  for i in range(len(y_pred)):\n",
        "    trueClass = y[i]\n",
        "    predClass = y_pred[i]\n",
        "    confusionMatrix[trueClass][predClass] += 1\n",
        "\n",
        "  if debug:\n",
        "    for trueClass in range(2):\n",
        "      print(\"True: \",trueClass,end=\"\")\n",
        "      for predClass in range(2):\n",
        "        print(\"\\t\",confusionMatrix[trueClass][predClass],end=\"\")\n",
        "      print()\n",
        "    print()\n",
        "  TP = confusionMatrix[1][1]\n",
        "  FP = confusionMatrix[0][1]\n",
        "  FN = confusionMatrix[1][0]\n",
        "  TN = confusionMatrix[0][0]\n",
        "  \n",
        "  if debug:\n",
        "    print(\"TP predicted true, actually true   \",TP)\n",
        "    print(\"FP predicted true, acutally false  \",FP)\n",
        "    print(\"TN predicted false, actually false \",TN)\n",
        "    print(\"FN predicted false, actually true  \",FN)\n",
        "\n",
        "\n",
        "  precision = TP / (TP + FP)\n",
        "  recall = TP / (TP + FN)\n",
        "  f1_score = 2.0 / ( (1.0/precision) + (1.0/recall) )\n",
        "  \n",
        "  if debug:\n",
        "    print(\"Precision = TP/(TP+FP) = fraction of predicted true actually true \",precision)\n",
        "    print(\"Recall = TP/(TP+FN) = fraction of true class predicted to be true \",recall)\n",
        "    print(\"F1 score = \",f1_score)\n",
        "\n",
        "  #\n",
        "  # Get the ROC curve.  We will use the sklearn function to do this\n",
        "  from sklearn import metrics\n",
        "  #fpr_train, tpr_train, thresholds_train = metrics.roc_curve(y_train, y_train_score, pos_label=1)\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y, y_score, pos_label=1)\n",
        "  #\n",
        "  # Get the auc\n",
        "  auc = metrics.roc_auc_score(y, y_score)\n",
        "  if debug:\n",
        "    print(\"AUC this sample: \",auc)\n",
        "  \n",
        "  return precision,recall,auc,fpr, tpr, thresholds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIhQBrAXz3V8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  The runFitter Method\n",
        "We will use the same form of the runFitter method we used for our k-fold validation study, with one exception.   In that case, our classifier was a pseudo-SVM (the LinearSVC).   To get a ROC curve (which is important for understanding perfromance) we need to **score** our results.   For LinearSVC, the score comes from its **decision_function** method.   For decision trees in sklearn, we will use the classifier **DecisionClassifier**, and this provides a **predict_prob** function (returning probabilites) instead of the decision_function method.   "
      ]
    },
    {
      "metadata": {
        "id": "PrB1nvOxz5gq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def runFitter(estimator,X_train,y_train,X_test,y_test,debug=False):\n",
        "#\n",
        "# Now fit to our training set\n",
        "  estimator.fit(X_train,y_train)\n",
        "#\n",
        "# Now predict the classes and get the score for our traing set\n",
        "  y_train_pred = estimator.predict(X_train)\n",
        "  y_train_score = estimator.predict_proba(X_train)[:,1]   # NOTE: some estimators have a predict_prob method instead od descision_function\n",
        "#\n",
        "# Now predict the classes and get the score for our test set\n",
        "  y_test_pred = estimator.predict(X_test)\n",
        "  y_test_score = estimator.predict_proba(X_test)[:,1]\n",
        "\n",
        "#\n",
        "# Now get the performaance\n",
        "  precision_test,recall_test,auc_test,fpr_test, tpr_test, thresholds_test\\\n",
        "    = binaryPerformance(y_test,y_test_pred,y_test_score,debug)\n",
        "  precision_train,recall_train,auc_train,fpr_train, tpr_train, thresholds_train\\\n",
        "    = binaryPerformance(y_train,y_train_pred,y_train_score,debug)\n",
        "#\n",
        "# Decide what you want to return: for now, just precision, recall, and auc for both test and train\n",
        "  results = {\n",
        "      'precision_train':precision_train,\n",
        "      'recall_train':recall_train,\n",
        "      'auc_train':auc_train,\n",
        "      'fpr_train':fpr_train, \n",
        "      'tpr_train':tpr_train, \n",
        "      'thresholds_train':thresholds_train,\n",
        "      'precision_test':precision_test,\n",
        "      'recall_test':recall_test,\n",
        "      'auc_test':auc_test,\n",
        "      'fpr_test':fpr_test, \n",
        "      'tpr_test':tpr_test, \n",
        "      'thresholds_test':thresholds_test}\n",
        "\n",
        "  return results\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lfCPNlDBGEzp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prepare the data\n",
        "As usual we shuffle it first and then dump the dataframe into an **X** features numpy array and a **y** labels numpy array."
      ]
    },
    {
      "metadata": {
        "id": "8BMr0hJsjM1c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "dfCombinedShuffle = shuffle(dfCombined,random_state=42)    # by setting the random state we will get reproducible results\n",
        "\n",
        "X = dfCombinedShuffle.as_matrix(columns=dfCombinedShuffle.columns[:8])\n",
        "y = dfCombinedShuffle['class'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nFBJwh-X0L0J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run the fitter\n",
        "Now we can use the function we defined above.   We are going to use the \"DecisionTreeClassifier\", and initially we will not use k-fold validation.   Let's just see how the decision tree works.\n"
      ]
    },
    {
      "metadata": {
        "id": "pFQ9B5vWOfPX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#\n",
        "# Note that we can give trainm_test_split *two* arrays as input!   The order of the output\n",
        "# is crucial though!!!\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
        "#\n",
        "# Define our estimator.     \n",
        "estimator = DecisionTreeClassifier(random_state=42)\n",
        "#\n",
        "# Get the results and printout\n",
        "results = runFitter(estimator,X_train,y_train,X_test,y_test,debug=False)\n",
        "print(\"Precision train/test \",round(results['precision_train'],3),round(results['precision_test'],3))\n",
        "print(\"Recall train/test    \",round(results['recall_train'],3),round(results['recall_test'],3))\n",
        "print(\"AUC train/test       \",round(results['auc_train'],3),round(results['auc_test'],3))\n",
        " \n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TjCwvGolR4E4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Examine Results\n",
        "Why are the training values all 1.0 and the testing values all around 0.9?   Perfect performance on the training sample is not actually a good thing!\n",
        "\n",
        "It turns out that the decision tree - if not limited, will continue to add nodes until it perfectly separates the training sample into signal and background.   It has **overfit** the training data, and effectively **memorized** the signal and background.  But this comes at the cost of **generalization**.    The trained model finds it difficult to generalize its procedure to new unseen testing data.   To fix this, we need to limit the decision tree so that it can't overfit.   \n",
        "\n",
        "If we look at sklearn in detail at how the DecisionTreeClassifier can be called, we will see the parameters that we can play with:\n",
        "\n",
        "    class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n",
        "\n",
        "Here are the descriptions of some of the key parameters:\n",
        "*  **max_depth : int or None, optional (default=None)**\n",
        "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "\n",
        "*  **min_samples_split : int, float, optional (default=2)**\n",
        "The minimum number of samples required to split an internal node:\n",
        "If int, then consider min_samples_split as the minimum number.\n",
        "If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
        "\n",
        "*  **min_samples_leaf : int, float, optional (default=1)**\n",
        "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
        "If int, then consider min_samples_leaf as the minimum number.\n",
        "If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
        "\n",
        "* **max_leaf_nodes : int or None, optional (default=None)**\n",
        "Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
        "\n",
        "To help us understand the decision tree we ended up with, it turns out we have access to a nice visualization tool (included in the methods at the top of this workbook).   Lets call it and see what the (overtrained) tree looks like when it is trained using all of the default parameters:"
      ]
    },
    {
      "metadata": {
        "id": "5UGE7PKuR7Pi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import SVG\n",
        "class_names = {0:\"background\",1:\"signal\"}\n",
        "classColumn='class'\n",
        "feature_names = [col for col in dfCombined.columns if col!=classColumn]\n",
        "graph = getDecisionTreeGraphic(estimator,feature_names,class_names)\n",
        "SVG(graph.pipe(format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hrDpGSsXVDRJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Making a simpler Decision Tree\n",
        "The default tree ended up with more than 200 **interior nodes** and **leaves**, which makes it quite difficult to understand.   (Aside: why is it so complicated?   By default, an unconstrained tree will **grow** until every final **leaf** is pure.   More on that later.)   Let's try limiting that by requiring the number of leaves (which are nodes with no children) to something more manageable: we will set max_leaf_nodes=3.   We will define the estimator with this parameter, run the algorithm, and then display the resultant decision tree."
      ]
    },
    {
      "metadata": {
        "id": "DBzy3oy_VVg9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Modify the parameters of the tree we will train\n",
        "estimator = DecisionTreeClassifier(random_state=42,max_leaf_nodes=3)\n",
        "#\n",
        "# Fit the tree\n",
        "results = runFitter(estimator,X_train,y_train,X_test,y_test,debug=False)\n",
        "#\n",
        "# Print results\n",
        "print(\"Precision train/test \",round(results['precision_train'],3),round(results['precision_test'],3))\n",
        "print(\"Recall train/test    \",round(results['recall_train'],3),round(results['recall_test'],3))\n",
        "print(\"AUC train/test       \",round(results['auc_train'],3),round(results['auc_test'],3))\n",
        "#\n",
        "# Plot the tree!\n",
        "graph = getDecisionTreeGraphic(estimator,feature_names,class_names)\n",
        "SVG(graph.pipe(format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RLtcNd9laYpU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Another simple (but different!) Decision Tree\n",
        "That one above looks much simpler!    Before going into detail on what the various parts mean, let's try another parameter: the number of levels in the tree.    We adjust this using **max_depth**."
      ]
    },
    {
      "metadata": {
        "id": "1R1o4Uh5aoav",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Restrict levels (depth) to 3\n",
        "estimator = DecisionTreeClassifier(random_state=42,max_depth=3)\n",
        "#\n",
        "# Fit the tree\n",
        "results = runFitter(estimator,X_train,y_train,X_test,y_test,debug=False)\n",
        "#\n",
        "# Print out\n",
        "print(\"Precision train/test \",round(results['precision_train'],3),round(results['precision_test'],3))\n",
        "print(\"Recall train/test    \",round(results['recall_train'],3),round(results['recall_test'],3))\n",
        "print(\"AUC train/test       \",round(results['auc_train'],3),round(results['auc_test'],3))\n",
        "#\n",
        "# PLot the tree\n",
        "graph = getDecisionTreeGraphic(estimator,feature_names,class_names)\n",
        "SVG(graph.pipe(format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n-vu15cVcB5u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Examination of the tree\n",
        "Let's look at this last tree in detail.   \n",
        "**NOTE**: since there may be slight variances in the random number generators used in my workbook and yours, the numbers above in the tree may be slightly different.   Here is a screenshot of the tree corresponding to the numbers below:\n",
        "![my tree](https://github.com/big-data-analytics-physics/data/blob/master/images/simleTree2.png?raw=true)\n",
        "\n",
        "There are a few things to point out:\n",
        "1.  The is one root node - node #0 at the very top of the tree.   Looking at the text in this node:\n",
        "    * node #0: This is the node name.\n",
        "    * Cut: Profile_skewness ≤ 0.889.    This the cut that the children of this node either pass and go to the left (labeled \"True\") or fail and go to the right (labeled \"False\").   \n",
        "     * gini = 0.5:   This is the **gini impurity** of the root node.   We will cover this below, but note that for a balanced dataset with equal or nearly equal signal and background, gini=0.5 for the root node. (**NOTE**: gini **coefficient** is a different idea.  It is also sometimes called the gini index, or the gini ratio, and is a measure of statistical dispersion.  Also note that, unfortunately, some writeups of Decision Trees confuse these terms.)\n",
        "     * samples = 2622: This is the number of samples in *this* node.\n",
        "     * value = [1288, 1334]:  This is the **actual** number of background (1288) and signal (1334) the node has.   Note the this is **different** than the number that go to the left (1431) and the right (1191), which you find by looking at the **samples** number of the children of this node.\n",
        "     * class = background: This is the class characterization of the node.   Since there are slighty more background than signal, this node is counted as background.   Note that the nodes get more orange the more background the node has, and more blue the more signal the node has.\n",
        "2.   There are 8 **leaves** in this tree.   A leaf is a node with no children.   It turns out that for this tree, all of the leaves are on the bottom level, but  this is not usually the case (look at the previous 2 trees to see this).\n",
        "3.  Looking at the left-most leaf on the bottom level we see two imortant things:\n",
        "     * value = [593, 9]  This means that if a new sample landed on this node, it would have a probability of $p_0$=593/602 of being background, and a probability of $p_1$=9/602 of being signal.\n",
        "     * gini=0.029:   A much lower **gini impurity**.   What is gini impurity?\n",
        "\\begin{equation*}\n",
        "G = 1.0 - \\sum_{i=1}^J p_i^2\n",
        "\\end{equation*}\n",
        "where in our case J=2 (because we have two classes).   Substituting $p_0$=593/602 and $p_1$=9/602 yields G=0.029.\n",
        "The **gini impurity**  of a node is the probability that a randomly chosen sample in a node would be incorrectly labeled, if it was labeled by the distribution of samples in the node. \n",
        "\n",
        "This gives us some insight into how a decision tree is formed:   At each node, the algorithm loops over all of the features *k* and all possible splits (or thresholds) $t_k$ among those features, to determine a split (or cut point) that results in the purest children of that node.   Mathematically, it finds the pair $(k,t_k)$ that minimizes the following cost function:\n",
        "\\begin{equation}\n",
        "J(k,t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}\n",
        "\\end{equation}\n",
        "where *left* and *right* refer the the child on the left or right.\n",
        "\n",
        "     "
      ]
    },
    {
      "metadata": {
        "id": "BmJAtZwNn39r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Now include k-fold cross validation\n",
        "Let's retrain the same tree (the second *simple* version), but now include cross validation."
      ]
    },
    {
      "metadata": {
        "id": "A6quxoM0oZyi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "kfolds = 5\n",
        "skf = StratifiedKFold(n_splits=kfolds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ByI13WOobjS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Get our estimator and predict\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "estimator = DecisionTreeClassifier(random_state=42,max_depth=3)\n",
        "avg_precision_train = 0.0\n",
        "avg_recall_train = 0.0\n",
        "avg_auc_train = 0.0\n",
        "avg_precision_test = 0.0\n",
        "avg_recall_test = 0.0\n",
        "avg_auc_test = 0.0\n",
        "numSplits = 0.0\n",
        "#\n",
        "# Now loop\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "  print(\"Training\")\n",
        "  numSplits += 1\n",
        "  X_train = X[train_index]\n",
        "  y_train = y[train_index]\n",
        "  X_test = X[test_index]\n",
        "  y_test = y[test_index]\n",
        "  \n",
        "#\n",
        "# Now fit to our training set\n",
        "  results = runFitter(estimator,X_train,y_train,X_test,y_test,debug=False)\n",
        "\n",
        "  avg_precision_train += results['precision_train']\n",
        "  avg_recall_train += results['recall_train']\n",
        "  avg_auc_train += results['auc_train']\n",
        "#\n",
        "  avg_precision_test += results['precision_test']\n",
        "  avg_recall_test += results['recall_test']\n",
        "  avg_auc_test += results['auc_test']\n",
        "#\n",
        "avg_precision_train /= numSplits\n",
        "avg_recall_train /= numSplits\n",
        "avg_auc_train /= numSplits\n",
        "avg_precision_test /= numSplits\n",
        "avg_recall_test /= numSplits\n",
        "avg_auc_test /= numSplits\n",
        "# \n",
        "# Now print\n",
        "print(\"Precision train/test \",round(avg_precision_train,3),round(avg_precision_test,3))\n",
        "print(\"Recall train/test    \",round(avg_recall_train,3),round(avg_recall_test,3))\n",
        "print(\"AUC train/test       \",round(avg_auc_train,3),round(avg_auc_test,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4mxLY47RPqZP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Overfitting vs Underfitting\n",
        "We have seen a very compex tree (allowing the tree to **fully grow** until all leaves are pure) as well as a couple of very simple trees.  Of course, we have many possible choices for the various hyperparameters, which will lead to a variety of different expected performance measures.   How do we choose among all of the possibilities?\n",
        "\n",
        "Let's modify the above k-fold validation, putting it inside of a loop that varies the hyperparameters.   We will store the resultant performance measures, and then plot them versus our hyperparameters.   \n",
        "\n",
        "For our test, we will vary just one hyperparemter (the tree max_depth) but we could easily do this over multiple hyperparameters.\n",
        "\n",
        "**NOTE**: we will make one change.   Rather than storing precision, recall, and AUC, which in some sense reflect **accuracy** of our models, we will instead store 1-precision, 1-recall, and 1-AUC, which will reflect **error** in our models."
      ]
    },
    {
      "metadata": {
        "id": "eg-04NVuRD0j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Get our estimator and predict\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "#\n",
        "# Create a dataframe to store our results\n",
        "dfError = pd.DataFrame(columns=['max_depth','trainError_pre','testError_pre',\n",
        "                                    'trainError_rec','testError_rec',\n",
        "                                    'trainError_auc','testError_auc'])\n",
        "#\n",
        "# OUTER loop over hyperparameters\n",
        "for max_depth in range(1,20):\n",
        "  print(\"training with max depth =\",max_depth)\n",
        "  estimator = DecisionTreeClassifier(random_state=42,max_depth=max_depth)\n",
        "  avg_precision_train = 0.0\n",
        "  avg_recall_train = 0.0\n",
        "  avg_auc_train = 0.0\n",
        "  avg_precision_test = 0.0\n",
        "  avg_recall_test = 0.0\n",
        "  avg_auc_test = 0.0\n",
        "  numSplits = 0.0\n",
        "#\n",
        "# INNER loop over k-fold splits\n",
        "  for train_index, test_index in skf.split(X, y):\n",
        "    numSplits += 1\n",
        "    X_train = X[train_index]\n",
        "    y_train = y[train_index]\n",
        "    X_test = X[test_index]\n",
        "    y_test = y[test_index]\n",
        "\n",
        "#\n",
        "# Now fit to our training set\n",
        "    results = runFitter(estimator,X_train,y_train,X_test,y_test,debug=False)\n",
        "    #print(\"results\",results)\n",
        "\n",
        "    avg_precision_train += results['precision_train']\n",
        "    avg_recall_train += results['recall_train']\n",
        "    avg_auc_train += results['auc_train']\n",
        "#\n",
        "    avg_precision_test += results['precision_test']\n",
        "    avg_recall_test += results['recall_test']\n",
        "    avg_auc_test += results['auc_test']\n",
        "#\n",
        "# Store the results for each HYPERPARAMETER iteration\n",
        "  avg_precision_train /= numSplits\n",
        "  avg_recall_train /= numSplits\n",
        "  avg_auc_train /= numSplits\n",
        "  avg_precision_test /= numSplits\n",
        "  avg_recall_test /= numSplits\n",
        "  avg_auc_test /= numSplits\n",
        "#\n",
        "# Fill dataframe\n",
        "  dfError = dfError.append({\n",
        "     'max_depth':max_depth,\n",
        "     'trainError_pre':1.0-avg_precision_train,'testError_pre':1.0-avg_precision_test,\n",
        "     'trainError_rec':1.0-avg_recall_train,'testError_rec':1.0-avg_recall_test,\n",
        "     'trainError_auc':1.0-avg_auc_train,'testError_auc':1.0-avg_auc_test\n",
        "      }, ignore_index=True)\n",
        "# \n",
        "# Now print\n",
        "print(dfError.head(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bBV3cwhiTYyC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Plotting results\n",
        "\n",
        "Lets look at the error measures:\n",
        "1.  train/test Error using precision: remember that **precision** = TP/(TP+FP) = \"Total actual positive found as positive\" / \"Total our model identified as positive\".   It is the fraction of identified postives that are truly positive.   **(1-precision)** is then the **error** - the fraction of our identified postives that are incorrect.\n",
        "2.  train/test Error using recall: remember that **recall** = TP/(TP+FN) = \"Total actual positive found as positive\" / \"Total actual positive\".   It is the fraction of actual positives that our model managed to identify.   **(1-recall)** is then the **error** - the fraction of actual postives that we failed to identify.\n",
        "3.  train/test Error using AUC: remember that AUC measures the probability that a randomly chosen positive example is properly ranked above a randomly chosen negative example.   **(1-AUC)** is then the probability that we will fail to do this."
      ]
    },
    {
      "metadata": {
        "id": "tmvKv77hToRD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import plotly.plotly as py\n",
        "import numpy as np\n",
        "from plotly.offline import iplot\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "enable_plotly_in_cell()\n",
        "# Create a trace\n",
        "trace1 = go.Scatter(\n",
        "    x = dfError['max_depth'],\n",
        "    y = dfError['trainError_pre'],\n",
        "    mode = 'line',\n",
        "    name = \"Training error\"\n",
        ")\n",
        "# Create a trace\n",
        "trace2 = go.Scatter(\n",
        "    x = dfError['max_depth'],\n",
        "    y = dfError['testError_pre'],\n",
        "    mode = 'line',\n",
        "    name = \"Testing Error\"\n",
        ")\n",
        "\n",
        "layout = dict(\n",
        "    title='Error (Precision) vs Model Complexity',\n",
        "    xaxis=dict(title='max_depth'),\n",
        "    yaxis=dict(title='Error (fraction)')\n",
        ")\n",
        "\n",
        "data = [trace1, trace2]\n",
        "iplot(dict(data=data,layout=layout),validate=False)\n",
        "\n",
        "# Create a trace\n",
        "trace1 = go.Scatter(\n",
        "    x = dfError['max_depth'],\n",
        "    y = dfError['trainError_rec'],\n",
        "    mode = 'line',\n",
        "    name = \"Training error\"\n",
        ")\n",
        "# Create a trace\n",
        "trace2 = go.Scatter(\n",
        "    x = dfError['max_depth'],\n",
        "    y = dfError['testError_rec'],\n",
        "    mode = 'line',\n",
        "    name = \"Testing Error\"\n",
        ")\n",
        "\n",
        "layout = dict(\n",
        "    title='Error (Recall) vs Model Complexity',\n",
        "    xaxis=dict(title='max_depth'),\n",
        "    yaxis=dict(title='Error (fraction)')\n",
        ")\n",
        "\n",
        "data = [trace1, trace2]\n",
        "iplot(dict(data=data,layout=layout),validate=False)\n",
        "# Create a trace\n",
        "trace1 = go.Scatter(\n",
        "    x = dfError['max_depth'],\n",
        "    y = dfError['trainError_auc'],\n",
        "    mode = 'line',\n",
        "    name = \"Training error\"\n",
        ")\n",
        "# Create a trace\n",
        "trace2 = go.Scatter(\n",
        "    x = dfError['max_depth'],\n",
        "    y = dfError['testError_auc'],\n",
        "    mode = 'line',\n",
        "    name = \"Testing Error\"\n",
        ")\n",
        "\n",
        "layout = dict(\n",
        "    title='Error (AUC) vs Model Complexity',\n",
        "    xaxis=dict(title='max_depth'),\n",
        "    yaxis=dict(title='Error (fraction)')\n",
        ")\n",
        "\n",
        "data = [trace1, trace2]\n",
        "iplot(dict(data=data,layout=layout),validate=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u4VVnNb2NtUF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Defining Underfitting and Overfitting Regions\n",
        "Looking at the AUC error vs max_depth curve we can see some general trends:\n",
        "1.   Note that as max_depth increases from left to right, we allow the model to get progressively more complex.\n",
        "2.   At the far left, as we go from very low complexity to slightly higher complexity, both the training error and the testing error go down.   This is typical.   Also note that the testing error is usually a bit above the trainining error in this region - also fairly typical.   In this region, the model is **underfit** - it still has room to improve on the testing set.\n",
        "      * Underfit models tend to have the similar fit performance on a testing set as they do on a training set.\n",
        "3.   As the complexity increases, around max_depth of 4 or so, the training error decreases, and so does the testing error, but the testing error begins to level out.   What is happening here is that our trained model is becoming more sensitive to features found only in the training set, and so the perfromance on the testing set improves less.\n",
        "4.  Beyond max_depth of 5-6, the model performance continue to improve on the training set, and begins to get **worse** on the testing set.   The model is now **overfit** - it is memorizing the training set, at the expense of learning general features which would allow it to improve on the testing set.\n",
        "      * Overfit models tend to have  worser fit performance on a testing set than they do on a training set.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "R9WHfXe2KMsP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import plotly.plotly as py\n",
        "import numpy as np\n",
        "from plotly.offline import iplot\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "enable_plotly_in_cell()\n",
        "\n",
        "# Create a trace\n",
        "trace1 = go.Scatter(\n",
        "    x = dfError['max_depth'],\n",
        "    y = dfError['trainError_auc'],\n",
        "    mode = 'line',\n",
        "    name = \"Training error\"\n",
        ")\n",
        "# Create a trace\n",
        "trace2 = go.Scatter(\n",
        "    x = dfError['max_depth'],\n",
        "    y = dfError['testError_auc'],\n",
        "    mode = 'line',\n",
        "    name = \"Testing Error\"\n",
        ")\n",
        "\n",
        "layout = dict(\n",
        "    title='Error (AUC) vs Model Complexity',\n",
        "    xaxis=dict(title='max_depth (model complexity)'),\n",
        "    yaxis=dict(title='Error (fraction)'),\n",
        "    shapes=[\n",
        "      {\n",
        "        \"opacity\": 0.2,\n",
        "        \"xref\": \"x\",\n",
        "        \"yref\": \"y\",\n",
        "        \"fillcolor\": \"#d3d3d3\",\n",
        "        \"y1\": 0.09,\n",
        "        \"y0\": 0.0,\n",
        "        \"x0\": 1,\n",
        "        \"x1\": 3,\n",
        "        \"type\": \"rect\"\n",
        "      },\n",
        "      {\n",
        "        \"opacity\": 0.2,\n",
        "        \"xref\": \"x\",\n",
        "        \"yref\": \"y\",\n",
        "        \"fillcolor\": \"#d3d3d3\",\n",
        "        \"y1\": 0.09,\n",
        "        \"y0\": 0.0,\n",
        "        \"x0\": 5,\n",
        "        \"x1\": 20,\n",
        "        \"type\": \"rect\"\n",
        "      }\n",
        "    ],\n",
        "    annotations= [\n",
        "      {\n",
        "        \"xref\": \"x\",\n",
        "        \"yref\": \"y\",\n",
        "        \"text\": \"Underfit Model\",\n",
        "        \"y\": 0.095,\n",
        "        \"x\": 2.0,\n",
        "        \"font\": {\n",
        "          \"color\": \"rgb(255, 0, 0)\",\n",
        "          \"size\": 18\n",
        "        },\n",
        "        \"showarrow\": False\n",
        "      },\n",
        "      {\n",
        "        \"xref\": \"x\",\n",
        "        \"yref\": \"y\",\n",
        "        \"text\": \"Progressively more Overfit Model\",\n",
        "        \"y\": 0.095,\n",
        "        \"x\": 12.5,\n",
        "        \"font\": {\n",
        "          \"color\": \"rgb(255, 0, 0)\",\n",
        "          \"size\": 18\n",
        "        },\n",
        "        \"showarrow\": False\n",
        "      }\n",
        "    ]\n",
        ")\n",
        "\n",
        "data = [trace1, trace2]\n",
        "iplot(dict(data=data,layout=layout),validate=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gZF3LyIWHV_m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The Bias-Variance Tradeoff\n",
        "The issue of over and under fitting in our models is strongly related to the concepts of bias and variance.   Within the context of classification:\n",
        "1.  **Bias** refers to the systematic difference between the average of our predictions and the average of the true labels in our dataset.\n",
        "     *  Bias decreases with increasing complexity of our model\n",
        "     *  Bias generally does not depend on training set size.\n",
        "     *  We want **low bias**.  If the bias is large, **both** training and testing error will be large, and the model is **underfit**.\n",
        "2. **Variance** refers to the difference between the predictions and our labels for different realizations of the same model.\n",
        "     * Variance increases with increasing complexity of our model.\n",
        "     * Variance decreases with training set size.\n",
        "     * We want **low variance**.   If the variance is large, the testing error will be significantly larger than the training error, and the model is **overfit**.\n",
        "     \n",
        "An excellent discussion of these points can be found in this [reference](http://scott.fortmann-roe.com/docs/BiasVariance.html).   The key idea is that although we want both **low bias** and **low variance**, we have only one knob to turn: the complexity of our model.   As we increase the complexity, we can get bias to go down, but eventually, variance will go up.   \n",
        "\n",
        "In practice this is handled by making the above error plot: we compare the error on the **testing** set to the error on the **training** set.   We expect these both to decrease up to some point, beyond which the testing error increases, while the training error continues to decrease.   We choose our model operating point (the hyperparameters) which correspond to the minimum of our testing error.\n"
      ]
    }
  ]
}