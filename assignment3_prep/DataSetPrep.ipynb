{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataSetPrep.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "4fQu0-BKjXCo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset preparation\n",
        "In this workbook we will deal with two (more) issues which come up in dataset preparation: feature scaling and text features.\n",
        "\n",
        "An extremely important thing to remember: when modifying the data sample that you use for training, you must use the **exact same** procedure before applying your trained model on test (or new unseen) data!   Asa we did earlier, we will make train and test samples to illustrate how this might be done.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "F_rQWhr7997K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Get the data and make train/test subsamples\n",
        "As before, we will use the **flights** data sample.   We will use a simple random split to make our train (80%) and test (20%) samples."
      ]
    },
    {
      "metadata": {
        "id": "dkVQH5QhkMqz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Now let's print some data to the screem\n",
        "flights = pd.read_csv(\"https://raw.githubusercontent.com/big-data-analytics-physics/data/master/flights/flights.csv\")\n",
        "train_flights,test_flights = train_test_split(flights, test_size=0.2, random_state=42,stratify=flights['carrier'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mDpvmD7D9QiX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling\n",
        "Many fitting algorithms have trouble with input variables (or features) whose ranges differ greatly.   To fix this we can do feature scaling.   Note that **targets** (the thing that we are trying to fit or predict) generally do **not** need to be scaled.   There are two feature scaling techniques which come up often:\n",
        "1.  min-max scaling: the feature values are scaled and shifted so that the lie from 0 to 1\n",
        "2.  standardization: here the mean (by column) is subtracted from each feature, and the result is divided by the variance so that it has unit vairance.\n",
        "\n",
        "We will use different techniques for different columns.  Generally we will not use these techniques for **categorical** columns (something different will be done).\n",
        "\n",
        "A quick exploration of the data will tell us more about which columns might need which type of scale.   A simple print and historgram of the data will help here!"
      ]
    },
    {
      "metadata": {
        "id": "q80qxI21kQjC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# \n",
        "# Need this code fragment for plotting\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.linear_model\n",
        "import plotly.offline as py\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "def enable_plotly_in_cell():\n",
        "  import IPython\n",
        "  from plotly.offline import init_notebook_mode\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "  '''))\n",
        "  init_notebook_mode(connected=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IuaE-fcYkVb4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "enable_plotly_in_cell()\n",
        "print(train_flights.head())\n",
        "train_flights.hist(bins=50,figsize=(20,15))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ylnTbEFk0Dz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looking at these plots and thinking about the meaning of the data, we will use the following:\n",
        "0.  Nothing for the categorical variables: carrier, tailnum, origin, dest\n",
        "1.   min-max scaling for: day, hour, minute\n",
        "2.   standardization for:  all of the others except arr_delay, minute, day, month, year\n",
        "\n",
        "Note: day, year, and month could be combined into a new categorical variable, which might have an impact on delays!\n",
        "\n",
        "**NOTE**: we won't scale arr_delay (the arrival time delay in minutes) because we will assume that this is the quantity we are interested in fitting - this is our **target** variable."
      ]
    },
    {
      "metadata": {
        "id": "auTBNXpLnC1-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Min-Max scaling and sci-kit learn estimators\n",
        "\n",
        "sklearn has a number of **estimators** which are incredibly useful for data preparation.   To use an estimator there are generally three steps:\n",
        "1.  Instantiate the estimator:\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "2.  Invoke the estimator.   For example:\n",
        "scaler = MinMaxScaler()\n",
        "There mighte be optional arguments that control how the estimator works.  Consult the documentation for this.\n",
        "\n",
        "3.  Fit the data to train the estimator.   For example:\n",
        "scaler.fit(train_flights[[\"day\"]])     ## WHY TWO BRACKET??\n",
        "You need to be careful with the type of argument expected by the estimator.  Most Scikit-Learn estimators require that data be strictly 2-dimensional. \n",
        "\n",
        "If we only want to transform one column of data and we select a single column like this:\n",
        "train_flights['day']\n",
        "technically, a **Pandas Series** is created which is a single dimension of data. We can force Pandas to create a one-column DataFrame, by passing a single-item list to the brackets like this:\n",
        "train_flights[]['day']]\n",
        "\n",
        "4.  Next we need to explicitly transform the data (remember: *fit* just trained the estimator):\n",
        "scaled_days = scaler.transform(flights[][\"day\"]])\n",
        "\n",
        "NOTE: we **trained** the estimator on the train_flight data.   How do we use this on the test_flight data?   **We only use the transform method!!**  This ensures that we treat our test data **exactly** like the training data:\n",
        "scaled_days_test = scaler.transform(test_flights[][\"day\"]])\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6LmGBkMJnZ2-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_flights[[\"day\",\"hour\"]])\n",
        "train_scaled_minmax = scaler.transform(train_flights[[\"day\",\"hour\"]])\n",
        "\n",
        "test_scaled_minmax = scaler.transform(test_flights[[\"day\",\"hour\"]])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ug5XQG25pmLW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Standardization\n",
        "Here we will form a new dataframe by droping all of the columns we don't want to scale, and then scaling the remaining:"
      ]
    },
    {
      "metadata": {
        "id": "jtINC8hrp1nG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "stanScal = StandardScaler()\n",
        "#\n",
        "# Get the numerical colums\n",
        "train_flights_subset = train_flights.drop([\"arr_delay\",\"carrier\",\"minute\", \"hour\", \"day\", \"month\", \"year\",\"origin\",\"dest\",\"tailnum\"], axis=1)\n",
        "stanScal.fit(train_flights_subset)\n",
        "train_flights_subset_stanscaled = stanScal.transform(train_flights_subset)\n",
        "for i in range(5):\n",
        "  print(train_flights_subset_stanscaled[i])\n",
        "  \n",
        "#\n",
        "# Do the same for the test data\n",
        "test_flights_subset = test_flights.drop([\"arr_delay\",\"carrier\",\"minute\", \"hour\", \"day\", \"month\", \"year\",\"origin\",\"dest\",\"tailnum\"], axis=1)\n",
        "test_flights_subset_stanscaled = stanScal.transform(test_flights_subset)\n",
        "for i in range(5):\n",
        "  print(test_flights_subset_stanscaled[i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NmABFICmrI2Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dealing with Text Features\n",
        "Let's take a quick look at one of the text features in our dataset: **origin**.   This is the 3 letter code the for airport the flight originates from:"
      ]
    },
    {
      "metadata": {
        "id": "GtavpkQdq3Ww",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# \n",
        "# Need this code fragment for plotting\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.linear_model\n",
        "import plotly.offline as py\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "def enable_plotly_in_cell():\n",
        "  import IPython\n",
        "  from plotly.offline import init_notebook_mode\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "  '''))\n",
        "  init_notebook_mode(connected=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9VkqFsmq7al",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import plotly.plotly as py\n",
        "import numpy as np\n",
        "from plotly.offline import iplot\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "enable_plotly_in_cell()\n",
        "trace1 = go.Histogram(\n",
        "    x=train_flights['origin'],\n",
        "    opacity=0.75,\n",
        "    name=\"Flight Origin\",\n",
        "    histnorm='probability'\n",
        ")\n",
        "\n",
        "data = [trace1]\n",
        "layout = go.Layout(barmode='overlay')\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "\n",
        "iplot(fig, filename='overlaid histogram')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W8xlJ9wwK1aU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The origin might be important for determing whether flight are delayed, but how can we use it in our fit.  (With just two items we might just do the analysis separately for each, but this might not be feasible with a large number of origins.)    "
      ]
    },
    {
      "metadata": {
        "id": "sz30zmfBrqEl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "#\n",
        "# Fit then transform with one-hot enocding\n",
        "columnToEncode = 'origin'\n",
        "onehot_encoder.fit(train_flights[[columnToEncode]])\n",
        "train_flights_cat_one_hot = onehot_encoder.transform(train_flights[[columnToEncode]])\n",
        "print(\"Transformed training data\",type(train_flights_cat_one_hot))\n",
        "for i in range(10):\n",
        "  print(train_flights.iloc[i][columnToEncode],train_flights_cat_one_hot[i])\n",
        "\n",
        "#\n",
        "# Now just transform the test data (don't refit!!)\n",
        "test_flights_cat_one_hot = onehot_encoder.transform(test_flights[[columnToEncode]])\n",
        "#\n",
        "# compare with:\n",
        "#test_flights_cat_one_hot = onehot_encoder.fit_transform(test_flights[[columnToEncode]])\n",
        "print(\"Transformed testing data\",type(test_flights_cat_one_hot))\n",
        "for i in range(10):\n",
        "  print(test_flights.iloc[i][columnToEncode],test_flights_cat_one_hot[i])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iBj4VDtNOwfO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Putting Humpty-Dumpty back together!\n",
        "The result of our three transforms on the training_flights data:\n",
        "1.   train_scaled_minmax: a 2 column by many row numpy array, obtained using min/max scaling\n",
        "2.   train_flights_subset_stanscaled: a multi-column y many row numpy array, obtained using standard scaling\n",
        "3.   train_flights_cat_one_hot: a multi column by many row numpy array, obtained by using 1-hot scaled\n",
        "\n",
        "If we want to use **ALL** of these in a subsequent fitting program, we need to recombine them into a single numpy array.   We will also pull out the **arr_delay** column - which we did not transform - as a **label** (also known as the target), since we will assume that this is the feature that we want to fit.\n",
        "Here is how we do this\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "HmOySQo0QTRN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_flights_labels = train_flights['arr_delay'].copy().values\n",
        "print(\"shape labels    \",type(train_flights_labels),train_flights_labels.shape)\n",
        "print(\"shape minmax    \",type(train_scaled_minmax),train_scaled_minmax.shape)\n",
        "print(\"shape stanscaled\",type(train_flights_subset_stanscaled),train_flights_subset_stanscaled.shape)\n",
        "print(\"shape one_hot   \",type(train_flights_cat_one_hot),train_flights_cat_one_hot.shape)\n",
        "train_flights_toFit = np.concatenate([train_scaled_minmax,train_flights_subset_stanscaled,train_flights_cat_one_hot], axis=1)\n",
        "print(\"shape ALL       \",type(train_flights_toFit),train_flights_toFit.shape)\n",
        "\n",
        "\n",
        "test_flights_labels = test_flights['arr_delay'].copy().values\n",
        "test_flights_toFit = np.concatenate([test_scaled_minmax,test_flights_subset_stanscaled,test_flights_cat_one_hot], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}