{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the filters of a CNN\n",
    "Understanding how our machine learning models actually work can be challenging.   In assignment 12,we used a method from the book to help us visualize CNN performance: we **made** a test image using backpropagation, such that the image **maximally activated** a given filter.\n",
    "\n",
    "We will use that same method again, using an already trained CNN (you can see the model by inspecting the file cnn_simple_mnist.py in this directory).  This model is developed on the MNIST sample included with Keras, and has two convolutional layers and a pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "model = load_model('/fs/scratch/PAS1495/physics6820/cnn_simple_mnist.h5')\n",
    "\n",
    "print(\"model summary\",model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "We will want to run on the data for some of the performance tests, particularly the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "input_shape = (1, img_rows, img_cols)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the filters of a CNN (continued)\n",
    "To understand what these filters are doing, we will use a method from Chapter 5 of the Deep Learning with Python text.   The idea is to find an image (starting from a randomly initialized image) which causes each of the filters to respond maximally.   In some sense, this is the stucture that the filter is \"looking for\" in the input images.\n",
    "\n",
    "Here is the relevant code from Chapter 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "def deprocess_image(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "#    x *= 0.3\n",
    "    \n",
    "    x+= 0.5\n",
    "    x = np.clip(x,0,1)\n",
    "    \n",
    "    #x*= 255\n",
    "    #x = np.clip(x,0,255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def generate_pattern(model,layer_name,filter_index,size=28):\n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    loss = K.mean(layer_output[:,:,:,filter_index])\n",
    "    grads = K.gradients(loss,model.input)[0]\n",
    "    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "    iterate = K.function([model.input],[loss,grads])\n",
    "    input_img_data = np.random.random((1,size,size,1))*20+128.0\n",
    "\n",
    "    step = 0.25\n",
    "    for i in range(50):\n",
    "        loss_value,grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "    \n",
    "    img = input_img_data[0]\n",
    "    return deprocess_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "n=32\n",
    "nn=16\n",
    "print(\"Layer conv2d_1\")\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, nn, i + 1)\n",
    "    thisImg = generate_pattern(model,'conv2d_1',i)\n",
    "    plt.imshow(thisImg.reshape(28,28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "n=64\n",
    "print(\"Layer max_pooling2d_1\")\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(4, nn, i + 1)\n",
    "    thisImg = generate_pattern(model,'max_pooling2d_1',i)\n",
    "    plt.imshow(thisImg.reshape(28,28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Filter Another Way!\n",
    "The above images may be a bit... dissatisfying.   There is another way to learn what the filters are doing, and this using the input images themselves.   The basic idea is this:\n",
    "1.  Loop over all of the images.\n",
    "2.  For each image, determine the output of each filter at a given layer.\n",
    "3.  Store the output for that layer and image - we will use a nested dictionary to do this.\n",
    "4.  Once finished, we can sort the output from highest to lowest, then plot the images which correspond to the largest filter output - we will plot the top 10 or 20 images.\n",
    "\n",
    "In this way, we can examine the images by filter, and look to see if there are any distinguishing features among the highest output images.\n",
    "\n",
    "How do we get at the outputs at intermediate layers in a network?   Keras has a nice feature that allows you to define a new Model based on an existing model, where the output of this new model is layer output you desire.   The only thing you need to jnow is the layer name: we get this from the model.summary() function (which we printed above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Here we can make a new model which has the same inoputs as the VGG model, but has\n",
    "# an output at an intermediate layer\n",
    "from keras.models import Model\n",
    "import time\n",
    "#\n",
    "# This info came from inspecting the output of print(model.summary()) above\n",
    "#\n",
    "# The output from the first convolutional layer\n",
    "layer = 1\n",
    "\n",
    "layer_name = 'conv2d_1'\n",
    "numxy = 26\n",
    "filters = 32\n",
    "\n",
    "if layer==2:\n",
    "#\n",
    "# The output from the middle convolutional layer\n",
    "    layer_name = 'conv2d_2'\n",
    "    numxy = 24\n",
    "    filters = 64\n",
    "#\n",
    "# The output from the last convolutional layer\n",
    "elif layer==3:\n",
    "    layer_name = 'max_pooling2d_1'\n",
    "    numxy = 12\n",
    "    filters = 64\n",
    "#\n",
    "# This defines a new model which has as its output the output of the above chosen layer\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "print(\"Predicting outputs for layer \",layer_name)\n",
    "#\n",
    "# We use partial instead of autovivify because otherwise we can't pickle the output (not sure why)\n",
    "#neuronsByImageIndex = autovivify(2,float)\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "neuronsByImageIndex = defaultdict(partial(defaultdict, float))\n",
    "\n",
    "#\n",
    "# Put each image in\n",
    "t0 = time.time()\n",
    "numFiles = 0\n",
    "for i in range(len(x_test)):\n",
    "    img = x_test[i]\n",
    "    numFiles += 1\n",
    "    if numFiles%1000 == 0:\n",
    "        print(\"Processed \",numFiles,time.time()-t0)\n",
    "        t0 = time.time()\n",
    "    #preds = model.predict(img_data)\n",
    "    decoded_imgs = intermediate_layer_model.predict(img.reshape(1,28,28,1))\n",
    "    count = 0\n",
    "    #print(decoded_imgs.shape)\n",
    "#\n",
    "# Sum up the activations for this filter in this layer, store them by filter (this is \n",
    "# referred to by count) and by the image number (this is referred to by i)\n",
    "    for i3 in range(filters):\n",
    "        neuronsByImageIndex[count][i] = decoded_imgs[0,:,:,i3].sum()\n",
    "        count += 1\n",
    "        \n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pickle\n",
    "The python pickle module is very useful for quickly \"serializing\" a python object to file.  You can then re-load this file using the pickle.load method.  We don't really need to do this for the MNIST case, but will we take advantage of this for a real image dataset: we will use straight python and qsub to calculate the *neuronsByImageIndex* dictionary, and then use pickel to save the results.   We then load that pickle file in jupyter so we can visualize it.\n",
    "\n",
    "Below we show how to dump the pickle file to disk and then reload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#\n",
    "# Save our dictionary\n",
    "pickle.dump(neuronsByImageIndex,open('neuronsByImageIndex_cnn_mnist_'+layer_name+'.pkl', 'wb') )\n",
    "\n",
    "#\n",
    "# Now load the dictionary back!   Usually the saving and re-loading are done in 2 different files!\n",
    "fname = open('neuronsByImageIndex_cnn_mnist_'+layer_name+'.pkl', 'rb')\n",
    "neuronsByImageIndex = pickle.load(fname) \n",
    "print(\"neuronsByImageIndex length\",len(neuronsByImageIndex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the output\n",
    "Now we can loop over the \"neurons\" (filters actually) and sort them - then plot the top 10.  Can you see any distinguising patterns among the resulting numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "\n",
    "print(\"numneurons \",len(neuronsByImageIndex))\n",
    "n=10\n",
    "plt.figure(figsize=(20, 4))\n",
    "numDisplay = min(len(neuronsByImageIndex),100)\n",
    "for neuron in range(numDisplay):\n",
    "    thisNeuron = neuronsByImageIndex[neuron]\n",
    "    i = 0\n",
    "#    print(\"neuron \",neuron)\n",
    "    for w in sorted(thisNeuron, key=thisNeuron.get, reverse=True)[:n]:\n",
    "        #print(w, thisNeuron[w])\n",
    "        # display original\n",
    "        if thisNeuron[w] > 0.0:\n",
    "            ax = plt.subplot(2, n, i + 1)\n",
    "#\n",
    "# Uncomment these lines if w is a pointer in the image array (like for MNIST)\n",
    "            img = x_test[w]\n",
    "            plt.imshow(img.reshape(28,28))\n",
    "# \n",
    "# Uncomment these lines if \"w\" is the path to the file (like for natural images)\n",
    "#            img = image.load_img(w.replace('PAS1043','PAS1495'), target_size=(224, 224))\n",
    "#            plt.imshow(img)\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            i += 1\n",
    "    if i>0:\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(20, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Task\n",
    "Rerun the above code, but use *layer=3* instead - this will let us look at the filters in the outermost layer.   Do the images you get this way look different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Apply to Real Images!\n",
    "We cac do exactly the same thing using a network that has been trained to classifiy real images.   It turns out that Keras comes with a number of such pre-trained classifiers, see \n",
    "the Keras Applications section [here](https://keras.io/applications/) for more details.\n",
    "\n",
    "We will use the model VGG16.   This is a neural network that performed very well in the Image Net Large Scale Visual Recognition Challenge (ILSVRC) in 2014. It scored first place on the image localization task and second place on the image classification task.\n",
    "\n",
    "Localization is finding where in the image a certain object is, described by a bounding box. Classification is describing what the object in the image is. This predicts a category label, such as “cat” or “bookcase”.\n",
    "\n",
    "ImageNet is a huge database of images for academic researchers.  The images used in the competition are divided into 1000 different categories. \n",
    "\n",
    "Let's load the model and inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "model = VGG16(weights='imagenet')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the pickle files and inspecting the important images\n",
    "\n",
    "I have already run the above code using this model on the validation images.   The code to run it is included in this directory in the following files:\n",
    "* pbs_run_calcviz_vgg16_gpu_layer5.sh\n",
    "* calcviz_vgg16.py\n",
    "\n",
    "You can rerun these if you like, but each script takes about an hour to run on a gpu!\n",
    "\n",
    "The resulting pickle files are stored in the scratch area:\n",
    "* /fs/scratch/PAS1495/physics6820/neuronsByImageIndex_block1_pool.pkl\n",
    "* /fs/scratch/PAS1495/physics6820/neuronsByImageIndex_block3_pool.pkl\n",
    "* /fs/scratch/PAS1495/physics6820/neuronsByImageIndex_block5_pool.pkl\n",
    "\n",
    "In the following code blocks, try looking at each of the files above.  You should see different behavior for each layer: start with the innermost layer, and proceed to the outermost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Now load the dictionary back!   Usually the saving and re-loading are done in 2 different files!\n",
    "fname = open(XXXXX, 'rb')\n",
    "neuronsByImageIndex = pickle.load(fname) \n",
    "print(\"neuronsByImageIndex length\",len(neuronsByImageIndex))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copy display code from above - be sure to comment/uncomment the appropriate lines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "sys_python36conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
