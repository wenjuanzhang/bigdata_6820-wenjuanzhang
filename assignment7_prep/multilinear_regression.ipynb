{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multilinear_regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "dVF9wo7WO42_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear Regression and Gradient Descent\n",
        "Up to this point we have investigated a number of different data sets.   Each dataset had a number of data samples, and each sample had a **label** and a number of **features**.  In each case we were either told or we were trying to find a relationship between the features and the labels, with the goal of developing a **model** which would allow us to predict the label, given new **unseen** data.\n",
        "\n",
        "Some examples:\n",
        "\n",
        "\n",
        "1.   Housing data: Here we were trying to find a linear relationship between various features in a  housing data set and the median house value across California districts.  This used the sklearn LinearRegression package to perform **regression**.\n",
        "2.   MNIST digits: Here we used the pixels in a 28x28 image as our features, and digit values as their labels.  We used the sklearn  LinearSVC package to perform **classification**.\n",
        "3.   Pulsar data:  Here we used descriptive features to classify samples as either pulsars or background.   We used \n",
        "the sklearn packages for decision trees and random forests to perform classification.\n",
        "In all of these cases we have used method from the **sckit learn** package.   \n",
        "\n",
        "Let's take some time and investigate some of the underlying mathematics for these tools and write our own tools, at least for some simple implementations.   \n",
        "\n",
        "We will start with Multiple Linear Regression,  (aka multivariable regression) which has  dependent variable and multiple independent variables.  Note that this is not formally Multivariate regression, which has multiple dependent variables and multiple independent variables.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VgDEKZyBSfsW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generation of a Toy Dataset\n",
        "Later on we will apply our methods to a previous example where we used sklearn.    But for now, we will generate a toy dataset, with two independent variables, which each have a fixed relationship with a single dependent variable."
      ]
    },
    {
      "metadata": {
        "id": "dvrxavi00sqa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "numPoints = 200\n",
        "X1 = 10.0* np.random.rand(numPoints,1)\n",
        "X2 = 20.0* np.random.rand(numPoints,1)\n",
        "beta0 = 4.2\n",
        "beta1 = -15.2\n",
        "beta2 = 7.7\n",
        "#\n",
        "yrand = np.random.normal(0.0, 10.0, numPoints)\n",
        "yrand = yrand.reshape(numPoints,1)\n",
        "y = beta0 + beta1*X1 + beta2*X2\n",
        "#\n",
        "# If you want to add some noise to the label, uncomment this line\n",
        "#y = y + yrand\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fs_RuL9UJW32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "57pqzkAxZmb8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Visualize the dataset\n",
        "To see this dataset, we will use Scatter3D from plotly.   By using your mouse you can rotate the plot and verify that the dependent variable **y** is linear in both **X1** and **X2**."
      ]
    },
    {
      "metadata": {
        "id": "4OSHPAR62kjj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def enable_plotly_in_cell():\n",
        "  import IPython\n",
        "  from plotly.offline import init_notebook_mode\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "  '''))\n",
        "  init_notebook_mode(connected=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LsX0HP1m1ny1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Lets plot this\n",
        "import plotly.plotly as py\n",
        "import numpy as np\n",
        "from plotly.offline import iplot\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "enable_plotly_in_cell()\n",
        "\n",
        "trace1 = go.Scatter3d(\n",
        "    x=np.squeeze(X1),\n",
        "    y=np.squeeze(X2),\n",
        "    z=np.squeeze(y),\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=5,\n",
        "        line=dict(\n",
        "            color='rgba(217, 217, 217, 0.14)',\n",
        "            width=0.2\n",
        "        ),\n",
        "        opacity=0.8\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "data = [trace1]\n",
        "layout = dict(\n",
        "    title='Toy Dataset',\n",
        "    xaxis=\"X1\",\n",
        "    yaxis=\"X2\",\n",
        "    zaxis=\"observed\"\n",
        ")\n",
        "\n",
        "#iplot(dict(data=data))\n",
        "#iplot(dict(data=data,layout=layout))\n",
        "\n",
        "fig = go.Figure(data=data)\n",
        "iplot(fig,validate=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wDzht8w88pQF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "We are to imagine that we were given this dataset, with labels **y**, and features **X1** and **X2**, and we are going to dtry to **discover** the underlying linear relationship - if there is one.   Of course since we generated the data we know that there is, but generally we don't know this for sure.   \n",
        "\n",
        "## The Hypothesis\n",
        "We will use the following hypothesis: that the labels can be predicted from the features using the following model:\n",
        "$$h_\\theta^{(i)} = y_{pred}^{(i)} = \\theta_0  + \\theta_1 \\cdot X_1^{(i)} +  \\theta_2 \\cdot X_2^{(i)} $$\n",
        "\n",
        "\n",
        "The vector of $\\theta$ values {$\\theta_0,\\theta_1,\\theta_2$} are the **parameters** of our model.   Our goal is to come up with a procedure to estimate these parameters.\n",
        "\n",
        "We will also define two counters:\n",
        "*  *m*: this will run from 1 to the number of samples (or data points) that we have.\n",
        "*  *n*: this will run from 1 to the number of features that we have\n",
        "\n",
        "For our current dataset, m=200, and n=2.  Also, note that we have m=200 $h_\\theta^{(i)}=y_{pred}^{(i)}$ values.\n",
        "\n",
        "## Fitting for the parameters of our model\n",
        "We would like to choose $\\theta_0,\\theta_1,\\theta_2$ so that $h_\\theta^{(i)}=y_{pred}^{(i)}$ is a close to all of the $y^{(i)}$ as possible.\n",
        "\n",
        "To do this, we will minimize the following **cost function** $J(\\theta)$:\n",
        "$$J(\\theta) = {1\\over{2m}}\\sum_{i=i}^m(h_\\theta^{(i)}-y^{(i)})^2$$\n",
        "\n",
        " $$J(\\theta) = {1\\over{2m}}\\sum_{i=i}^m(\\theta_0  + \\theta_1 \\cdot X_1^{(i)} +  \\theta_2 \\cdot X_2^{(i)}-y^{(i)})^2$$\n",
        " \n",
        " ## Gradient Descent\n",
        " We want to find the values $\\theta_0,\\theta_1,\\theta_2$  that minimize $J(\\theta)$.   The basic idea is to try slowly modify all of our $\\theta$ parameters in such a way that we can move in a direction that minimizes J.   Gradient descent is an algorithm that allows us to do this.   The basic idea is this, for each parameter, we iteratively update according to the following scheme:\n",
        " $$\\theta_j := \\theta_j - \\alpha {\\delta J\\over \\delta \\theta_j}(\\theta)$$\n",
        " for j=0,1,..n.   Note that we have (n+1) parameters and not $n$ because, although we have $n$ features, due to the intercept term $\\theta_0$, we actually have $n+1$ parameters.\n",
        " \n",
        " The term $\\alpha$ is called the **learning rate**, and it controls how quickly we will update (or correct) our parameters to find the minimum of *J(\\theta)$.   More on this below.\n",
        " \n",
        " Note the use of the special symbols \":=\".   This means that we need to **simultaneously** update all of the $\\theta$ parameters.   \n",
        " \n",
        " ## Adding a Dummy Features Column\n",
        " It turns out that we can make our calculations much easier if we employ a little trick.   For our present problem we have two features, X1 and X2, which take on a variety of different values.  We are going to add, for every sample in our data, a new column X0, which always has the value of 1.0.   By doing this, we can rewrite our cost function like this:\n",
        " $$J(\\theta) = {1\\over{2m}}\\sum_{i=i}^m(\\theta_0\\cdot X_0^{(i)}  + \\theta_1 \\cdot X_1^{(i)} +  \\theta_2 \\cdot X_2^{(i)}-y^{(i)})^2$$\n",
        " \n",
        " We can now write $J(\\theta)$  using matrix notation like this:\n",
        " $$J(\\theta) = {1\\over{2m}}\\sum_{i=i}^m(X^{(i)} \\cdot \\theta  -y^{(i)})^2= {1\\over{2m}}\\sum_{i=i}^m(h_\\theta(X^{(i)})  -y^{(i)})^2$$\n",
        " \n",
        " Let's remember the dimensions of each term:\n",
        " 1.  $\\theta$: this is a matrix of dimension 3x1\n",
        " 2.  $X$: This is a matrix of dimension mx3 (since we have m samples).\n",
        " 3. $h_\\theta(X)$: This is a matrix of dimension mx1.\n",
        " 4.  $y$: This is a matrix of dimension mx1.\n",
        " \n",
        "Note that although I described each of the above as *matrices*, we will use numpy (2 dimensional) *arrays* to implement all fo them.\n",
        " \n",
        " We can implement the above calculation in a single line of code (though we will use 2 for clarity):\n",
        " $$hTheta = np.dot(X,Theta)$$\n",
        "  $$J=np.sum(np.square(hTheta-y))/(2*len(y))$$\n",
        "  \n",
        "The actual code is listed here (using slightly different names for the variables):\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "Fh3-0WYBP8z0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calc_cost(Theta,Xp,y):\n",
        "  hTheta = np.dot(Xp,Theta)\n",
        "  cost=np.sum(np.square(hTheta-y))/(2*len(y))\n",
        "  return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5gOxMu18QHbP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementing Gradient Descent\n",
        "In order to implement gradient descent, we first need the gradient - the derivative of J with respect to our $\\theta$.\n",
        "\n",
        "Here is our cost function $J(\\theta)$:\n",
        " $$J(\\theta) = {1\\over{2m}}\\sum_{i=i}^m(X^{(i)} \\cdot \\theta  -y^{(i)})^2= {1\\over{2m}}\\sum_{i=i}^m(h_\\theta(X^{(i)})  -y^{(i)})^2$$\n",
        " \n",
        " And here is the derivative with respect to $\\theta$:\n",
        " $${\\delta J\\over \\delta \\theta_j} = {1\\over{m}}\\sum_{i=i}^m(X^{(i)} \\cdot \\theta  -y^{(i)})\\cdot X^{(i)}= {1\\over{m}}\\sum_{i=i}^m(h_\\theta(X^{(i)})  -y^{(i)})\\cdot X^{(i)}$$\n",
        " \n",
        " Again - we can implement the above calculation in a single line of code (though we will use 3 for clarity):\n",
        "  \n",
        "$hTheta = np.dot(Xp,Theta)$\n",
        "\n",
        "$delTheta = np.dot(Xp.transpose(),(hTheta-yp))$\n",
        "\n",
        "$delTheta = delTheta / (2*len(y))$\n",
        "\n",
        "\n",
        "Notice that we don't have to actually do the summation in the second line - that is taken care of for us when we do the dot-product.\n",
        "\n",
        "The actual code is listed here (using slightly different names for the variables):\n",
        "\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "1MhwwtChc67F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calc_gradient_descent(Theta,Xp,yp):\n",
        "  hTheta = np.dot(Xp,Theta)\n",
        "  delTheta = np.dot(Xp.transpose(),(hTheta-yp))\n",
        "  delTheta /= (2*len(y))\n",
        "  return delTheta\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OZHH68WLfPAJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementing Gradient Descent: The Learning Rate\n",
        "We mentioned above the $\\alpha$ controls the learning rate.   To understand this, refer to this figure:![learningRate](https://i.stack.imgur.com/0tirm.png)\n",
        "\n",
        "The learing rate $\\alpha$ is a parameter you have to set by hand.  Typical values are 0.01-0.0001.   If the learning rate is too small, as shown on the left in the figure, it may take many iterations to get to the minimum of the cost function.   If the learning rate is too high, you may overshoot the minimum and your search might not converge.   The basic way to test this is to plot the cost as a fucntion of the iteration - this will help you see if the learning rate needs to be adjusted."
      ]
    },
    {
      "metadata": {
        "id": "zyEYjFVKh2yO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Iterating until we converge\n",
        "The basic algorithm then to implement gradient descent looks like this:\n",
        "0. Initialize each of the $\\theta$ parameters to some reasonable value (0 is common, or a random number).\n",
        "1. Have an outer loop that checks that we have not exceeded our maximum number of allowed iterations **AND** that are cost is still decreasing.\n",
        "2. Calculate the gradient and update our parameters like so:\n",
        "$$\\theta_j := \\theta_j - \\alpha {\\delta J\\over \\delta \\theta_j}(\\theta)$$\n",
        "3. Calculate the cost for this iteration and compare it to the cost of the previous iteration.\n",
        "4. If the change in cost is small enough, declare victory and jump out of the loop.\n",
        "\n",
        "It is helpful to keep track of the cost for each iteration, so you can plot it and inspect its behavior.   And of course you need to keep track of the last value of the $\\theta$ parameters so you can return them.\n",
        "\n",
        "An implementation of this iteration algorithm is shown below."
      ]
    },
    {
      "metadata": {
        "id": "DUDb-LEh9EOV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def fit_data(Xp,yp,learningRate,max_iterations,scale=True,delta=0.001):\n",
        "#\n",
        "# Get the initial values\n",
        "  m,features = Xp.shape\n",
        "  Theta = np.zeros((features,1))\n",
        "  costList = []\n",
        "  cost = calc_cost(Theta,Xp,yp)\n",
        "  cost_change = delta+0.1\n",
        "  iterations = 0\n",
        "#\n",
        "# Iterate until we reach max allowed iterations or the cost function has plateaued\n",
        "  while (iterations<iterations_max) and (cost_change>delta):\n",
        "    last_cost = cost\n",
        "#\n",
        "# Update the parameters all at once\n",
        "    Theta = Theta - learningRate*calc_gradient_descent(Theta,Xp,yp)\n",
        "#\n",
        "# Calculate the new cost, and see how much it has changed from the previous cost\n",
        "    cost = calc_cost(Theta,Xp,yp)\n",
        "    cost_change = last_cost - cost\n",
        "\n",
        "    costList.append(cost)\n",
        "    iterations += 1\n",
        "    #print()\n",
        "    #print(\"   iter,cost.cost_change\",iterations,cost,cost_change)\n",
        "    \n",
        "  return Theta,iterations,costList\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4iJTGXEJmJZG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the data and running the algorithm\n",
        "We are finally ready to apply our algorithm to our toy dataset.   Remember our dataset has two features X1 and X2, and one label y.   We will need to combine X1 and X2 into a single 2D numpy array, and add a \"ones\" column (to the front of that array). \n",
        "\n",
        "However, before we add the ones column, it is **very helpful** to scale our input features.   This will **greatly** aid in converging.    We will use min-max scaling.   \n",
        "\n",
        "Keep in mind that if you use scaling:\n",
        "1.  If you want to predict labels using new (or old for that matter) features, you will have to scale those features using the same scaling parameters.\n",
        "2.  The parameters we get from minimizing our cost function are using the scaled features, so the $\\theta$ values we get won't correspond to the model used to generate the data.  If you want **those** $\\theta$ values (and you probably will), you will need to apply a transform to obtain them.   This is shown below.\n",
        "\n",
        "The code below prepares the data, and then runs the fit.\n"
      ]
    },
    {
      "metadata": {
        "id": "bw6c1MA09_w8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scl = MinMaxScaler()\n",
        "\n",
        "#\n",
        "# Form data\n",
        "XToFit = np.append(X1,X2,axis=1)\n",
        "yToFit = y\n",
        "#print(XToFit.shape,yToFit.shape)\n",
        "#\n",
        "# Make sure feature data is normalized\n",
        "XToFit2 = scl.fit_transform(XToFit)\n",
        "#XToFit2 = XToFit\n",
        "#\n",
        "# Prepend the \"ones\" column\n",
        "ones = np.ones((len(XToFit2),1))\n",
        "XToFit2 = np.append(ones,XToFit2,axis=1)\n",
        "#\n",
        "# Make sure label data has the correct shape\n",
        "yToFit2 = yToFit.reshape(len(yToFit),1)\n",
        "#\n",
        "# Check shapes\n",
        "print(\"Features shapes: \",XToFit2.shape)\n",
        "print(\"Labels shapes:   \",yToFit2.shape)\n",
        "\n",
        "iterations_max = 50000\n",
        "learningRate = 0.01\n",
        "Theta,iterations,costList = fit_data(XToFit2,yToFit2,learningRate,iterations_max,delta=0.00001)\n",
        "#Theta,costList = fit_data_minimize(XToFit,yToFit,learningRate,iterations)\n",
        "print(\"Iterations:\",iterations)\n",
        "print(\"Final cost:\",costList[-1])\n",
        "print(\"fit Theta \",Theta)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wCCdAw_voKlA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inspection of results.\n",
        "First off, it the cost looks low, and the number of iterations is well below our maximum.    But the theta values are nothing like our starting values of \n",
        "* beta0 = 4.2\n",
        "* beta1 = -15.2\n",
        "* beta2 = 7.7\n",
        "\n",
        "We need to transform them!\n",
        "\n",
        "The following code assumes a max-min scaler has been applied, and using the parameters from that scaler, recalculates the $\\theta$ values."
      ]
    },
    {
      "metadata": {
        "id": "Uj8gc86olQnQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def coef_transform(Theta,scl):\n",
        "  mxs = scl.data_max_\n",
        "  mns = scl.data_min_\n",
        "  print(\"pars \",mxs,mns,len(Theta))\n",
        "  clist = []\n",
        "  alist = []\n",
        "  for mn,mx in zip(mns,mxs):\n",
        "    clist.append(1.0/(mx-mn))\n",
        "    alist.append(mn/(mx-mn))\n",
        "  print(\"clist \",clist)\n",
        "  print(\"alist \",alist)\n",
        "  newTheta = Theta\n",
        "  print(\"newTheta shape \",newTheta.shape)\n",
        "  \n",
        "  for i in range(len(clist)):\n",
        "    newTheta[0,0] -= Theta[i+1,0]*alist[i] \n",
        "    newTheta[i+1,0] = Theta[i+1,0]*clist[i] \n",
        "  return newTheta\n",
        "\n",
        "#\n",
        "# Transform coeficients back\n",
        "Theta_transform = coef_transform(Theta,scl)\n",
        "print(\"Theta_transform\",Theta_transform)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jiJvH6NgpC5Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Much better!**"
      ]
    },
    {
      "metadata": {
        "id": "fyzJebEak_TF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Plotting the cost versus iteration.\n",
        "Looking at this will ensure that the algorithm converged properly."
      ]
    },
    {
      "metadata": {
        "id": "Nuo2xPU0Xy0o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "enable_plotly_in_cell()\n",
        "data = go.Scatter(\n",
        "    x=np.array(range(0,len(costList))),\n",
        "    y=costList,\n",
        "    mode='markers',\n",
        "    name=\"fitted data\"\n",
        ")\n",
        "\n",
        "\n",
        "iplot(dict(data=[data]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XaOgUssGsrnF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Application to a new dataset\n",
        "To make sure you understand this, apply the above procedure to the first dataset that we fit: the GDP dataset.\n",
        "\n",
        "In assignmen3_prep/ch1_scikit_intro.ipynb,  we fit the single feature 'GDP' to the label 'Life satisfaction' using the sklearn LinearRegression model.   Apply the gradient descent method from above to this dataset, and compare the results to our previous results.\n",
        "\n",
        "The dataset is read in below.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pIvmdumMtS4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/big-data-analytics-physics/data/master/ch1/gdp_oecd_data_byCountry.csv\"\n",
        "data=pd.read_csv(url)\n",
        "print(data.head())\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}